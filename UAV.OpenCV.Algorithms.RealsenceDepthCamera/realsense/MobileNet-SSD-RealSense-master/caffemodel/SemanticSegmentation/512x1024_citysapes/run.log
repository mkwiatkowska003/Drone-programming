I1006 03:22:01.930742  5661 caffe.cpp:902] This is NVCaffe 0.17.0 started at Sat Oct  6 03:22:01 2018
I1006 03:22:01.930873  5661 caffe.cpp:904] CuDNN version: 7201
I1006 03:22:01.930879  5661 caffe.cpp:905] CuBLAS version: 9000
I1006 03:22:01.930881  5661 caffe.cpp:906] CUDA version: 9000
I1006 03:22:01.930884  5661 caffe.cpp:907] CUDA driver version: 9020
I1006 03:22:01.930888  5661 caffe.cpp:908] Arguments: 
[0]: /home/b920405/git/caffe-jacinto/build/tools/caffe.bin
[1]: train
[2]: --solver=training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/solver.prototxt
[3]: --weights=training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/l1reg/cityscapes5_jsegnet21v2_iter_60000.caffemodel
[4]: --gpu
[5]: 0
I1006 03:22:01.970507  5661 gpu_memory.cpp:105] GPUMemory::Manager initialized
I1006 03:22:01.970948  5661 gpu_memory.cpp:107] Total memory: 8513978368, Free: 8149401600, dev_info[0]: total=8513978368 free=8149401600
I1006 03:22:01.970957  5661 caffe.cpp:226] Using GPUs 0
I1006 03:22:01.971235  5661 caffe.cpp:230] GPU 0: GeForce GTX 1070 with Max-Q Design
I1006 03:22:01.971292  5661 solver.cpp:41] Solver data type: FLOAT
I1006 03:22:01.977784  5661 solver.cpp:44] Initializing solver from parameters: 
train_net: "training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/train.prototxt"
test_net: "training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/test.prototxt"
test_iter: 500
test_interval: 2000
base_lr: 0.01
display: 100
max_iter: 60000
lr_policy: "multistep"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 1e-05
snapshot: 10000
snapshot_prefix: "training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2"
solver_mode: GPU
device_id: 0
random_seed: 33
debug_info: false
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
regularization_type: "L1"
test_initialization: false
stepvalue: 30000
stepvalue: 45000
iter_size: 2
type: "SGD"
display_sparsity: 1000
sparse_mode: SPARSE_UPDATE
sparsity_target: 0.8
sparsity_step_factor: 0.01
sparsity_step_iter: 1000
sparsity_start_iter: 1000
sparsity_start_factor: 0.6
I1006 03:22:01.977879  5661 solver.cpp:76] Creating training net from train_net file: training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/train.prototxt
I1006 03:22:01.978631  5661 net.cpp:457] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top1
I1006 03:22:01.978641  5661 net.cpp:457] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy/top5
I1006 03:22:01.979044  5661 net.cpp:80] Initializing net from parameters: 
name: "jsegnet21v2_train"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: true
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/train-image-lmdb"
    label_list_path: "data/train-label-lmdb"
    batch_size: 8
    shuffle: true
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
I1006 03:22:01.979347  5661 net.cpp:110] Using FLOAT as default forward math type
I1006 03:22:01.979359  5661 net.cpp:116] Using FLOAT as default backward math type
I1006 03:22:01.979365  5661 layer_factory.hpp:172] Creating layer 'data' of type 'ImageLabelData'
I1006 03:22:01.979372  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:01.979388  5661 net.cpp:200] Created Layer data (0)
I1006 03:22:01.979398  5661 net.cpp:542] data -> data
I1006 03:22:01.979425  5661 net.cpp:542] data -> label
I1006 03:22:01.979499  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:01.979977  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:01.979981  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 03:22:01.980352  5661 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 8
I1006 03:22:01.980408  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:01.981088  5682 db_lmdb.cpp:36] Opened lmdb data/train-image-lmdb
I1006 03:22:01.999236  5661 data_layer.cpp:199] [0] Output data size: 8, 3, 640, 640
I1006 03:22:01.999269  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:01.999320  5661 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 8
I1006 03:22:01.999336  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:01.999866  5683 data_layer.cpp:105] [0] Parser threads: 1
I1006 03:22:01.999876  5683 data_layer.cpp:107] [0] Transformer threads: 1
I1006 03:22:02.000669  5684 db_lmdb.cpp:36] Opened lmdb data/train-label-lmdb
I1006 03:22:02.012998  5661 data_layer.cpp:199] [0] Output data size: 8, 1, 640, 640
I1006 03:22:02.013059  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:02.013289  5661 net.cpp:260] Setting up data
I1006 03:22:02.013308  5661 net.cpp:267] TRAIN Top shape for layer 0 'data' 8 3 640 640 (9830400)
I1006 03:22:02.013315  5661 net.cpp:267] TRAIN Top shape for layer 0 'data' 8 1 640 640 (3276800)
I1006 03:22:02.013324  5661 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I1006 03:22:02.013331  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.013355  5661 net.cpp:200] Created Layer data/bias (1)
I1006 03:22:02.013365  5661 net.cpp:572] data/bias <- data
I1006 03:22:02.013383  5661 net.cpp:542] data/bias -> data/bias
I1006 03:22:02.013963  5685 data_layer.cpp:105] [0] Parser threads: 1
I1006 03:22:02.013991  5685 data_layer.cpp:107] [0] Transformer threads: 1
I1006 03:22:02.014984  5661 net.cpp:260] Setting up data/bias
I1006 03:22:02.015027  5661 net.cpp:267] TRAIN Top shape for layer 1 'data/bias' 8 3 640 640 (9830400)
I1006 03:22:02.015061  5661 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I1006 03:22:02.015101  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.015159  5661 net.cpp:200] Created Layer conv1a (2)
I1006 03:22:02.015167  5661 net.cpp:572] conv1a <- data/bias
I1006 03:22:02.015174  5661 net.cpp:542] conv1a -> conv1a
I1006 03:22:02.020406  5683 blocking_queue.cpp:40] Waiting for datum
I1006 03:22:02.584179  5661 net.cpp:260] Setting up conv1a
I1006 03:22:02.584206  5661 net.cpp:267] TRAIN Top shape for layer 2 'conv1a' 8 32 320 320 (26214400)
I1006 03:22:02.584223  5661 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I1006 03:22:02.584229  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.584244  5661 net.cpp:200] Created Layer conv1a/bn (3)
I1006 03:22:02.584247  5661 net.cpp:572] conv1a/bn <- conv1a
I1006 03:22:02.584254  5661 net.cpp:527] conv1a/bn -> conv1a (in-place)
I1006 03:22:02.585316  5661 net.cpp:260] Setting up conv1a/bn
I1006 03:22:02.585330  5661 net.cpp:267] TRAIN Top shape for layer 3 'conv1a/bn' 8 32 320 320 (26214400)
I1006 03:22:02.585343  5661 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I1006 03:22:02.585347  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.585355  5661 net.cpp:200] Created Layer conv1a/relu (4)
I1006 03:22:02.585359  5661 net.cpp:572] conv1a/relu <- conv1a
I1006 03:22:02.585363  5661 net.cpp:527] conv1a/relu -> conv1a (in-place)
I1006 03:22:02.585377  5661 net.cpp:260] Setting up conv1a/relu
I1006 03:22:02.585383  5661 net.cpp:267] TRAIN Top shape for layer 4 'conv1a/relu' 8 32 320 320 (26214400)
I1006 03:22:02.585387  5661 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I1006 03:22:02.585392  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.585407  5661 net.cpp:200] Created Layer conv1b (5)
I1006 03:22:02.585410  5661 net.cpp:572] conv1b <- conv1a
I1006 03:22:02.585414  5661 net.cpp:542] conv1b -> conv1b
I1006 03:22:02.586205  5661 net.cpp:260] Setting up conv1b
I1006 03:22:02.586216  5661 net.cpp:267] TRAIN Top shape for layer 5 'conv1b' 8 32 320 320 (26214400)
I1006 03:22:02.586225  5661 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I1006 03:22:02.586230  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.586237  5661 net.cpp:200] Created Layer conv1b/bn (6)
I1006 03:22:02.586241  5661 net.cpp:572] conv1b/bn <- conv1b
I1006 03:22:02.586244  5661 net.cpp:527] conv1b/bn -> conv1b (in-place)
I1006 03:22:02.586638  5661 net.cpp:260] Setting up conv1b/bn
I1006 03:22:02.586645  5661 net.cpp:267] TRAIN Top shape for layer 6 'conv1b/bn' 8 32 320 320 (26214400)
I1006 03:22:02.586655  5661 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I1006 03:22:02.586658  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.586663  5661 net.cpp:200] Created Layer conv1b/relu (7)
I1006 03:22:02.586666  5661 net.cpp:572] conv1b/relu <- conv1b
I1006 03:22:02.586670  5661 net.cpp:527] conv1b/relu -> conv1b (in-place)
I1006 03:22:02.586675  5661 net.cpp:260] Setting up conv1b/relu
I1006 03:22:02.586679  5661 net.cpp:267] TRAIN Top shape for layer 7 'conv1b/relu' 8 32 320 320 (26214400)
I1006 03:22:02.586683  5661 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I1006 03:22:02.586686  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.586700  5661 net.cpp:200] Created Layer pool1 (8)
I1006 03:22:02.586704  5661 net.cpp:572] pool1 <- conv1b
I1006 03:22:02.586709  5661 net.cpp:542] pool1 -> pool1
I1006 03:22:02.586769  5661 net.cpp:260] Setting up pool1
I1006 03:22:02.586776  5661 net.cpp:267] TRAIN Top shape for layer 8 'pool1' 8 32 160 160 (6553600)
I1006 03:22:02.586781  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I1006 03:22:02.586797  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.586808  5661 net.cpp:200] Created Layer res2a_branch2a (9)
I1006 03:22:02.586812  5661 net.cpp:572] res2a_branch2a <- pool1
I1006 03:22:02.586817  5661 net.cpp:542] res2a_branch2a -> res2a_branch2a
I1006 03:22:02.587792  5661 net.cpp:260] Setting up res2a_branch2a
I1006 03:22:02.587805  5661 net.cpp:267] TRAIN Top shape for layer 9 'res2a_branch2a' 8 64 160 160 (13107200)
I1006 03:22:02.587815  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I1006 03:22:02.587819  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.587828  5661 net.cpp:200] Created Layer res2a_branch2a/bn (10)
I1006 03:22:02.587832  5661 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I1006 03:22:02.587836  5661 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I1006 03:22:02.588763  5661 net.cpp:260] Setting up res2a_branch2a/bn
I1006 03:22:02.588775  5661 net.cpp:267] TRAIN Top shape for layer 10 'res2a_branch2a/bn' 8 64 160 160 (13107200)
I1006 03:22:02.588786  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I1006 03:22:02.588791  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.588798  5661 net.cpp:200] Created Layer res2a_branch2a/relu (11)
I1006 03:22:02.588801  5661 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I1006 03:22:02.588805  5661 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I1006 03:22:02.588811  5661 net.cpp:260] Setting up res2a_branch2a/relu
I1006 03:22:02.588816  5661 net.cpp:267] TRAIN Top shape for layer 11 'res2a_branch2a/relu' 8 64 160 160 (13107200)
I1006 03:22:02.588821  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I1006 03:22:02.588825  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.588835  5661 net.cpp:200] Created Layer res2a_branch2b (12)
I1006 03:22:02.588840  5661 net.cpp:572] res2a_branch2b <- res2a_branch2a
I1006 03:22:02.588842  5661 net.cpp:542] res2a_branch2b -> res2a_branch2b
I1006 03:22:02.589738  5661 net.cpp:260] Setting up res2a_branch2b
I1006 03:22:02.589751  5661 net.cpp:267] TRAIN Top shape for layer 12 'res2a_branch2b' 8 64 160 160 (13107200)
I1006 03:22:02.589758  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I1006 03:22:02.589763  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.589771  5661 net.cpp:200] Created Layer res2a_branch2b/bn (13)
I1006 03:22:02.589776  5661 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I1006 03:22:02.589781  5661 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I1006 03:22:02.590113  5661 net.cpp:260] Setting up res2a_branch2b/bn
I1006 03:22:02.590121  5661 net.cpp:267] TRAIN Top shape for layer 13 'res2a_branch2b/bn' 8 64 160 160 (13107200)
I1006 03:22:02.590131  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I1006 03:22:02.590135  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.590140  5661 net.cpp:200] Created Layer res2a_branch2b/relu (14)
I1006 03:22:02.590144  5661 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I1006 03:22:02.590147  5661 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I1006 03:22:02.590153  5661 net.cpp:260] Setting up res2a_branch2b/relu
I1006 03:22:02.590157  5661 net.cpp:267] TRAIN Top shape for layer 14 'res2a_branch2b/relu' 8 64 160 160 (13107200)
I1006 03:22:02.590162  5661 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I1006 03:22:02.590167  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.590173  5661 net.cpp:200] Created Layer pool2 (15)
I1006 03:22:02.590176  5661 net.cpp:572] pool2 <- res2a_branch2b
I1006 03:22:02.590191  5661 net.cpp:542] pool2 -> pool2
I1006 03:22:02.590240  5661 net.cpp:260] Setting up pool2
I1006 03:22:02.590246  5661 net.cpp:267] TRAIN Top shape for layer 15 'pool2' 8 64 80 80 (3276800)
I1006 03:22:02.590250  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I1006 03:22:02.590255  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.590267  5661 net.cpp:200] Created Layer res3a_branch2a (16)
I1006 03:22:02.590271  5661 net.cpp:572] res3a_branch2a <- pool2
I1006 03:22:02.590276  5661 net.cpp:542] res3a_branch2a -> res3a_branch2a
I1006 03:22:02.591157  5661 net.cpp:260] Setting up res3a_branch2a
I1006 03:22:02.591166  5661 net.cpp:267] TRAIN Top shape for layer 16 'res3a_branch2a' 8 128 80 80 (6553600)
I1006 03:22:02.591173  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I1006 03:22:02.591177  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.591187  5661 net.cpp:200] Created Layer res3a_branch2a/bn (17)
I1006 03:22:02.591190  5661 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I1006 03:22:02.591194  5661 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I1006 03:22:02.591485  5661 net.cpp:260] Setting up res3a_branch2a/bn
I1006 03:22:02.591492  5661 net.cpp:267] TRAIN Top shape for layer 17 'res3a_branch2a/bn' 8 128 80 80 (6553600)
I1006 03:22:02.591504  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I1006 03:22:02.591509  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.591513  5661 net.cpp:200] Created Layer res3a_branch2a/relu (18)
I1006 03:22:02.591517  5661 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I1006 03:22:02.591521  5661 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I1006 03:22:02.591526  5661 net.cpp:260] Setting up res3a_branch2a/relu
I1006 03:22:02.591531  5661 net.cpp:267] TRAIN Top shape for layer 18 'res3a_branch2a/relu' 8 128 80 80 (6553600)
I1006 03:22:02.591536  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I1006 03:22:02.591539  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.591549  5661 net.cpp:200] Created Layer res3a_branch2b (19)
I1006 03:22:02.591553  5661 net.cpp:572] res3a_branch2b <- res3a_branch2a
I1006 03:22:02.591557  5661 net.cpp:542] res3a_branch2b -> res3a_branch2b
I1006 03:22:02.592100  5661 net.cpp:260] Setting up res3a_branch2b
I1006 03:22:02.592109  5661 net.cpp:267] TRAIN Top shape for layer 19 'res3a_branch2b' 8 128 80 80 (6553600)
I1006 03:22:02.592116  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I1006 03:22:02.592120  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.592128  5661 net.cpp:200] Created Layer res3a_branch2b/bn (20)
I1006 03:22:02.592131  5661 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I1006 03:22:02.592134  5661 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I1006 03:22:02.592427  5661 net.cpp:260] Setting up res3a_branch2b/bn
I1006 03:22:02.592434  5661 net.cpp:267] TRAIN Top shape for layer 20 'res3a_branch2b/bn' 8 128 80 80 (6553600)
I1006 03:22:02.592443  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I1006 03:22:02.592447  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.592453  5661 net.cpp:200] Created Layer res3a_branch2b/relu (21)
I1006 03:22:02.592456  5661 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I1006 03:22:02.592459  5661 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I1006 03:22:02.592465  5661 net.cpp:260] Setting up res3a_branch2b/relu
I1006 03:22:02.592469  5661 net.cpp:267] TRAIN Top shape for layer 21 'res3a_branch2b/relu' 8 128 80 80 (6553600)
I1006 03:22:02.592474  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I1006 03:22:02.592486  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.592494  5661 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (22)
I1006 03:22:02.592499  5661 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I1006 03:22:02.592502  5661 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I1006 03:22:02.592509  5661 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I1006 03:22:02.592557  5661 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I1006 03:22:02.592566  5661 net.cpp:267] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 80 80 (6553600)
I1006 03:22:02.592569  5661 net.cpp:267] TRAIN Top shape for layer 22 'res3a_branch2b_res3a_branch2b/relu_0_split' 8 128 80 80 (6553600)
I1006 03:22:02.592574  5661 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I1006 03:22:02.592579  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.592587  5661 net.cpp:200] Created Layer pool3 (23)
I1006 03:22:02.592592  5661 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I1006 03:22:02.592595  5661 net.cpp:542] pool3 -> pool3
I1006 03:22:02.592643  5661 net.cpp:260] Setting up pool3
I1006 03:22:02.592649  5661 net.cpp:267] TRAIN Top shape for layer 23 'pool3' 8 128 40 40 (1638400)
I1006 03:22:02.592654  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I1006 03:22:02.592658  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.592669  5661 net.cpp:200] Created Layer res4a_branch2a (24)
I1006 03:22:02.592672  5661 net.cpp:572] res4a_branch2a <- pool3
I1006 03:22:02.592676  5661 net.cpp:542] res4a_branch2a -> res4a_branch2a
I1006 03:22:02.596223  5661 net.cpp:260] Setting up res4a_branch2a
I1006 03:22:02.596240  5661 net.cpp:267] TRAIN Top shape for layer 24 'res4a_branch2a' 8 256 40 40 (3276800)
I1006 03:22:02.596254  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I1006 03:22:02.596261  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.596274  5661 net.cpp:200] Created Layer res4a_branch2a/bn (25)
I1006 03:22:02.596282  5661 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I1006 03:22:02.596289  5661 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I1006 03:22:02.596611  5661 net.cpp:260] Setting up res4a_branch2a/bn
I1006 03:22:02.596621  5661 net.cpp:267] TRAIN Top shape for layer 25 'res4a_branch2a/bn' 8 256 40 40 (3276800)
I1006 03:22:02.596630  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I1006 03:22:02.596635  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.596640  5661 net.cpp:200] Created Layer res4a_branch2a/relu (26)
I1006 03:22:02.596644  5661 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I1006 03:22:02.596648  5661 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I1006 03:22:02.596654  5661 net.cpp:260] Setting up res4a_branch2a/relu
I1006 03:22:02.596659  5661 net.cpp:267] TRAIN Top shape for layer 26 'res4a_branch2a/relu' 8 256 40 40 (3276800)
I1006 03:22:02.596663  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I1006 03:22:02.596668  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.596683  5661 net.cpp:200] Created Layer res4a_branch2b (27)
I1006 03:22:02.596688  5661 net.cpp:572] res4a_branch2b <- res4a_branch2a
I1006 03:22:02.596691  5661 net.cpp:542] res4a_branch2b -> res4a_branch2b
I1006 03:22:02.598268  5661 net.cpp:260] Setting up res4a_branch2b
I1006 03:22:02.598276  5661 net.cpp:267] TRAIN Top shape for layer 27 'res4a_branch2b' 8 256 40 40 (3276800)
I1006 03:22:02.598295  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I1006 03:22:02.598301  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.598309  5661 net.cpp:200] Created Layer res4a_branch2b/bn (28)
I1006 03:22:02.598314  5661 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I1006 03:22:02.598317  5661 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I1006 03:22:02.598635  5661 net.cpp:260] Setting up res4a_branch2b/bn
I1006 03:22:02.598644  5661 net.cpp:267] TRAIN Top shape for layer 28 'res4a_branch2b/bn' 8 256 40 40 (3276800)
I1006 03:22:02.598652  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I1006 03:22:02.598656  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.598661  5661 net.cpp:200] Created Layer res4a_branch2b/relu (29)
I1006 03:22:02.598665  5661 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I1006 03:22:02.598670  5661 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I1006 03:22:02.598675  5661 net.cpp:260] Setting up res4a_branch2b/relu
I1006 03:22:02.598680  5661 net.cpp:267] TRAIN Top shape for layer 29 'res4a_branch2b/relu' 8 256 40 40 (3276800)
I1006 03:22:02.598685  5661 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I1006 03:22:02.598688  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.598695  5661 net.cpp:200] Created Layer pool4 (30)
I1006 03:22:02.598700  5661 net.cpp:572] pool4 <- res4a_branch2b
I1006 03:22:02.598703  5661 net.cpp:542] pool4 -> pool4
I1006 03:22:02.598755  5661 net.cpp:260] Setting up pool4
I1006 03:22:02.598762  5661 net.cpp:267] TRAIN Top shape for layer 30 'pool4' 8 256 40 40 (3276800)
I1006 03:22:02.598767  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I1006 03:22:02.598773  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.598788  5661 net.cpp:200] Created Layer res5a_branch2a (31)
I1006 03:22:02.598793  5661 net.cpp:572] res5a_branch2a <- pool4
I1006 03:22:02.598796  5661 net.cpp:542] res5a_branch2a -> res5a_branch2a
I1006 03:22:02.611061  5661 net.cpp:260] Setting up res5a_branch2a
I1006 03:22:02.611078  5661 net.cpp:267] TRAIN Top shape for layer 31 'res5a_branch2a' 8 512 40 40 (6553600)
I1006 03:22:02.611085  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I1006 03:22:02.611090  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.611102  5661 net.cpp:200] Created Layer res5a_branch2a/bn (32)
I1006 03:22:02.611107  5661 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I1006 03:22:02.611114  5661 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I1006 03:22:02.611460  5661 net.cpp:260] Setting up res5a_branch2a/bn
I1006 03:22:02.611469  5661 net.cpp:267] TRAIN Top shape for layer 32 'res5a_branch2a/bn' 8 512 40 40 (6553600)
I1006 03:22:02.611479  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I1006 03:22:02.611485  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.611491  5661 net.cpp:200] Created Layer res5a_branch2a/relu (33)
I1006 03:22:02.611496  5661 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I1006 03:22:02.611500  5661 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I1006 03:22:02.611506  5661 net.cpp:260] Setting up res5a_branch2a/relu
I1006 03:22:02.611510  5661 net.cpp:267] TRAIN Top shape for layer 33 'res5a_branch2a/relu' 8 512 40 40 (6553600)
I1006 03:22:02.611516  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I1006 03:22:02.611521  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.611534  5661 net.cpp:200] Created Layer res5a_branch2b (34)
I1006 03:22:02.611548  5661 net.cpp:572] res5a_branch2b <- res5a_branch2a
I1006 03:22:02.611552  5661 net.cpp:542] res5a_branch2b -> res5a_branch2b
I1006 03:22:02.618120  5661 net.cpp:260] Setting up res5a_branch2b
I1006 03:22:02.618137  5661 net.cpp:267] TRAIN Top shape for layer 34 'res5a_branch2b' 8 512 40 40 (6553600)
I1006 03:22:02.618155  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I1006 03:22:02.618160  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.618170  5661 net.cpp:200] Created Layer res5a_branch2b/bn (35)
I1006 03:22:02.618175  5661 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I1006 03:22:02.618180  5661 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I1006 03:22:02.618538  5661 net.cpp:260] Setting up res5a_branch2b/bn
I1006 03:22:02.618548  5661 net.cpp:267] TRAIN Top shape for layer 35 'res5a_branch2b/bn' 8 512 40 40 (6553600)
I1006 03:22:02.618558  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I1006 03:22:02.618563  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.618569  5661 net.cpp:200] Created Layer res5a_branch2b/relu (36)
I1006 03:22:02.618573  5661 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I1006 03:22:02.618577  5661 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I1006 03:22:02.618583  5661 net.cpp:260] Setting up res5a_branch2b/relu
I1006 03:22:02.618590  5661 net.cpp:267] TRAIN Top shape for layer 36 'res5a_branch2b/relu' 8 512 40 40 (6553600)
I1006 03:22:02.618594  5661 layer_factory.hpp:172] Creating layer 'out5a' of type 'Convolution'
I1006 03:22:02.618599  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.618614  5661 net.cpp:200] Created Layer out5a (37)
I1006 03:22:02.618619  5661 net.cpp:572] out5a <- res5a_branch2b
I1006 03:22:02.618623  5661 net.cpp:542] out5a -> out5a
I1006 03:22:02.620928  5661 net.cpp:260] Setting up out5a
I1006 03:22:02.620940  5661 net.cpp:267] TRAIN Top shape for layer 37 'out5a' 8 64 40 40 (819200)
I1006 03:22:02.620949  5661 layer_factory.hpp:172] Creating layer 'out5a/bn' of type 'BatchNorm'
I1006 03:22:02.620954  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.620963  5661 net.cpp:200] Created Layer out5a/bn (38)
I1006 03:22:02.620968  5661 net.cpp:572] out5a/bn <- out5a
I1006 03:22:02.620972  5661 net.cpp:527] out5a/bn -> out5a (in-place)
I1006 03:22:02.621340  5661 net.cpp:260] Setting up out5a/bn
I1006 03:22:02.621351  5661 net.cpp:267] TRAIN Top shape for layer 38 'out5a/bn' 8 64 40 40 (819200)
I1006 03:22:02.621361  5661 layer_factory.hpp:172] Creating layer 'out5a/relu' of type 'ReLU'
I1006 03:22:02.621364  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.621371  5661 net.cpp:200] Created Layer out5a/relu (39)
I1006 03:22:02.621374  5661 net.cpp:572] out5a/relu <- out5a
I1006 03:22:02.621377  5661 net.cpp:527] out5a/relu -> out5a (in-place)
I1006 03:22:02.621383  5661 net.cpp:260] Setting up out5a/relu
I1006 03:22:02.621388  5661 net.cpp:267] TRAIN Top shape for layer 39 'out5a/relu' 8 64 40 40 (819200)
I1006 03:22:02.621393  5661 layer_factory.hpp:172] Creating layer 'out5a_up2' of type 'Deconvolution'
I1006 03:22:02.621397  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.621412  5661 net.cpp:200] Created Layer out5a_up2 (40)
I1006 03:22:02.621417  5661 net.cpp:572] out5a_up2 <- out5a
I1006 03:22:02.621420  5661 net.cpp:542] out5a_up2 -> out5a_up2
I1006 03:22:02.621650  5661 net.cpp:260] Setting up out5a_up2
I1006 03:22:02.621659  5661 net.cpp:267] TRAIN Top shape for layer 40 'out5a_up2' 8 64 80 80 (3276800)
I1006 03:22:02.621665  5661 layer_factory.hpp:172] Creating layer 'out3a' of type 'Convolution'
I1006 03:22:02.621670  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.621692  5661 net.cpp:200] Created Layer out3a (41)
I1006 03:22:02.621698  5661 net.cpp:572] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I1006 03:22:02.621703  5661 net.cpp:542] out3a -> out3a
I1006 03:22:02.622299  5661 net.cpp:260] Setting up out3a
I1006 03:22:02.622309  5661 net.cpp:267] TRAIN Top shape for layer 41 'out3a' 8 64 80 80 (3276800)
I1006 03:22:02.622318  5661 layer_factory.hpp:172] Creating layer 'out3a/bn' of type 'BatchNorm'
I1006 03:22:02.622321  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.622328  5661 net.cpp:200] Created Layer out3a/bn (42)
I1006 03:22:02.622332  5661 net.cpp:572] out3a/bn <- out3a
I1006 03:22:02.622336  5661 net.cpp:527] out3a/bn -> out3a (in-place)
I1006 03:22:02.622699  5661 net.cpp:260] Setting up out3a/bn
I1006 03:22:02.622709  5661 net.cpp:267] TRAIN Top shape for layer 42 'out3a/bn' 8 64 80 80 (3276800)
I1006 03:22:02.622717  5661 layer_factory.hpp:172] Creating layer 'out3a/relu' of type 'ReLU'
I1006 03:22:02.622722  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.622727  5661 net.cpp:200] Created Layer out3a/relu (43)
I1006 03:22:02.622731  5661 net.cpp:572] out3a/relu <- out3a
I1006 03:22:02.622735  5661 net.cpp:527] out3a/relu -> out3a (in-place)
I1006 03:22:02.622740  5661 net.cpp:260] Setting up out3a/relu
I1006 03:22:02.622746  5661 net.cpp:267] TRAIN Top shape for layer 43 'out3a/relu' 8 64 80 80 (3276800)
I1006 03:22:02.622751  5661 layer_factory.hpp:172] Creating layer 'out3_out5_combined' of type 'Eltwise'
I1006 03:22:02.622756  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.622766  5661 net.cpp:200] Created Layer out3_out5_combined (44)
I1006 03:22:02.622771  5661 net.cpp:572] out3_out5_combined <- out5a_up2
I1006 03:22:02.622774  5661 net.cpp:572] out3_out5_combined <- out3a
I1006 03:22:02.622779  5661 net.cpp:542] out3_out5_combined -> out3_out5_combined
I1006 03:22:02.622807  5661 net.cpp:260] Setting up out3_out5_combined
I1006 03:22:02.622812  5661 net.cpp:267] TRAIN Top shape for layer 44 'out3_out5_combined' 8 64 80 80 (3276800)
I1006 03:22:02.622817  5661 layer_factory.hpp:172] Creating layer 'ctx_conv1' of type 'Convolution'
I1006 03:22:02.622822  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.622838  5661 net.cpp:200] Created Layer ctx_conv1 (45)
I1006 03:22:02.622841  5661 net.cpp:572] ctx_conv1 <- out3_out5_combined
I1006 03:22:02.622845  5661 net.cpp:542] ctx_conv1 -> ctx_conv1
I1006 03:22:02.623440  5661 net.cpp:260] Setting up ctx_conv1
I1006 03:22:02.623452  5661 net.cpp:267] TRAIN Top shape for layer 45 'ctx_conv1' 8 64 80 80 (3276800)
I1006 03:22:02.623463  5661 layer_factory.hpp:172] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I1006 03:22:02.623471  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.623481  5661 net.cpp:200] Created Layer ctx_conv1/bn (46)
I1006 03:22:02.623486  5661 net.cpp:572] ctx_conv1/bn <- ctx_conv1
I1006 03:22:02.623491  5661 net.cpp:527] ctx_conv1/bn -> ctx_conv1 (in-place)
I1006 03:22:02.623859  5661 net.cpp:260] Setting up ctx_conv1/bn
I1006 03:22:02.623867  5661 net.cpp:267] TRAIN Top shape for layer 46 'ctx_conv1/bn' 8 64 80 80 (3276800)
I1006 03:22:02.623877  5661 layer_factory.hpp:172] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I1006 03:22:02.623880  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.623886  5661 net.cpp:200] Created Layer ctx_conv1/relu (47)
I1006 03:22:02.623891  5661 net.cpp:572] ctx_conv1/relu <- ctx_conv1
I1006 03:22:02.623895  5661 net.cpp:527] ctx_conv1/relu -> ctx_conv1 (in-place)
I1006 03:22:02.623900  5661 net.cpp:260] Setting up ctx_conv1/relu
I1006 03:22:02.623905  5661 net.cpp:267] TRAIN Top shape for layer 47 'ctx_conv1/relu' 8 64 80 80 (3276800)
I1006 03:22:02.623919  5661 layer_factory.hpp:172] Creating layer 'ctx_conv2' of type 'Convolution'
I1006 03:22:02.623922  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.623939  5661 net.cpp:200] Created Layer ctx_conv2 (48)
I1006 03:22:02.623942  5661 net.cpp:572] ctx_conv2 <- ctx_conv1
I1006 03:22:02.623947  5661 net.cpp:542] ctx_conv2 -> ctx_conv2
I1006 03:22:02.624552  5661 net.cpp:260] Setting up ctx_conv2
I1006 03:22:02.624562  5661 net.cpp:267] TRAIN Top shape for layer 48 'ctx_conv2' 8 64 80 80 (3276800)
I1006 03:22:02.624569  5661 layer_factory.hpp:172] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I1006 03:22:02.624573  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.624583  5661 net.cpp:200] Created Layer ctx_conv2/bn (49)
I1006 03:22:02.624588  5661 net.cpp:572] ctx_conv2/bn <- ctx_conv2
I1006 03:22:02.624593  5661 net.cpp:527] ctx_conv2/bn -> ctx_conv2 (in-place)
I1006 03:22:02.624954  5661 net.cpp:260] Setting up ctx_conv2/bn
I1006 03:22:02.624963  5661 net.cpp:267] TRAIN Top shape for layer 49 'ctx_conv2/bn' 8 64 80 80 (3276800)
I1006 03:22:02.624971  5661 layer_factory.hpp:172] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I1006 03:22:02.624976  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.624980  5661 net.cpp:200] Created Layer ctx_conv2/relu (50)
I1006 03:22:02.624984  5661 net.cpp:572] ctx_conv2/relu <- ctx_conv2
I1006 03:22:02.624989  5661 net.cpp:527] ctx_conv2/relu -> ctx_conv2 (in-place)
I1006 03:22:02.624995  5661 net.cpp:260] Setting up ctx_conv2/relu
I1006 03:22:02.625000  5661 net.cpp:267] TRAIN Top shape for layer 50 'ctx_conv2/relu' 8 64 80 80 (3276800)
I1006 03:22:02.625005  5661 layer_factory.hpp:172] Creating layer 'ctx_conv3' of type 'Convolution'
I1006 03:22:02.625010  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.625022  5661 net.cpp:200] Created Layer ctx_conv3 (51)
I1006 03:22:02.625027  5661 net.cpp:572] ctx_conv3 <- ctx_conv2
I1006 03:22:02.625031  5661 net.cpp:542] ctx_conv3 -> ctx_conv3
I1006 03:22:02.625627  5661 net.cpp:260] Setting up ctx_conv3
I1006 03:22:02.625636  5661 net.cpp:267] TRAIN Top shape for layer 51 'ctx_conv3' 8 64 80 80 (3276800)
I1006 03:22:02.625643  5661 layer_factory.hpp:172] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I1006 03:22:02.625648  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.625655  5661 net.cpp:200] Created Layer ctx_conv3/bn (52)
I1006 03:22:02.625660  5661 net.cpp:572] ctx_conv3/bn <- ctx_conv3
I1006 03:22:02.625664  5661 net.cpp:527] ctx_conv3/bn -> ctx_conv3 (in-place)
I1006 03:22:02.626036  5661 net.cpp:260] Setting up ctx_conv3/bn
I1006 03:22:02.626044  5661 net.cpp:267] TRAIN Top shape for layer 52 'ctx_conv3/bn' 8 64 80 80 (3276800)
I1006 03:22:02.626055  5661 layer_factory.hpp:172] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I1006 03:22:02.626058  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.626065  5661 net.cpp:200] Created Layer ctx_conv3/relu (53)
I1006 03:22:02.626070  5661 net.cpp:572] ctx_conv3/relu <- ctx_conv3
I1006 03:22:02.626073  5661 net.cpp:527] ctx_conv3/relu -> ctx_conv3 (in-place)
I1006 03:22:02.626078  5661 net.cpp:260] Setting up ctx_conv3/relu
I1006 03:22:02.626083  5661 net.cpp:267] TRAIN Top shape for layer 53 'ctx_conv3/relu' 8 64 80 80 (3276800)
I1006 03:22:02.626088  5661 layer_factory.hpp:172] Creating layer 'ctx_conv4' of type 'Convolution'
I1006 03:22:02.626092  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.626106  5661 net.cpp:200] Created Layer ctx_conv4 (54)
I1006 03:22:02.626111  5661 net.cpp:572] ctx_conv4 <- ctx_conv3
I1006 03:22:02.626116  5661 net.cpp:542] ctx_conv4 -> ctx_conv4
I1006 03:22:02.626704  5661 net.cpp:260] Setting up ctx_conv4
I1006 03:22:02.626724  5661 net.cpp:267] TRAIN Top shape for layer 54 'ctx_conv4' 8 64 80 80 (3276800)
I1006 03:22:02.626731  5661 layer_factory.hpp:172] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I1006 03:22:02.626735  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.626742  5661 net.cpp:200] Created Layer ctx_conv4/bn (55)
I1006 03:22:02.626746  5661 net.cpp:572] ctx_conv4/bn <- ctx_conv4
I1006 03:22:02.626750  5661 net.cpp:527] ctx_conv4/bn -> ctx_conv4 (in-place)
I1006 03:22:02.627120  5661 net.cpp:260] Setting up ctx_conv4/bn
I1006 03:22:02.627128  5661 net.cpp:267] TRAIN Top shape for layer 55 'ctx_conv4/bn' 8 64 80 80 (3276800)
I1006 03:22:02.627137  5661 layer_factory.hpp:172] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I1006 03:22:02.627142  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.627146  5661 net.cpp:200] Created Layer ctx_conv4/relu (56)
I1006 03:22:02.627151  5661 net.cpp:572] ctx_conv4/relu <- ctx_conv4
I1006 03:22:02.627154  5661 net.cpp:527] ctx_conv4/relu -> ctx_conv4 (in-place)
I1006 03:22:02.627161  5661 net.cpp:260] Setting up ctx_conv4/relu
I1006 03:22:02.627166  5661 net.cpp:267] TRAIN Top shape for layer 56 'ctx_conv4/relu' 8 64 80 80 (3276800)
I1006 03:22:02.627171  5661 layer_factory.hpp:172] Creating layer 'ctx_final' of type 'Convolution'
I1006 03:22:02.627176  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.627188  5661 net.cpp:200] Created Layer ctx_final (57)
I1006 03:22:02.627193  5661 net.cpp:572] ctx_final <- ctx_conv4
I1006 03:22:02.627197  5661 net.cpp:542] ctx_final -> ctx_final
I1006 03:22:02.627475  5661 net.cpp:260] Setting up ctx_final
I1006 03:22:02.627483  5661 net.cpp:267] TRAIN Top shape for layer 57 'ctx_final' 8 8 80 80 (409600)
I1006 03:22:02.627490  5661 layer_factory.hpp:172] Creating layer 'ctx_final/relu' of type 'ReLU'
I1006 03:22:02.627494  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.627499  5661 net.cpp:200] Created Layer ctx_final/relu (58)
I1006 03:22:02.627503  5661 net.cpp:572] ctx_final/relu <- ctx_final
I1006 03:22:02.627507  5661 net.cpp:527] ctx_final/relu -> ctx_final (in-place)
I1006 03:22:02.627513  5661 net.cpp:260] Setting up ctx_final/relu
I1006 03:22:02.627518  5661 net.cpp:267] TRAIN Top shape for layer 58 'ctx_final/relu' 8 8 80 80 (409600)
I1006 03:22:02.627523  5661 layer_factory.hpp:172] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I1006 03:22:02.627527  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.627539  5661 net.cpp:200] Created Layer out_deconv_final_up2 (59)
I1006 03:22:02.627543  5661 net.cpp:572] out_deconv_final_up2 <- ctx_final
I1006 03:22:02.627547  5661 net.cpp:542] out_deconv_final_up2 -> out_deconv_final_up2
I1006 03:22:02.627727  5661 net.cpp:260] Setting up out_deconv_final_up2
I1006 03:22:02.627733  5661 net.cpp:267] TRAIN Top shape for layer 59 'out_deconv_final_up2' 8 8 160 160 (1638400)
I1006 03:22:02.627740  5661 layer_factory.hpp:172] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I1006 03:22:02.627744  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.627755  5661 net.cpp:200] Created Layer out_deconv_final_up4 (60)
I1006 03:22:02.627759  5661 net.cpp:572] out_deconv_final_up4 <- out_deconv_final_up2
I1006 03:22:02.627763  5661 net.cpp:542] out_deconv_final_up4 -> out_deconv_final_up4
I1006 03:22:02.627939  5661 net.cpp:260] Setting up out_deconv_final_up4
I1006 03:22:02.627948  5661 net.cpp:267] TRAIN Top shape for layer 60 'out_deconv_final_up4' 8 8 320 320 (6553600)
I1006 03:22:02.627952  5661 layer_factory.hpp:172] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I1006 03:22:02.627957  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.627974  5661 net.cpp:200] Created Layer out_deconv_final_up8 (61)
I1006 03:22:02.627980  5661 net.cpp:572] out_deconv_final_up8 <- out_deconv_final_up4
I1006 03:22:02.627985  5661 net.cpp:542] out_deconv_final_up8 -> out_deconv_final_up8
I1006 03:22:02.628165  5661 net.cpp:260] Setting up out_deconv_final_up8
I1006 03:22:02.628172  5661 net.cpp:267] TRAIN Top shape for layer 61 'out_deconv_final_up8' 8 8 640 640 (26214400)
I1006 03:22:02.628178  5661 layer_factory.hpp:172] Creating layer 'loss' of type 'SoftmaxWithLoss'
I1006 03:22:02.628183  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.628195  5661 net.cpp:200] Created Layer loss (62)
I1006 03:22:02.628198  5661 net.cpp:572] loss <- out_deconv_final_up8
I1006 03:22:02.628202  5661 net.cpp:572] loss <- label
I1006 03:22:02.628208  5661 net.cpp:542] loss -> loss
I1006 03:22:02.668001  5661 net.cpp:260] Setting up loss
I1006 03:22:02.668030  5661 net.cpp:267] TRAIN Top shape for layer 62 'loss' (1)
I1006 03:22:02.668033  5661 net.cpp:271]     with loss weight 1
I1006 03:22:02.668054  5661 net.cpp:336] loss needs backward computation.
I1006 03:22:02.668061  5661 net.cpp:336] out_deconv_final_up8 needs backward computation.
I1006 03:22:02.668064  5661 net.cpp:336] out_deconv_final_up4 needs backward computation.
I1006 03:22:02.668067  5661 net.cpp:336] out_deconv_final_up2 needs backward computation.
I1006 03:22:02.668071  5661 net.cpp:336] ctx_final/relu needs backward computation.
I1006 03:22:02.668074  5661 net.cpp:336] ctx_final needs backward computation.
I1006 03:22:02.668077  5661 net.cpp:336] ctx_conv4/relu needs backward computation.
I1006 03:22:02.668081  5661 net.cpp:336] ctx_conv4/bn needs backward computation.
I1006 03:22:02.668083  5661 net.cpp:336] ctx_conv4 needs backward computation.
I1006 03:22:02.668087  5661 net.cpp:336] ctx_conv3/relu needs backward computation.
I1006 03:22:02.668090  5661 net.cpp:336] ctx_conv3/bn needs backward computation.
I1006 03:22:02.668093  5661 net.cpp:336] ctx_conv3 needs backward computation.
I1006 03:22:02.668097  5661 net.cpp:336] ctx_conv2/relu needs backward computation.
I1006 03:22:02.668099  5661 net.cpp:336] ctx_conv2/bn needs backward computation.
I1006 03:22:02.668102  5661 net.cpp:336] ctx_conv2 needs backward computation.
I1006 03:22:02.668105  5661 net.cpp:336] ctx_conv1/relu needs backward computation.
I1006 03:22:02.668108  5661 net.cpp:336] ctx_conv1/bn needs backward computation.
I1006 03:22:02.668112  5661 net.cpp:336] ctx_conv1 needs backward computation.
I1006 03:22:02.668118  5661 net.cpp:336] out3_out5_combined needs backward computation.
I1006 03:22:02.668123  5661 net.cpp:336] out3a/relu needs backward computation.
I1006 03:22:02.668128  5661 net.cpp:336] out3a/bn needs backward computation.
I1006 03:22:02.668131  5661 net.cpp:336] out3a needs backward computation.
I1006 03:22:02.668134  5661 net.cpp:336] out5a_up2 needs backward computation.
I1006 03:22:02.668139  5661 net.cpp:336] out5a/relu needs backward computation.
I1006 03:22:02.668143  5661 net.cpp:336] out5a/bn needs backward computation.
I1006 03:22:02.668146  5661 net.cpp:336] out5a needs backward computation.
I1006 03:22:02.668150  5661 net.cpp:336] res5a_branch2b/relu needs backward computation.
I1006 03:22:02.668154  5661 net.cpp:336] res5a_branch2b/bn needs backward computation.
I1006 03:22:02.668157  5661 net.cpp:336] res5a_branch2b needs backward computation.
I1006 03:22:02.668161  5661 net.cpp:336] res5a_branch2a/relu needs backward computation.
I1006 03:22:02.668164  5661 net.cpp:336] res5a_branch2a/bn needs backward computation.
I1006 03:22:02.668169  5661 net.cpp:336] res5a_branch2a needs backward computation.
I1006 03:22:02.668172  5661 net.cpp:336] pool4 needs backward computation.
I1006 03:22:02.668176  5661 net.cpp:336] res4a_branch2b/relu needs backward computation.
I1006 03:22:02.668179  5661 net.cpp:336] res4a_branch2b/bn needs backward computation.
I1006 03:22:02.668184  5661 net.cpp:336] res4a_branch2b needs backward computation.
I1006 03:22:02.668186  5661 net.cpp:336] res4a_branch2a/relu needs backward computation.
I1006 03:22:02.668201  5661 net.cpp:336] res4a_branch2a/bn needs backward computation.
I1006 03:22:02.668205  5661 net.cpp:336] res4a_branch2a needs backward computation.
I1006 03:22:02.668210  5661 net.cpp:336] pool3 needs backward computation.
I1006 03:22:02.668215  5661 net.cpp:336] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I1006 03:22:02.668218  5661 net.cpp:336] res3a_branch2b/relu needs backward computation.
I1006 03:22:02.668222  5661 net.cpp:336] res3a_branch2b/bn needs backward computation.
I1006 03:22:02.668226  5661 net.cpp:336] res3a_branch2b needs backward computation.
I1006 03:22:02.668231  5661 net.cpp:336] res3a_branch2a/relu needs backward computation.
I1006 03:22:02.668233  5661 net.cpp:336] res3a_branch2a/bn needs backward computation.
I1006 03:22:02.668237  5661 net.cpp:336] res3a_branch2a needs backward computation.
I1006 03:22:02.668241  5661 net.cpp:336] pool2 needs backward computation.
I1006 03:22:02.668244  5661 net.cpp:336] res2a_branch2b/relu needs backward computation.
I1006 03:22:02.668248  5661 net.cpp:336] res2a_branch2b/bn needs backward computation.
I1006 03:22:02.668251  5661 net.cpp:336] res2a_branch2b needs backward computation.
I1006 03:22:02.668256  5661 net.cpp:336] res2a_branch2a/relu needs backward computation.
I1006 03:22:02.668258  5661 net.cpp:336] res2a_branch2a/bn needs backward computation.
I1006 03:22:02.668262  5661 net.cpp:336] res2a_branch2a needs backward computation.
I1006 03:22:02.668267  5661 net.cpp:336] pool1 needs backward computation.
I1006 03:22:02.668269  5661 net.cpp:336] conv1b/relu needs backward computation.
I1006 03:22:02.668275  5661 net.cpp:336] conv1b/bn needs backward computation.
I1006 03:22:02.668282  5661 net.cpp:336] conv1b needs backward computation.
I1006 03:22:02.668287  5661 net.cpp:336] conv1a/relu needs backward computation.
I1006 03:22:02.668292  5661 net.cpp:336] conv1a/bn needs backward computation.
I1006 03:22:02.668295  5661 net.cpp:336] conv1a needs backward computation.
I1006 03:22:02.668304  5661 net.cpp:338] data/bias does not need backward computation.
I1006 03:22:02.668310  5661 net.cpp:338] data does not need backward computation.
I1006 03:22:02.668313  5661 net.cpp:380] This network produces output loss
I1006 03:22:02.668390  5661 net.cpp:403] Top memory (TRAIN) required for data: 1913651208 diff: 1913651208
I1006 03:22:02.668395  5661 net.cpp:406] Bottom memory (TRAIN) required for data: 1913651200 diff: 1913651200
I1006 03:22:02.668400  5661 net.cpp:409] Shared (in-place) memory (TRAIN) by data: 1030553600 diff: 1030553600
I1006 03:22:02.668402  5661 net.cpp:412] Parameters memory (TRAIN) required for data: 10817840 diff: 10817840
I1006 03:22:02.668408  5661 net.cpp:415] Parameters shared memory (TRAIN) by data: 0 diff: 0
I1006 03:22:02.668411  5661 net.cpp:421] Network initialization done.
I1006 03:22:02.669721  5661 solver.cpp:175] Creating test net (#0) specified by test_net file: training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/test.prototxt
I1006 03:22:02.670205  5661 net.cpp:80] Initializing net from parameters: 
name: "jsegnet21v2_test"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "ImageLabelData"
  top: "data"
  top: "label"
  transform_param {
    mirror: false
    crop_size: 640
    mean_value: 0
  }
  image_label_data_param {
    image_list_path: "data/val-image-lmdb"
    label_list_path: "data/val-label-lmdb"
    batch_size: 1
    threads: 1
    backend: LMDB
  }
}
layer {
  name: "data/bias"
  type: "Bias"
  bottom: "data"
  top: "data/bias"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  bias_param {
    filler {
      type: "constant"
      value: -128
    }
  }
}
layer {
  name: "conv1a"
  type: "Convolution"
  bottom: "data/bias"
  top: "conv1a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1a/bn"
  type: "BatchNorm"
  bottom: "conv1a"
  top: "conv1a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1a/relu"
  type: "ReLU"
  bottom: "conv1a"
  top: "conv1a"
}
layer {
  name: "conv1b"
  type: "Convolution"
  bottom: "conv1a"
  top: "conv1b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "conv1b/bn"
  type: "BatchNorm"
  bottom: "conv1b"
  top: "conv1b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "conv1b/relu"
  type: "ReLU"
  bottom: "conv1b"
  top: "conv1b"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1b"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res2a_branch2a"
  type: "Convolution"
  bottom: "pool1"
  top: "res2a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2a/relu"
  type: "ReLU"
  bottom: "res2a_branch2a"
  top: "res2a_branch2a"
}
layer {
  name: "res2a_branch2b"
  type: "Convolution"
  bottom: "res2a_branch2a"
  top: "res2a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res2a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res2a_branch2b/relu"
  type: "ReLU"
  bottom: "res2a_branch2b"
  top: "res2a_branch2b"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "res2a_branch2b"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res3a_branch2a"
  type: "Convolution"
  bottom: "pool2"
  top: "res3a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2a/relu"
  type: "ReLU"
  bottom: "res3a_branch2a"
  top: "res3a_branch2a"
}
layer {
  name: "res3a_branch2b"
  type: "Convolution"
  bottom: "res3a_branch2a"
  top: "res3a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res3a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res3a_branch2b/relu"
  type: "ReLU"
  bottom: "res3a_branch2b"
  top: "res3a_branch2b"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "res3a_branch2b"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "res4a_branch2a"
  type: "Convolution"
  bottom: "pool3"
  top: "res4a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2a/relu"
  type: "ReLU"
  bottom: "res4a_branch2a"
  top: "res4a_branch2a"
}
layer {
  name: "res4a_branch2b"
  type: "Convolution"
  bottom: "res4a_branch2a"
  top: "res4a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "res4a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res4a_branch2b/relu"
  type: "ReLU"
  bottom: "res4a_branch2b"
  top: "res4a_branch2b"
}
layer {
  name: "pool4"
  type: "Pooling"
  bottom: "res4a_branch2b"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "res5a_branch2a"
  type: "Convolution"
  bottom: "pool4"
  top: "res5a_branch2a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2a/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2a/relu"
  type: "ReLU"
  bottom: "res5a_branch2a"
  top: "res5a_branch2a"
}
layer {
  name: "res5a_branch2b"
  type: "Convolution"
  bottom: "res5a_branch2a"
  top: "res5a_branch2b"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 2
    kernel_size: 3
    group: 4
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 2
  }
}
layer {
  name: "res5a_branch2b/bn"
  type: "BatchNorm"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "res5a_branch2b/relu"
  type: "ReLU"
  bottom: "res5a_branch2b"
  top: "res5a_branch2b"
}
layer {
  name: "out5a"
  type: "Convolution"
  bottom: "res5a_branch2b"
  top: "out5a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "out5a/bn"
  type: "BatchNorm"
  bottom: "out5a"
  top: "out5a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out5a/relu"
  type: "ReLU"
  bottom: "out5a"
  top: "out5a"
}
layer {
  name: "out5a_up2"
  type: "Deconvolution"
  bottom: "out5a"
  top: "out5a_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 64
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out3a"
  type: "Convolution"
  bottom: "res3a_branch2b"
  top: "out3a"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "out3a/bn"
  type: "BatchNorm"
  bottom: "out3a"
  top: "out3a"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "out3a/relu"
  type: "ReLU"
  bottom: "out3a"
  top: "out3a"
}
layer {
  name: "out3_out5_combined"
  type: "Eltwise"
  bottom: "out5a_up2"
  bottom: "out3a"
  top: "out3_out5_combined"
}
layer {
  name: "ctx_conv1"
  type: "Convolution"
  bottom: "out3_out5_combined"
  top: "ctx_conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_conv1/bn"
  type: "BatchNorm"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv1/relu"
  type: "ReLU"
  bottom: "ctx_conv1"
  top: "ctx_conv1"
}
layer {
  name: "ctx_conv2"
  type: "Convolution"
  bottom: "ctx_conv1"
  top: "ctx_conv2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv2/bn"
  type: "BatchNorm"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv2/relu"
  type: "ReLU"
  bottom: "ctx_conv2"
  top: "ctx_conv2"
}
layer {
  name: "ctx_conv3"
  type: "Convolution"
  bottom: "ctx_conv2"
  top: "ctx_conv3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv3/bn"
  type: "BatchNorm"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv3/relu"
  type: "ReLU"
  bottom: "ctx_conv3"
  top: "ctx_conv3"
}
layer {
  name: "ctx_conv4"
  type: "Convolution"
  bottom: "ctx_conv3"
  top: "ctx_conv4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 4
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 4
  }
}
layer {
  name: "ctx_conv4/bn"
  type: "BatchNorm"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
  batch_norm_param {
    moving_average_fraction: 0.99
    eps: 0.0001
    scale_bias: true
  }
}
layer {
  name: "ctx_conv4/relu"
  type: "ReLU"
  bottom: "ctx_conv4"
  top: "ctx_conv4"
}
layer {
  name: "ctx_final"
  type: "Convolution"
  bottom: "ctx_conv4"
  top: "ctx_final"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: true
    pad: 1
    kernel_size: 3
    kernel_size: 3
    group: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
    dilation: 1
  }
}
layer {
  name: "ctx_final/relu"
  type: "ReLU"
  bottom: "ctx_final"
  top: "ctx_final"
}
layer {
  name: "out_deconv_final_up2"
  type: "Deconvolution"
  bottom: "ctx_final"
  top: "out_deconv_final_up2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up4"
  type: "Deconvolution"
  bottom: "out_deconv_final_up2"
  top: "out_deconv_final_up4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "out_deconv_final_up8"
  type: "Deconvolution"
  bottom: "out_deconv_final_up4"
  top: "out_deconv_final_up8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 8
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 8
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "loss"
  propagate_down: true
  propagate_down: false
  loss_param {
    ignore_label: 255
    normalization: VALID
  }
}
layer {
  name: "accuracy/top1"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top1"
  include {
    phase: TEST
  }
  accuracy_param {
    ignore_label: 255
  }
}
layer {
  name: "accuracy/top5"
  type: "Accuracy"
  bottom: "out_deconv_final_up8"
  bottom: "label"
  top: "accuracy/top5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
    ignore_label: 255
  }
}
I1006 03:22:02.670532  5661 net.cpp:110] Using FLOAT as default forward math type
I1006 03:22:02.670542  5661 net.cpp:116] Using FLOAT as default backward math type
I1006 03:22:02.670547  5661 layer_factory.hpp:172] Creating layer 'data' of type 'ImageLabelData'
I1006 03:22:02.670554  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.670564  5661 net.cpp:200] Created Layer data (0)
I1006 03:22:02.670570  5661 net.cpp:542] data -> data
I1006 03:22:02.670579  5661 net.cpp:542] data -> label
I1006 03:22:02.670610  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:02.670764  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:02.670804  5661 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 1
I1006 03:22:02.671180  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:02.672644  5688 db_lmdb.cpp:36] Opened lmdb data/val-image-lmdb
I1006 03:22:02.679249  5661 data_layer.cpp:199] (0) Output data size: 1, 3, 640, 640
I1006 03:22:02.679282  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:02.679342  5661 data_reader.cpp:58] Data Reader threads: 1, out queues: 1, depth: 1
I1006 03:22:02.679358  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:02.679854  5689 data_layer.cpp:105] (0) Parser threads: 1
I1006 03:22:02.679865  5689 data_layer.cpp:107] (0) Transformer threads: 1
I1006 03:22:02.680711  5690 db_lmdb.cpp:36] Opened lmdb data/val-label-lmdb
I1006 03:22:02.683135  5661 data_layer.cpp:199] (0) Output data size: 1, 1, 640, 640
I1006 03:22:02.683169  5661 internal_thread.cpp:19] Starting 1 internal thread(s) on device 0
I1006 03:22:02.683238  5661 net.cpp:260] Setting up data
I1006 03:22:02.683264  5661 net.cpp:267] TEST Top shape for layer 0 'data' 1 3 640 640 (1228800)
I1006 03:22:02.683269  5661 net.cpp:267] TEST Top shape for layer 0 'data' 1 1 640 640 (409600)
I1006 03:22:02.683277  5661 layer_factory.hpp:172] Creating layer 'label_data_1_split' of type 'Split'
I1006 03:22:02.683284  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.683293  5661 net.cpp:200] Created Layer label_data_1_split (1)
I1006 03:22:02.683298  5661 net.cpp:572] label_data_1_split <- label
I1006 03:22:02.683306  5661 net.cpp:542] label_data_1_split -> label_data_1_split_0
I1006 03:22:02.683779  5661 net.cpp:542] label_data_1_split -> label_data_1_split_1
I1006 03:22:02.683786  5691 data_layer.cpp:105] (0) Parser threads: 1
I1006 03:22:02.683791  5661 net.cpp:542] label_data_1_split -> label_data_1_split_2
I1006 03:22:02.683799  5691 data_layer.cpp:107] (0) Transformer threads: 1
I1006 03:22:02.683874  5661 net.cpp:260] Setting up label_data_1_split
I1006 03:22:02.683883  5661 net.cpp:267] TEST Top shape for layer 1 'label_data_1_split' 1 1 640 640 (409600)
I1006 03:22:02.683887  5661 net.cpp:267] TEST Top shape for layer 1 'label_data_1_split' 1 1 640 640 (409600)
I1006 03:22:02.683892  5661 net.cpp:267] TEST Top shape for layer 1 'label_data_1_split' 1 1 640 640 (409600)
I1006 03:22:02.683897  5661 layer_factory.hpp:172] Creating layer 'data/bias' of type 'Bias'
I1006 03:22:02.683900  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.683910  5661 net.cpp:200] Created Layer data/bias (2)
I1006 03:22:02.683914  5661 net.cpp:572] data/bias <- data
I1006 03:22:02.683920  5661 net.cpp:542] data/bias -> data/bias
I1006 03:22:02.685009  5661 net.cpp:260] Setting up data/bias
I1006 03:22:02.685029  5661 net.cpp:267] TEST Top shape for layer 2 'data/bias' 1 3 640 640 (1228800)
I1006 03:22:02.685040  5661 layer_factory.hpp:172] Creating layer 'conv1a' of type 'Convolution'
I1006 03:22:02.685046  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.685066  5661 net.cpp:200] Created Layer conv1a (3)
I1006 03:22:02.685071  5661 net.cpp:572] conv1a <- data/bias
I1006 03:22:02.685077  5661 net.cpp:542] conv1a -> conv1a
I1006 03:22:02.685531  5661 net.cpp:260] Setting up conv1a
I1006 03:22:02.685544  5661 net.cpp:267] TEST Top shape for layer 3 'conv1a' 1 32 320 320 (3276800)
I1006 03:22:02.685554  5661 layer_factory.hpp:172] Creating layer 'conv1a/bn' of type 'BatchNorm'
I1006 03:22:02.685560  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.685575  5661 net.cpp:200] Created Layer conv1a/bn (4)
I1006 03:22:02.685578  5661 net.cpp:572] conv1a/bn <- conv1a
I1006 03:22:02.685583  5661 net.cpp:527] conv1a/bn -> conv1a (in-place)
I1006 03:22:02.689733  5661 net.cpp:260] Setting up conv1a/bn
I1006 03:22:02.689759  5661 net.cpp:267] TEST Top shape for layer 4 'conv1a/bn' 1 32 320 320 (3276800)
I1006 03:22:02.689780  5661 layer_factory.hpp:172] Creating layer 'conv1a/relu' of type 'ReLU'
I1006 03:22:02.689786  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.689796  5661 net.cpp:200] Created Layer conv1a/relu (5)
I1006 03:22:02.689802  5661 net.cpp:572] conv1a/relu <- conv1a
I1006 03:22:02.689810  5661 net.cpp:527] conv1a/relu -> conv1a (in-place)
I1006 03:22:02.689818  5661 net.cpp:260] Setting up conv1a/relu
I1006 03:22:02.689822  5661 net.cpp:267] TEST Top shape for layer 5 'conv1a/relu' 1 32 320 320 (3276800)
I1006 03:22:02.689828  5661 layer_factory.hpp:172] Creating layer 'conv1b' of type 'Convolution'
I1006 03:22:02.689833  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.689851  5661 net.cpp:200] Created Layer conv1b (6)
I1006 03:22:02.689857  5661 net.cpp:572] conv1b <- conv1a
I1006 03:22:02.689862  5661 net.cpp:542] conv1b -> conv1b
I1006 03:22:02.690413  5661 net.cpp:260] Setting up conv1b
I1006 03:22:02.690438  5661 net.cpp:267] TEST Top shape for layer 6 'conv1b' 1 32 320 320 (3276800)
I1006 03:22:02.690449  5661 layer_factory.hpp:172] Creating layer 'conv1b/bn' of type 'BatchNorm'
I1006 03:22:02.690456  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.690467  5661 net.cpp:200] Created Layer conv1b/bn (7)
I1006 03:22:02.690472  5661 net.cpp:572] conv1b/bn <- conv1b
I1006 03:22:02.690477  5661 net.cpp:527] conv1b/bn -> conv1b (in-place)
I1006 03:22:02.691006  5661 net.cpp:260] Setting up conv1b/bn
I1006 03:22:02.691016  5661 net.cpp:267] TEST Top shape for layer 7 'conv1b/bn' 1 32 320 320 (3276800)
I1006 03:22:02.691026  5661 layer_factory.hpp:172] Creating layer 'conv1b/relu' of type 'ReLU'
I1006 03:22:02.691031  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.691043  5661 net.cpp:200] Created Layer conv1b/relu (8)
I1006 03:22:02.691048  5661 net.cpp:572] conv1b/relu <- conv1b
I1006 03:22:02.691053  5661 net.cpp:527] conv1b/relu -> conv1b (in-place)
I1006 03:22:02.691059  5661 net.cpp:260] Setting up conv1b/relu
I1006 03:22:02.691064  5661 net.cpp:267] TEST Top shape for layer 8 'conv1b/relu' 1 32 320 320 (3276800)
I1006 03:22:02.691069  5661 layer_factory.hpp:172] Creating layer 'pool1' of type 'Pooling'
I1006 03:22:02.691073  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.691083  5661 net.cpp:200] Created Layer pool1 (9)
I1006 03:22:02.691088  5661 net.cpp:572] pool1 <- conv1b
I1006 03:22:02.691092  5661 net.cpp:542] pool1 -> pool1
I1006 03:22:02.691256  5661 net.cpp:260] Setting up pool1
I1006 03:22:02.691267  5661 net.cpp:267] TEST Top shape for layer 9 'pool1' 1 32 160 160 (819200)
I1006 03:22:02.691272  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2a' of type 'Convolution'
I1006 03:22:02.691277  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.691296  5661 net.cpp:200] Created Layer res2a_branch2a (10)
I1006 03:22:02.691301  5661 net.cpp:572] res2a_branch2a <- pool1
I1006 03:22:02.691308  5661 net.cpp:542] res2a_branch2a -> res2a_branch2a
I1006 03:22:02.692139  5661 net.cpp:260] Setting up res2a_branch2a
I1006 03:22:02.692173  5661 net.cpp:267] TEST Top shape for layer 10 'res2a_branch2a' 1 64 160 160 (1638400)
I1006 03:22:02.692194  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2a/bn' of type 'BatchNorm'
I1006 03:22:02.692204  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.692220  5661 net.cpp:200] Created Layer res2a_branch2a/bn (11)
I1006 03:22:02.692227  5661 net.cpp:572] res2a_branch2a/bn <- res2a_branch2a
I1006 03:22:02.692236  5661 net.cpp:527] res2a_branch2a/bn -> res2a_branch2a (in-place)
I1006 03:22:02.692771  5661 net.cpp:260] Setting up res2a_branch2a/bn
I1006 03:22:02.692790  5661 net.cpp:267] TEST Top shape for layer 11 'res2a_branch2a/bn' 1 64 160 160 (1638400)
I1006 03:22:02.692802  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2a/relu' of type 'ReLU'
I1006 03:22:02.692808  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.692818  5661 net.cpp:200] Created Layer res2a_branch2a/relu (12)
I1006 03:22:02.692824  5661 net.cpp:572] res2a_branch2a/relu <- res2a_branch2a
I1006 03:22:02.692831  5661 net.cpp:527] res2a_branch2a/relu -> res2a_branch2a (in-place)
I1006 03:22:02.692843  5661 net.cpp:260] Setting up res2a_branch2a/relu
I1006 03:22:02.692849  5661 net.cpp:267] TEST Top shape for layer 12 'res2a_branch2a/relu' 1 64 160 160 (1638400)
I1006 03:22:02.692854  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2b' of type 'Convolution'
I1006 03:22:02.692860  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.692881  5661 net.cpp:200] Created Layer res2a_branch2b (13)
I1006 03:22:02.692886  5661 net.cpp:572] res2a_branch2b <- res2a_branch2a
I1006 03:22:02.692908  5661 net.cpp:542] res2a_branch2b -> res2a_branch2b
I1006 03:22:02.693339  5661 net.cpp:260] Setting up res2a_branch2b
I1006 03:22:02.693351  5661 net.cpp:267] TEST Top shape for layer 13 'res2a_branch2b' 1 64 160 160 (1638400)
I1006 03:22:02.693359  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2b/bn' of type 'BatchNorm'
I1006 03:22:02.693365  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.693375  5661 net.cpp:200] Created Layer res2a_branch2b/bn (14)
I1006 03:22:02.693382  5661 net.cpp:572] res2a_branch2b/bn <- res2a_branch2b
I1006 03:22:02.693387  5661 net.cpp:527] res2a_branch2b/bn -> res2a_branch2b (in-place)
I1006 03:22:02.693845  5661 net.cpp:260] Setting up res2a_branch2b/bn
I1006 03:22:02.693856  5661 net.cpp:267] TEST Top shape for layer 14 'res2a_branch2b/bn' 1 64 160 160 (1638400)
I1006 03:22:02.693866  5661 layer_factory.hpp:172] Creating layer 'res2a_branch2b/relu' of type 'ReLU'
I1006 03:22:02.693872  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.693878  5661 net.cpp:200] Created Layer res2a_branch2b/relu (15)
I1006 03:22:02.693883  5661 net.cpp:572] res2a_branch2b/relu <- res2a_branch2b
I1006 03:22:02.693888  5661 net.cpp:527] res2a_branch2b/relu -> res2a_branch2b (in-place)
I1006 03:22:02.693897  5661 net.cpp:260] Setting up res2a_branch2b/relu
I1006 03:22:02.693902  5661 net.cpp:267] TEST Top shape for layer 15 'res2a_branch2b/relu' 1 64 160 160 (1638400)
I1006 03:22:02.693907  5661 layer_factory.hpp:172] Creating layer 'pool2' of type 'Pooling'
I1006 03:22:02.693912  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.693928  5661 net.cpp:200] Created Layer pool2 (16)
I1006 03:22:02.693933  5661 net.cpp:572] pool2 <- res2a_branch2b
I1006 03:22:02.693938  5661 net.cpp:542] pool2 -> pool2
I1006 03:22:02.694002  5661 net.cpp:260] Setting up pool2
I1006 03:22:02.694011  5661 net.cpp:267] TEST Top shape for layer 16 'pool2' 1 64 80 80 (409600)
I1006 03:22:02.694018  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2a' of type 'Convolution'
I1006 03:22:02.694022  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.694037  5661 net.cpp:200] Created Layer res3a_branch2a (17)
I1006 03:22:02.694043  5661 net.cpp:572] res3a_branch2a <- pool2
I1006 03:22:02.694048  5661 net.cpp:542] res3a_branch2a -> res3a_branch2a
I1006 03:22:02.695154  5661 net.cpp:260] Setting up res3a_branch2a
I1006 03:22:02.695168  5661 net.cpp:267] TEST Top shape for layer 17 'res3a_branch2a' 1 128 80 80 (819200)
I1006 03:22:02.695176  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2a/bn' of type 'BatchNorm'
I1006 03:22:02.695183  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.695194  5661 net.cpp:200] Created Layer res3a_branch2a/bn (18)
I1006 03:22:02.695199  5661 net.cpp:572] res3a_branch2a/bn <- res3a_branch2a
I1006 03:22:02.695206  5661 net.cpp:527] res3a_branch2a/bn -> res3a_branch2a (in-place)
I1006 03:22:02.695623  5661 net.cpp:260] Setting up res3a_branch2a/bn
I1006 03:22:02.695633  5661 net.cpp:267] TEST Top shape for layer 18 'res3a_branch2a/bn' 1 128 80 80 (819200)
I1006 03:22:02.695652  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2a/relu' of type 'ReLU'
I1006 03:22:02.695658  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.695664  5661 net.cpp:200] Created Layer res3a_branch2a/relu (19)
I1006 03:22:02.695669  5661 net.cpp:572] res3a_branch2a/relu <- res3a_branch2a
I1006 03:22:02.695675  5661 net.cpp:527] res3a_branch2a/relu -> res3a_branch2a (in-place)
I1006 03:22:02.695683  5661 net.cpp:260] Setting up res3a_branch2a/relu
I1006 03:22:02.695690  5661 net.cpp:267] TEST Top shape for layer 19 'res3a_branch2a/relu' 1 128 80 80 (819200)
I1006 03:22:02.695695  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2b' of type 'Convolution'
I1006 03:22:02.695711  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.695730  5661 net.cpp:200] Created Layer res3a_branch2b (20)
I1006 03:22:02.695736  5661 net.cpp:572] res3a_branch2b <- res3a_branch2a
I1006 03:22:02.695741  5661 net.cpp:542] res3a_branch2b -> res3a_branch2b
I1006 03:22:02.696524  5661 net.cpp:260] Setting up res3a_branch2b
I1006 03:22:02.696547  5661 net.cpp:267] TEST Top shape for layer 20 'res3a_branch2b' 1 128 80 80 (819200)
I1006 03:22:02.696563  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2b/bn' of type 'BatchNorm'
I1006 03:22:02.696570  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.696584  5661 net.cpp:200] Created Layer res3a_branch2b/bn (21)
I1006 03:22:02.696590  5661 net.cpp:572] res3a_branch2b/bn <- res3a_branch2b
I1006 03:22:02.696596  5661 net.cpp:527] res3a_branch2b/bn -> res3a_branch2b (in-place)
I1006 03:22:02.697047  5661 net.cpp:260] Setting up res3a_branch2b/bn
I1006 03:22:02.697063  5661 net.cpp:267] TEST Top shape for layer 21 'res3a_branch2b/bn' 1 128 80 80 (819200)
I1006 03:22:02.697082  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2b/relu' of type 'ReLU'
I1006 03:22:02.697093  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.697104  5661 net.cpp:200] Created Layer res3a_branch2b/relu (22)
I1006 03:22:02.697109  5661 net.cpp:572] res3a_branch2b/relu <- res3a_branch2b
I1006 03:22:02.697115  5661 net.cpp:527] res3a_branch2b/relu -> res3a_branch2b (in-place)
I1006 03:22:02.697124  5661 net.cpp:260] Setting up res3a_branch2b/relu
I1006 03:22:02.697130  5661 net.cpp:267] TEST Top shape for layer 22 'res3a_branch2b/relu' 1 128 80 80 (819200)
I1006 03:22:02.697135  5661 layer_factory.hpp:172] Creating layer 'res3a_branch2b_res3a_branch2b/relu_0_split' of type 'Split'
I1006 03:22:02.697140  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.697149  5661 net.cpp:200] Created Layer res3a_branch2b_res3a_branch2b/relu_0_split (23)
I1006 03:22:02.697154  5661 net.cpp:572] res3a_branch2b_res3a_branch2b/relu_0_split <- res3a_branch2b
I1006 03:22:02.697158  5661 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_0
I1006 03:22:02.697165  5661 net.cpp:542] res3a_branch2b_res3a_branch2b/relu_0_split -> res3a_branch2b_res3a_branch2b/relu_0_split_1
I1006 03:22:02.697227  5661 net.cpp:260] Setting up res3a_branch2b_res3a_branch2b/relu_0_split
I1006 03:22:02.697239  5661 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 80 80 (819200)
I1006 03:22:02.697248  5661 net.cpp:267] TEST Top shape for layer 23 'res3a_branch2b_res3a_branch2b/relu_0_split' 1 128 80 80 (819200)
I1006 03:22:02.697257  5661 layer_factory.hpp:172] Creating layer 'pool3' of type 'Pooling'
I1006 03:22:02.697265  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.697279  5661 net.cpp:200] Created Layer pool3 (24)
I1006 03:22:02.697288  5661 net.cpp:572] pool3 <- res3a_branch2b_res3a_branch2b/relu_0_split_0
I1006 03:22:02.697297  5661 net.cpp:542] pool3 -> pool3
I1006 03:22:02.697373  5661 net.cpp:260] Setting up pool3
I1006 03:22:02.697382  5661 net.cpp:267] TEST Top shape for layer 24 'pool3' 1 128 40 40 (204800)
I1006 03:22:02.697388  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2a' of type 'Convolution'
I1006 03:22:02.697393  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.697424  5661 net.cpp:200] Created Layer res4a_branch2a (25)
I1006 03:22:02.697433  5661 net.cpp:572] res4a_branch2a <- pool3
I1006 03:22:02.697443  5661 net.cpp:542] res4a_branch2a -> res4a_branch2a
I1006 03:22:02.702009  5661 net.cpp:260] Setting up res4a_branch2a
I1006 03:22:02.702028  5661 net.cpp:267] TEST Top shape for layer 25 'res4a_branch2a' 1 256 40 40 (409600)
I1006 03:22:02.702051  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2a/bn' of type 'BatchNorm'
I1006 03:22:02.702059  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.702071  5661 net.cpp:200] Created Layer res4a_branch2a/bn (26)
I1006 03:22:02.702077  5661 net.cpp:572] res4a_branch2a/bn <- res4a_branch2a
I1006 03:22:02.702085  5661 net.cpp:527] res4a_branch2a/bn -> res4a_branch2a (in-place)
I1006 03:22:02.702529  5661 net.cpp:260] Setting up res4a_branch2a/bn
I1006 03:22:02.702539  5661 net.cpp:267] TEST Top shape for layer 26 'res4a_branch2a/bn' 1 256 40 40 (409600)
I1006 03:22:02.702550  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2a/relu' of type 'ReLU'
I1006 03:22:02.702556  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.702563  5661 net.cpp:200] Created Layer res4a_branch2a/relu (27)
I1006 03:22:02.702567  5661 net.cpp:572] res4a_branch2a/relu <- res4a_branch2a
I1006 03:22:02.702574  5661 net.cpp:527] res4a_branch2a/relu -> res4a_branch2a (in-place)
I1006 03:22:02.702582  5661 net.cpp:260] Setting up res4a_branch2a/relu
I1006 03:22:02.702589  5661 net.cpp:267] TEST Top shape for layer 27 'res4a_branch2a/relu' 1 256 40 40 (409600)
I1006 03:22:02.702594  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2b' of type 'Convolution'
I1006 03:22:02.702599  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.702617  5661 net.cpp:200] Created Layer res4a_branch2b (28)
I1006 03:22:02.702622  5661 net.cpp:572] res4a_branch2b <- res4a_branch2a
I1006 03:22:02.702627  5661 net.cpp:542] res4a_branch2b -> res4a_branch2b
I1006 03:22:02.705147  5661 net.cpp:260] Setting up res4a_branch2b
I1006 03:22:02.705163  5661 net.cpp:267] TEST Top shape for layer 28 'res4a_branch2b' 1 256 40 40 (409600)
I1006 03:22:02.705176  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2b/bn' of type 'BatchNorm'
I1006 03:22:02.705184  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.705200  5661 net.cpp:200] Created Layer res4a_branch2b/bn (29)
I1006 03:22:02.705209  5661 net.cpp:572] res4a_branch2b/bn <- res4a_branch2b
I1006 03:22:02.705216  5661 net.cpp:527] res4a_branch2b/bn -> res4a_branch2b (in-place)
I1006 03:22:02.705827  5661 net.cpp:260] Setting up res4a_branch2b/bn
I1006 03:22:02.705840  5661 net.cpp:267] TEST Top shape for layer 29 'res4a_branch2b/bn' 1 256 40 40 (409600)
I1006 03:22:02.705858  5661 layer_factory.hpp:172] Creating layer 'res4a_branch2b/relu' of type 'ReLU'
I1006 03:22:02.705866  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.705879  5661 net.cpp:200] Created Layer res4a_branch2b/relu (30)
I1006 03:22:02.705886  5661 net.cpp:572] res4a_branch2b/relu <- res4a_branch2b
I1006 03:22:02.705894  5661 net.cpp:527] res4a_branch2b/relu -> res4a_branch2b (in-place)
I1006 03:22:02.705904  5661 net.cpp:260] Setting up res4a_branch2b/relu
I1006 03:22:02.705914  5661 net.cpp:267] TEST Top shape for layer 30 'res4a_branch2b/relu' 1 256 40 40 (409600)
I1006 03:22:02.705921  5661 layer_factory.hpp:172] Creating layer 'pool4' of type 'Pooling'
I1006 03:22:02.705929  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.705942  5661 net.cpp:200] Created Layer pool4 (31)
I1006 03:22:02.705951  5661 net.cpp:572] pool4 <- res4a_branch2b
I1006 03:22:02.705960  5661 net.cpp:542] pool4 -> pool4
I1006 03:22:02.706045  5661 net.cpp:260] Setting up pool4
I1006 03:22:02.706058  5661 net.cpp:267] TEST Top shape for layer 31 'pool4' 1 256 40 40 (409600)
I1006 03:22:02.706066  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2a' of type 'Convolution'
I1006 03:22:02.706074  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.706102  5661 net.cpp:200] Created Layer res5a_branch2a (32)
I1006 03:22:02.706122  5661 net.cpp:572] res5a_branch2a <- pool4
I1006 03:22:02.706132  5661 net.cpp:542] res5a_branch2a -> res5a_branch2a
I1006 03:22:02.721045  5661 net.cpp:260] Setting up res5a_branch2a
I1006 03:22:02.721066  5661 net.cpp:267] TEST Top shape for layer 32 'res5a_branch2a' 1 512 40 40 (819200)
I1006 03:22:02.721076  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2a/bn' of type 'BatchNorm'
I1006 03:22:02.721082  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.721096  5661 net.cpp:200] Created Layer res5a_branch2a/bn (33)
I1006 03:22:02.721102  5661 net.cpp:572] res5a_branch2a/bn <- res5a_branch2a
I1006 03:22:02.721109  5661 net.cpp:527] res5a_branch2a/bn -> res5a_branch2a (in-place)
I1006 03:22:02.721520  5661 net.cpp:260] Setting up res5a_branch2a/bn
I1006 03:22:02.721530  5661 net.cpp:267] TEST Top shape for layer 33 'res5a_branch2a/bn' 1 512 40 40 (819200)
I1006 03:22:02.721541  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2a/relu' of type 'ReLU'
I1006 03:22:02.721545  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.721554  5661 net.cpp:200] Created Layer res5a_branch2a/relu (34)
I1006 03:22:02.721557  5661 net.cpp:572] res5a_branch2a/relu <- res5a_branch2a
I1006 03:22:02.721562  5661 net.cpp:527] res5a_branch2a/relu -> res5a_branch2a (in-place)
I1006 03:22:02.721568  5661 net.cpp:260] Setting up res5a_branch2a/relu
I1006 03:22:02.721577  5661 net.cpp:267] TEST Top shape for layer 34 'res5a_branch2a/relu' 1 512 40 40 (819200)
I1006 03:22:02.721582  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2b' of type 'Convolution'
I1006 03:22:02.721585  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.721602  5661 net.cpp:200] Created Layer res5a_branch2b (35)
I1006 03:22:02.721607  5661 net.cpp:572] res5a_branch2b <- res5a_branch2a
I1006 03:22:02.721611  5661 net.cpp:542] res5a_branch2b -> res5a_branch2b
I1006 03:22:02.728848  5661 net.cpp:260] Setting up res5a_branch2b
I1006 03:22:02.728863  5661 net.cpp:267] TEST Top shape for layer 35 'res5a_branch2b' 1 512 40 40 (819200)
I1006 03:22:02.728883  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2b/bn' of type 'BatchNorm'
I1006 03:22:02.728889  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.728900  5661 net.cpp:200] Created Layer res5a_branch2b/bn (36)
I1006 03:22:02.728906  5661 net.cpp:572] res5a_branch2b/bn <- res5a_branch2b
I1006 03:22:02.728912  5661 net.cpp:527] res5a_branch2b/bn -> res5a_branch2b (in-place)
I1006 03:22:02.729313  5661 net.cpp:260] Setting up res5a_branch2b/bn
I1006 03:22:02.729322  5661 net.cpp:267] TEST Top shape for layer 36 'res5a_branch2b/bn' 1 512 40 40 (819200)
I1006 03:22:02.729333  5661 layer_factory.hpp:172] Creating layer 'res5a_branch2b/relu' of type 'ReLU'
I1006 03:22:02.729339  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.729346  5661 net.cpp:200] Created Layer res5a_branch2b/relu (37)
I1006 03:22:02.729351  5661 net.cpp:572] res5a_branch2b/relu <- res5a_branch2b
I1006 03:22:02.729355  5661 net.cpp:527] res5a_branch2b/relu -> res5a_branch2b (in-place)
I1006 03:22:02.729363  5661 net.cpp:260] Setting up res5a_branch2b/relu
I1006 03:22:02.729369  5661 net.cpp:267] TEST Top shape for layer 37 'res5a_branch2b/relu' 1 512 40 40 (819200)
I1006 03:22:02.729373  5661 layer_factory.hpp:172] Creating layer 'out5a' of type 'Convolution'
I1006 03:22:02.729378  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.729393  5661 net.cpp:200] Created Layer out5a (38)
I1006 03:22:02.729398  5661 net.cpp:572] out5a <- res5a_branch2b
I1006 03:22:02.729403  5661 net.cpp:542] out5a -> out5a
I1006 03:22:02.731936  5661 net.cpp:260] Setting up out5a
I1006 03:22:02.731950  5661 net.cpp:267] TEST Top shape for layer 38 'out5a' 1 64 40 40 (102400)
I1006 03:22:02.731971  5661 layer_factory.hpp:172] Creating layer 'out5a/bn' of type 'BatchNorm'
I1006 03:22:02.731976  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.731992  5661 net.cpp:200] Created Layer out5a/bn (39)
I1006 03:22:02.731997  5661 net.cpp:572] out5a/bn <- out5a
I1006 03:22:02.732002  5661 net.cpp:527] out5a/bn -> out5a (in-place)
I1006 03:22:02.732439  5661 net.cpp:260] Setting up out5a/bn
I1006 03:22:02.732450  5661 net.cpp:267] TEST Top shape for layer 39 'out5a/bn' 1 64 40 40 (102400)
I1006 03:22:02.732460  5661 layer_factory.hpp:172] Creating layer 'out5a/relu' of type 'ReLU'
I1006 03:22:02.732465  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.732472  5661 net.cpp:200] Created Layer out5a/relu (40)
I1006 03:22:02.732477  5661 net.cpp:572] out5a/relu <- out5a
I1006 03:22:02.732481  5661 net.cpp:527] out5a/relu -> out5a (in-place)
I1006 03:22:02.732488  5661 net.cpp:260] Setting up out5a/relu
I1006 03:22:02.732493  5661 net.cpp:267] TEST Top shape for layer 40 'out5a/relu' 1 64 40 40 (102400)
I1006 03:22:02.732498  5661 layer_factory.hpp:172] Creating layer 'out5a_up2' of type 'Deconvolution'
I1006 03:22:02.732503  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.732523  5661 net.cpp:200] Created Layer out5a_up2 (41)
I1006 03:22:02.732528  5661 net.cpp:572] out5a_up2 <- out5a
I1006 03:22:02.732533  5661 net.cpp:542] out5a_up2 -> out5a_up2
I1006 03:22:02.732791  5661 net.cpp:260] Setting up out5a_up2
I1006 03:22:02.732800  5661 net.cpp:267] TEST Top shape for layer 41 'out5a_up2' 1 64 80 80 (409600)
I1006 03:22:02.732806  5661 layer_factory.hpp:172] Creating layer 'out3a' of type 'Convolution'
I1006 03:22:02.732810  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.732825  5661 net.cpp:200] Created Layer out3a (42)
I1006 03:22:02.732831  5661 net.cpp:572] out3a <- res3a_branch2b_res3a_branch2b/relu_0_split_1
I1006 03:22:02.732837  5661 net.cpp:542] out3a -> out3a
I1006 03:22:02.734236  5661 net.cpp:260] Setting up out3a
I1006 03:22:02.734252  5661 net.cpp:267] TEST Top shape for layer 42 'out3a' 1 64 80 80 (409600)
I1006 03:22:02.734264  5661 layer_factory.hpp:172] Creating layer 'out3a/bn' of type 'BatchNorm'
I1006 03:22:02.734272  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.734288  5661 net.cpp:200] Created Layer out3a/bn (43)
I1006 03:22:02.734297  5661 net.cpp:572] out3a/bn <- out3a
I1006 03:22:02.734304  5661 net.cpp:527] out3a/bn -> out3a (in-place)
I1006 03:22:02.734903  5661 net.cpp:260] Setting up out3a/bn
I1006 03:22:02.734915  5661 net.cpp:267] TEST Top shape for layer 43 'out3a/bn' 1 64 80 80 (409600)
I1006 03:22:02.734931  5661 layer_factory.hpp:172] Creating layer 'out3a/relu' of type 'ReLU'
I1006 03:22:02.734937  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.734948  5661 net.cpp:200] Created Layer out3a/relu (44)
I1006 03:22:02.734954  5661 net.cpp:572] out3a/relu <- out3a
I1006 03:22:02.734962  5661 net.cpp:527] out3a/relu -> out3a (in-place)
I1006 03:22:02.734972  5661 net.cpp:260] Setting up out3a/relu
I1006 03:22:02.734979  5661 net.cpp:267] TEST Top shape for layer 44 'out3a/relu' 1 64 80 80 (409600)
I1006 03:22:02.734987  5661 layer_factory.hpp:172] Creating layer 'out3_out5_combined' of type 'Eltwise'
I1006 03:22:02.734992  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.735004  5661 net.cpp:200] Created Layer out3_out5_combined (45)
I1006 03:22:02.735011  5661 net.cpp:572] out3_out5_combined <- out5a_up2
I1006 03:22:02.735018  5661 net.cpp:572] out3_out5_combined <- out3a
I1006 03:22:02.735026  5661 net.cpp:542] out3_out5_combined -> out3_out5_combined
I1006 03:22:02.735065  5661 net.cpp:260] Setting up out3_out5_combined
I1006 03:22:02.735075  5661 net.cpp:267] TEST Top shape for layer 45 'out3_out5_combined' 1 64 80 80 (409600)
I1006 03:22:02.735093  5661 layer_factory.hpp:172] Creating layer 'ctx_conv1' of type 'Convolution'
I1006 03:22:02.735101  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.735126  5661 net.cpp:200] Created Layer ctx_conv1 (46)
I1006 03:22:02.735132  5661 net.cpp:572] ctx_conv1 <- out3_out5_combined
I1006 03:22:02.735141  5661 net.cpp:542] ctx_conv1 -> ctx_conv1
I1006 03:22:02.736202  5661 net.cpp:260] Setting up ctx_conv1
I1006 03:22:02.736219  5661 net.cpp:267] TEST Top shape for layer 46 'ctx_conv1' 1 64 80 80 (409600)
I1006 03:22:02.736232  5661 layer_factory.hpp:172] Creating layer 'ctx_conv1/bn' of type 'BatchNorm'
I1006 03:22:02.736239  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.736251  5661 net.cpp:200] Created Layer ctx_conv1/bn (47)
I1006 03:22:02.736258  5661 net.cpp:572] ctx_conv1/bn <- ctx_conv1
I1006 03:22:02.736266  5661 net.cpp:527] ctx_conv1/bn -> ctx_conv1 (in-place)
I1006 03:22:02.736887  5661 net.cpp:260] Setting up ctx_conv1/bn
I1006 03:22:02.736902  5661 net.cpp:267] TEST Top shape for layer 47 'ctx_conv1/bn' 1 64 80 80 (409600)
I1006 03:22:02.736918  5661 layer_factory.hpp:172] Creating layer 'ctx_conv1/relu' of type 'ReLU'
I1006 03:22:02.736927  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.736937  5661 net.cpp:200] Created Layer ctx_conv1/relu (48)
I1006 03:22:02.736944  5661 net.cpp:572] ctx_conv1/relu <- ctx_conv1
I1006 03:22:02.736950  5661 net.cpp:527] ctx_conv1/relu -> ctx_conv1 (in-place)
I1006 03:22:02.736960  5661 net.cpp:260] Setting up ctx_conv1/relu
I1006 03:22:02.736966  5661 net.cpp:267] TEST Top shape for layer 48 'ctx_conv1/relu' 1 64 80 80 (409600)
I1006 03:22:02.736973  5661 layer_factory.hpp:172] Creating layer 'ctx_conv2' of type 'Convolution'
I1006 03:22:02.736976  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.736992  5661 net.cpp:200] Created Layer ctx_conv2 (49)
I1006 03:22:02.736996  5661 net.cpp:572] ctx_conv2 <- ctx_conv1
I1006 03:22:02.737001  5661 net.cpp:542] ctx_conv2 -> ctx_conv2
I1006 03:22:02.737653  5661 net.cpp:260] Setting up ctx_conv2
I1006 03:22:02.737663  5661 net.cpp:267] TEST Top shape for layer 49 'ctx_conv2' 1 64 80 80 (409600)
I1006 03:22:02.737670  5661 layer_factory.hpp:172] Creating layer 'ctx_conv2/bn' of type 'BatchNorm'
I1006 03:22:02.737675  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.737684  5661 net.cpp:200] Created Layer ctx_conv2/bn (50)
I1006 03:22:02.737689  5661 net.cpp:572] ctx_conv2/bn <- ctx_conv2
I1006 03:22:02.737694  5661 net.cpp:527] ctx_conv2/bn -> ctx_conv2 (in-place)
I1006 03:22:02.738111  5661 net.cpp:260] Setting up ctx_conv2/bn
I1006 03:22:02.738119  5661 net.cpp:267] TEST Top shape for layer 50 'ctx_conv2/bn' 1 64 80 80 (409600)
I1006 03:22:02.738129  5661 layer_factory.hpp:172] Creating layer 'ctx_conv2/relu' of type 'ReLU'
I1006 03:22:02.738134  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.738139  5661 net.cpp:200] Created Layer ctx_conv2/relu (51)
I1006 03:22:02.738143  5661 net.cpp:572] ctx_conv2/relu <- ctx_conv2
I1006 03:22:02.738147  5661 net.cpp:527] ctx_conv2/relu -> ctx_conv2 (in-place)
I1006 03:22:02.738155  5661 net.cpp:260] Setting up ctx_conv2/relu
I1006 03:22:02.738160  5661 net.cpp:267] TEST Top shape for layer 51 'ctx_conv2/relu' 1 64 80 80 (409600)
I1006 03:22:02.738164  5661 layer_factory.hpp:172] Creating layer 'ctx_conv3' of type 'Convolution'
I1006 03:22:02.738168  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.738181  5661 net.cpp:200] Created Layer ctx_conv3 (52)
I1006 03:22:02.738186  5661 net.cpp:572] ctx_conv3 <- ctx_conv2
I1006 03:22:02.738190  5661 net.cpp:542] ctx_conv3 -> ctx_conv3
I1006 03:22:02.738840  5661 net.cpp:260] Setting up ctx_conv3
I1006 03:22:02.738850  5661 net.cpp:267] TEST Top shape for layer 52 'ctx_conv3' 1 64 80 80 (409600)
I1006 03:22:02.738857  5661 layer_factory.hpp:172] Creating layer 'ctx_conv3/bn' of type 'BatchNorm'
I1006 03:22:02.738862  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.738871  5661 net.cpp:200] Created Layer ctx_conv3/bn (53)
I1006 03:22:02.738875  5661 net.cpp:572] ctx_conv3/bn <- ctx_conv3
I1006 03:22:02.738879  5661 net.cpp:527] ctx_conv3/bn -> ctx_conv3 (in-place)
I1006 03:22:02.739302  5661 net.cpp:260] Setting up ctx_conv3/bn
I1006 03:22:02.739312  5661 net.cpp:267] TEST Top shape for layer 53 'ctx_conv3/bn' 1 64 80 80 (409600)
I1006 03:22:02.739322  5661 layer_factory.hpp:172] Creating layer 'ctx_conv3/relu' of type 'ReLU'
I1006 03:22:02.739327  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.739332  5661 net.cpp:200] Created Layer ctx_conv3/relu (54)
I1006 03:22:02.739336  5661 net.cpp:572] ctx_conv3/relu <- ctx_conv3
I1006 03:22:02.739341  5661 net.cpp:527] ctx_conv3/relu -> ctx_conv3 (in-place)
I1006 03:22:02.739346  5661 net.cpp:260] Setting up ctx_conv3/relu
I1006 03:22:02.739353  5661 net.cpp:267] TEST Top shape for layer 54 'ctx_conv3/relu' 1 64 80 80 (409600)
I1006 03:22:02.739357  5661 layer_factory.hpp:172] Creating layer 'ctx_conv4' of type 'Convolution'
I1006 03:22:02.739362  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.739377  5661 net.cpp:200] Created Layer ctx_conv4 (55)
I1006 03:22:02.739382  5661 net.cpp:572] ctx_conv4 <- ctx_conv3
I1006 03:22:02.739385  5661 net.cpp:542] ctx_conv4 -> ctx_conv4
I1006 03:22:02.740029  5661 net.cpp:260] Setting up ctx_conv4
I1006 03:22:02.740038  5661 net.cpp:267] TEST Top shape for layer 55 'ctx_conv4' 1 64 80 80 (409600)
I1006 03:22:02.740046  5661 layer_factory.hpp:172] Creating layer 'ctx_conv4/bn' of type 'BatchNorm'
I1006 03:22:02.740051  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.740059  5661 net.cpp:200] Created Layer ctx_conv4/bn (56)
I1006 03:22:02.740063  5661 net.cpp:572] ctx_conv4/bn <- ctx_conv4
I1006 03:22:02.740068  5661 net.cpp:527] ctx_conv4/bn -> ctx_conv4 (in-place)
I1006 03:22:02.740490  5661 net.cpp:260] Setting up ctx_conv4/bn
I1006 03:22:02.740499  5661 net.cpp:267] TEST Top shape for layer 56 'ctx_conv4/bn' 1 64 80 80 (409600)
I1006 03:22:02.740509  5661 layer_factory.hpp:172] Creating layer 'ctx_conv4/relu' of type 'ReLU'
I1006 03:22:02.740519  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.740526  5661 net.cpp:200] Created Layer ctx_conv4/relu (57)
I1006 03:22:02.740530  5661 net.cpp:572] ctx_conv4/relu <- ctx_conv4
I1006 03:22:02.740535  5661 net.cpp:527] ctx_conv4/relu -> ctx_conv4 (in-place)
I1006 03:22:02.740541  5661 net.cpp:260] Setting up ctx_conv4/relu
I1006 03:22:02.740547  5661 net.cpp:267] TEST Top shape for layer 57 'ctx_conv4/relu' 1 64 80 80 (409600)
I1006 03:22:02.740552  5661 layer_factory.hpp:172] Creating layer 'ctx_final' of type 'Convolution'
I1006 03:22:02.740556  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.740571  5661 net.cpp:200] Created Layer ctx_final (58)
I1006 03:22:02.740574  5661 net.cpp:572] ctx_final <- ctx_conv4
I1006 03:22:02.740579  5661 net.cpp:542] ctx_final -> ctx_final
I1006 03:22:02.740898  5661 net.cpp:260] Setting up ctx_final
I1006 03:22:02.740907  5661 net.cpp:267] TEST Top shape for layer 58 'ctx_final' 1 8 80 80 (51200)
I1006 03:22:02.740916  5661 layer_factory.hpp:172] Creating layer 'ctx_final/relu' of type 'ReLU'
I1006 03:22:02.740921  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.740927  5661 net.cpp:200] Created Layer ctx_final/relu (59)
I1006 03:22:02.740931  5661 net.cpp:572] ctx_final/relu <- ctx_final
I1006 03:22:02.740945  5661 net.cpp:527] ctx_final/relu -> ctx_final (in-place)
I1006 03:22:02.740952  5661 net.cpp:260] Setting up ctx_final/relu
I1006 03:22:02.740957  5661 net.cpp:267] TEST Top shape for layer 59 'ctx_final/relu' 1 8 80 80 (51200)
I1006 03:22:02.740962  5661 layer_factory.hpp:172] Creating layer 'out_deconv_final_up2' of type 'Deconvolution'
I1006 03:22:02.740967  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.740981  5661 net.cpp:200] Created Layer out_deconv_final_up2 (60)
I1006 03:22:02.740985  5661 net.cpp:572] out_deconv_final_up2 <- ctx_final
I1006 03:22:02.740990  5661 net.cpp:542] out_deconv_final_up2 -> out_deconv_final_up2
I1006 03:22:02.741197  5661 net.cpp:260] Setting up out_deconv_final_up2
I1006 03:22:02.741204  5661 net.cpp:267] TEST Top shape for layer 60 'out_deconv_final_up2' 1 8 160 160 (204800)
I1006 03:22:02.741211  5661 layer_factory.hpp:172] Creating layer 'out_deconv_final_up4' of type 'Deconvolution'
I1006 03:22:02.741215  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.741228  5661 net.cpp:200] Created Layer out_deconv_final_up4 (61)
I1006 03:22:02.741233  5661 net.cpp:572] out_deconv_final_up4 <- out_deconv_final_up2
I1006 03:22:02.741237  5661 net.cpp:542] out_deconv_final_up4 -> out_deconv_final_up4
I1006 03:22:02.741443  5661 net.cpp:260] Setting up out_deconv_final_up4
I1006 03:22:02.741451  5661 net.cpp:267] TEST Top shape for layer 61 'out_deconv_final_up4' 1 8 320 320 (819200)
I1006 03:22:02.741458  5661 layer_factory.hpp:172] Creating layer 'out_deconv_final_up8' of type 'Deconvolution'
I1006 03:22:02.741462  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.741474  5661 net.cpp:200] Created Layer out_deconv_final_up8 (62)
I1006 03:22:02.741478  5661 net.cpp:572] out_deconv_final_up8 <- out_deconv_final_up4
I1006 03:22:02.741483  5661 net.cpp:542] out_deconv_final_up8 -> out_deconv_final_up8
I1006 03:22:02.741688  5661 net.cpp:260] Setting up out_deconv_final_up8
I1006 03:22:02.741694  5661 net.cpp:267] TEST Top shape for layer 62 'out_deconv_final_up8' 1 8 640 640 (3276800)
I1006 03:22:02.741701  5661 layer_factory.hpp:172] Creating layer 'out_deconv_final_up8_out_deconv_final_up8_0_split' of type 'Split'
I1006 03:22:02.741705  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.741711  5661 net.cpp:200] Created Layer out_deconv_final_up8_out_deconv_final_up8_0_split (63)
I1006 03:22:02.741715  5661 net.cpp:572] out_deconv_final_up8_out_deconv_final_up8_0_split <- out_deconv_final_up8
I1006 03:22:02.741720  5661 net.cpp:542] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_0
I1006 03:22:02.741729  5661 net.cpp:542] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_1
I1006 03:22:02.741734  5661 net.cpp:542] out_deconv_final_up8_out_deconv_final_up8_0_split -> out_deconv_final_up8_out_deconv_final_up8_0_split_2
I1006 03:22:02.741791  5661 net.cpp:260] Setting up out_deconv_final_up8_out_deconv_final_up8_0_split
I1006 03:22:02.741798  5661 net.cpp:267] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 1 8 640 640 (3276800)
I1006 03:22:02.741803  5661 net.cpp:267] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 1 8 640 640 (3276800)
I1006 03:22:02.741809  5661 net.cpp:267] TEST Top shape for layer 63 'out_deconv_final_up8_out_deconv_final_up8_0_split' 1 8 640 640 (3276800)
I1006 03:22:02.741813  5661 layer_factory.hpp:172] Creating layer 'loss' of type 'SoftmaxWithLoss'
I1006 03:22:02.741818  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.741832  5661 net.cpp:200] Created Layer loss (64)
I1006 03:22:02.741837  5661 net.cpp:572] loss <- out_deconv_final_up8_out_deconv_final_up8_0_split_0
I1006 03:22:02.741849  5661 net.cpp:572] loss <- label_data_1_split_0
I1006 03:22:02.741855  5661 net.cpp:542] loss -> loss
I1006 03:22:02.747618  5661 net.cpp:260] Setting up loss
I1006 03:22:02.747650  5661 net.cpp:267] TEST Top shape for layer 64 'loss' (1)
I1006 03:22:02.747654  5661 net.cpp:271]     with loss weight 1
I1006 03:22:02.747670  5661 layer_factory.hpp:172] Creating layer 'accuracy/top1' of type 'Accuracy'
I1006 03:22:02.747678  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.747694  5661 net.cpp:200] Created Layer accuracy/top1 (65)
I1006 03:22:02.747701  5661 net.cpp:572] accuracy/top1 <- out_deconv_final_up8_out_deconv_final_up8_0_split_1
I1006 03:22:02.747709  5661 net.cpp:572] accuracy/top1 <- label_data_1_split_1
I1006 03:22:02.747715  5661 net.cpp:542] accuracy/top1 -> accuracy/top1
I1006 03:22:02.747730  5661 net.cpp:260] Setting up accuracy/top1
I1006 03:22:02.747735  5661 net.cpp:267] TEST Top shape for layer 65 'accuracy/top1' (1)
I1006 03:22:02.747740  5661 layer_factory.hpp:172] Creating layer 'accuracy/top5' of type 'Accuracy'
I1006 03:22:02.747745  5661 layer_factory.hpp:184] Layer's types are Ftype:FLOAT Btype:FLOAT Fmath:FLOAT Bmath:FLOAT
I1006 03:22:02.747751  5661 net.cpp:200] Created Layer accuracy/top5 (66)
I1006 03:22:02.747756  5661 net.cpp:572] accuracy/top5 <- out_deconv_final_up8_out_deconv_final_up8_0_split_2
I1006 03:22:02.747761  5661 net.cpp:572] accuracy/top5 <- label_data_1_split_2
I1006 03:22:02.747766  5661 net.cpp:542] accuracy/top5 -> accuracy/top5
I1006 03:22:02.747772  5661 net.cpp:260] Setting up accuracy/top5
I1006 03:22:02.747778  5661 net.cpp:267] TEST Top shape for layer 66 'accuracy/top5' (1)
I1006 03:22:02.747782  5661 net.cpp:338] accuracy/top5 does not need backward computation.
I1006 03:22:02.747788  5661 net.cpp:338] accuracy/top1 does not need backward computation.
I1006 03:22:02.747794  5661 net.cpp:336] loss needs backward computation.
I1006 03:22:02.747799  5661 net.cpp:336] out_deconv_final_up8_out_deconv_final_up8_0_split needs backward computation.
I1006 03:22:02.747803  5661 net.cpp:336] out_deconv_final_up8 needs backward computation.
I1006 03:22:02.747809  5661 net.cpp:336] out_deconv_final_up4 needs backward computation.
I1006 03:22:02.747812  5661 net.cpp:336] out_deconv_final_up2 needs backward computation.
I1006 03:22:02.747817  5661 net.cpp:336] ctx_final/relu needs backward computation.
I1006 03:22:02.747822  5661 net.cpp:336] ctx_final needs backward computation.
I1006 03:22:02.747826  5661 net.cpp:336] ctx_conv4/relu needs backward computation.
I1006 03:22:02.747830  5661 net.cpp:336] ctx_conv4/bn needs backward computation.
I1006 03:22:02.747833  5661 net.cpp:336] ctx_conv4 needs backward computation.
I1006 03:22:02.747838  5661 net.cpp:336] ctx_conv3/relu needs backward computation.
I1006 03:22:02.747841  5661 net.cpp:336] ctx_conv3/bn needs backward computation.
I1006 03:22:02.747845  5661 net.cpp:336] ctx_conv3 needs backward computation.
I1006 03:22:02.747849  5661 net.cpp:336] ctx_conv2/relu needs backward computation.
I1006 03:22:02.747853  5661 net.cpp:336] ctx_conv2/bn needs backward computation.
I1006 03:22:02.747858  5661 net.cpp:336] ctx_conv2 needs backward computation.
I1006 03:22:02.747861  5661 net.cpp:336] ctx_conv1/relu needs backward computation.
I1006 03:22:02.747864  5661 net.cpp:336] ctx_conv1/bn needs backward computation.
I1006 03:22:02.747869  5661 net.cpp:336] ctx_conv1 needs backward computation.
I1006 03:22:02.747872  5661 net.cpp:336] out3_out5_combined needs backward computation.
I1006 03:22:02.747877  5661 net.cpp:336] out3a/relu needs backward computation.
I1006 03:22:02.747882  5661 net.cpp:336] out3a/bn needs backward computation.
I1006 03:22:02.747885  5661 net.cpp:336] out3a needs backward computation.
I1006 03:22:02.747891  5661 net.cpp:336] out5a_up2 needs backward computation.
I1006 03:22:02.747895  5661 net.cpp:336] out5a/relu needs backward computation.
I1006 03:22:02.747900  5661 net.cpp:336] out5a/bn needs backward computation.
I1006 03:22:02.747903  5661 net.cpp:336] out5a needs backward computation.
I1006 03:22:02.747920  5661 net.cpp:336] res5a_branch2b/relu needs backward computation.
I1006 03:22:02.747925  5661 net.cpp:336] res5a_branch2b/bn needs backward computation.
I1006 03:22:02.747928  5661 net.cpp:336] res5a_branch2b needs backward computation.
I1006 03:22:02.747932  5661 net.cpp:336] res5a_branch2a/relu needs backward computation.
I1006 03:22:02.747936  5661 net.cpp:336] res5a_branch2a/bn needs backward computation.
I1006 03:22:02.747941  5661 net.cpp:336] res5a_branch2a needs backward computation.
I1006 03:22:02.747943  5661 net.cpp:336] pool4 needs backward computation.
I1006 03:22:02.747948  5661 net.cpp:336] res4a_branch2b/relu needs backward computation.
I1006 03:22:02.747951  5661 net.cpp:336] res4a_branch2b/bn needs backward computation.
I1006 03:22:02.747956  5661 net.cpp:336] res4a_branch2b needs backward computation.
I1006 03:22:02.747959  5661 net.cpp:336] res4a_branch2a/relu needs backward computation.
I1006 03:22:02.747963  5661 net.cpp:336] res4a_branch2a/bn needs backward computation.
I1006 03:22:02.747967  5661 net.cpp:336] res4a_branch2a needs backward computation.
I1006 03:22:02.747972  5661 net.cpp:336] pool3 needs backward computation.
I1006 03:22:02.747977  5661 net.cpp:336] res3a_branch2b_res3a_branch2b/relu_0_split needs backward computation.
I1006 03:22:02.747982  5661 net.cpp:336] res3a_branch2b/relu needs backward computation.
I1006 03:22:02.747985  5661 net.cpp:336] res3a_branch2b/bn needs backward computation.
I1006 03:22:02.747988  5661 net.cpp:336] res3a_branch2b needs backward computation.
I1006 03:22:02.747993  5661 net.cpp:336] res3a_branch2a/relu needs backward computation.
I1006 03:22:02.747997  5661 net.cpp:336] res3a_branch2a/bn needs backward computation.
I1006 03:22:02.748001  5661 net.cpp:336] res3a_branch2a needs backward computation.
I1006 03:22:02.748005  5661 net.cpp:336] pool2 needs backward computation.
I1006 03:22:02.748009  5661 net.cpp:336] res2a_branch2b/relu needs backward computation.
I1006 03:22:02.748013  5661 net.cpp:336] res2a_branch2b/bn needs backward computation.
I1006 03:22:02.748016  5661 net.cpp:336] res2a_branch2b needs backward computation.
I1006 03:22:02.748021  5661 net.cpp:336] res2a_branch2a/relu needs backward computation.
I1006 03:22:02.748025  5661 net.cpp:336] res2a_branch2a/bn needs backward computation.
I1006 03:22:02.748029  5661 net.cpp:336] res2a_branch2a needs backward computation.
I1006 03:22:02.748034  5661 net.cpp:336] pool1 needs backward computation.
I1006 03:22:02.748037  5661 net.cpp:336] conv1b/relu needs backward computation.
I1006 03:22:02.748041  5661 net.cpp:336] conv1b/bn needs backward computation.
I1006 03:22:02.748044  5661 net.cpp:336] conv1b needs backward computation.
I1006 03:22:02.748049  5661 net.cpp:336] conv1a/relu needs backward computation.
I1006 03:22:02.748052  5661 net.cpp:336] conv1a/bn needs backward computation.
I1006 03:22:02.748056  5661 net.cpp:336] conv1a needs backward computation.
I1006 03:22:02.748060  5661 net.cpp:338] data/bias does not need backward computation.
I1006 03:22:02.748065  5661 net.cpp:338] label_data_1_split does not need backward computation.
I1006 03:22:02.748070  5661 net.cpp:338] data does not need backward computation.
I1006 03:22:02.748075  5661 net.cpp:380] This network produces output accuracy/top1
I1006 03:22:02.748078  5661 net.cpp:380] This network produces output accuracy/top5
I1006 03:22:02.748082  5661 net.cpp:380] This network produces output loss
I1006 03:22:02.748157  5661 net.cpp:403] Top memory (TEST) required for data: 283443224 diff: 283443224
I1006 03:22:02.748162  5661 net.cpp:406] Bottom memory (TEST) required for data: 283443200 diff: 283443200
I1006 03:22:02.748167  5661 net.cpp:409] Shared (in-place) memory (TEST) by data: 128819200 diff: 128819200
I1006 03:22:02.748170  5661 net.cpp:412] Parameters memory (TEST) required for data: 10817840 diff: 10817840
I1006 03:22:02.748173  5661 net.cpp:415] Parameters shared memory (TEST) by data: 0 diff: 0
I1006 03:22:02.748176  5661 net.cpp:421] Network initialization done.
I1006 03:22:02.748378  5661 solver.cpp:55] Solver scaffolding done.
I1006 03:22:02.753361  5661 caffe.cpp:158] Finetuning from training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/l1reg/cityscapes5_jsegnet21v2_iter_60000.caffemodel
I1006 03:22:02.760291  5661 net.cpp:1153] Copying source layer data Type:ImageLabelData #blobs=0
I1006 03:22:02.760308  5661 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I1006 03:22:02.760339  5661 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I1006 03:22:02.760355  5661 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.760378  5661 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I1006 03:22:02.760383  5661 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I1006 03:22:02.760396  5661 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.760417  5661 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I1006 03:22:02.760421  5661 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I1006 03:22:02.760424  5661 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I1006 03:22:02.760443  5661 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.760465  5661 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I1006 03:22:02.760469  5661 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I1006 03:22:02.760484  5661 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.760505  5661 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I1006 03:22:02.760510  5661 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I1006 03:22:02.760514  5661 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I1006 03:22:02.760560  5661 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.760583  5661 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I1006 03:22:02.760587  5661 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I1006 03:22:02.760613  5661 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.760635  5661 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I1006 03:22:02.760640  5661 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I1006 03:22:02.760643  5661 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I1006 03:22:02.760646  5661 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I1006 03:22:02.760763  5661 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.760787  5661 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I1006 03:22:02.760792  5661 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I1006 03:22:02.760854  5661 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.760879  5661 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I1006 03:22:02.760882  5661 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I1006 03:22:02.760885  5661 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I1006 03:22:02.761252  5661 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.761277  5661 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I1006 03:22:02.761282  5661 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I1006 03:22:02.761448  5661 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.761471  5661 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I1006 03:22:02.761476  5661 net.cpp:1153] Copying source layer out5a Type:Convolution #blobs=2
I1006 03:22:02.761538  5661 net.cpp:1153] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.761560  5661 net.cpp:1153] Copying source layer out5a/relu Type:ReLU #blobs=0
I1006 03:22:02.761565  5661 net.cpp:1153] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I1006 03:22:02.761574  5661 net.cpp:1153] Copying source layer out3a Type:Convolution #blobs=2
I1006 03:22:02.761596  5661 net.cpp:1153] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.761620  5661 net.cpp:1153] Copying source layer out3a/relu Type:ReLU #blobs=0
I1006 03:22:02.761624  5661 net.cpp:1153] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I1006 03:22:02.761627  5661 net.cpp:1153] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I1006 03:22:02.761648  5661 net.cpp:1153] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I1006 03:22:02.761672  5661 net.cpp:1153] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I1006 03:22:02.761677  5661 net.cpp:1153] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I1006 03:22:02.761696  5661 net.cpp:1153] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I1006 03:22:02.761718  5661 net.cpp:1153] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I1006 03:22:02.761723  5661 net.cpp:1153] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I1006 03:22:02.761744  5661 net.cpp:1153] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I1006 03:22:02.761767  5661 net.cpp:1153] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I1006 03:22:02.761772  5661 net.cpp:1153] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I1006 03:22:02.761793  5661 net.cpp:1153] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I1006 03:22:02.761816  5661 net.cpp:1153] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I1006 03:22:02.761821  5661 net.cpp:1153] Copying source layer ctx_final Type:Convolution #blobs=2
I1006 03:22:02.761832  5661 net.cpp:1153] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I1006 03:22:02.761837  5661 net.cpp:1153] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I1006 03:22:02.761845  5661 net.cpp:1153] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I1006 03:22:02.761854  5661 net.cpp:1153] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I1006 03:22:02.761863  5661 net.cpp:1153] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I1006 03:22:02.765240  5661 net.cpp:1153] Copying source layer data Type:ImageLabelData #blobs=0
I1006 03:22:02.765257  5661 net.cpp:1153] Copying source layer data/bias Type:Bias #blobs=1
I1006 03:22:02.765290  5661 net.cpp:1153] Copying source layer conv1a Type:Convolution #blobs=2
I1006 03:22:02.765312  5661 net.cpp:1153] Copying source layer conv1a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.765347  5661 net.cpp:1153] Copying source layer conv1a/relu Type:ReLU #blobs=0
I1006 03:22:02.765353  5661 net.cpp:1153] Copying source layer conv1b Type:Convolution #blobs=2
I1006 03:22:02.765372  5661 net.cpp:1153] Copying source layer conv1b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.765406  5661 net.cpp:1153] Copying source layer conv1b/relu Type:ReLU #blobs=0
I1006 03:22:02.765413  5661 net.cpp:1153] Copying source layer pool1 Type:Pooling #blobs=0
I1006 03:22:02.765419  5661 net.cpp:1153] Copying source layer res2a_branch2a Type:Convolution #blobs=2
I1006 03:22:02.765444  5661 net.cpp:1153] Copying source layer res2a_branch2a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.765480  5661 net.cpp:1153] Copying source layer res2a_branch2a/relu Type:ReLU #blobs=0
I1006 03:22:02.765486  5661 net.cpp:1153] Copying source layer res2a_branch2b Type:Convolution #blobs=2
I1006 03:22:02.765508  5661 net.cpp:1153] Copying source layer res2a_branch2b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.765542  5661 net.cpp:1153] Copying source layer res2a_branch2b/relu Type:ReLU #blobs=0
I1006 03:22:02.765549  5661 net.cpp:1153] Copying source layer pool2 Type:Pooling #blobs=0
I1006 03:22:02.765554  5661 net.cpp:1153] Copying source layer res3a_branch2a Type:Convolution #blobs=2
I1006 03:22:02.765614  5661 net.cpp:1153] Copying source layer res3a_branch2a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.765650  5661 net.cpp:1153] Copying source layer res3a_branch2a/relu Type:ReLU #blobs=0
I1006 03:22:02.765656  5661 net.cpp:1153] Copying source layer res3a_branch2b Type:Convolution #blobs=2
I1006 03:22:02.765688  5661 net.cpp:1153] Copying source layer res3a_branch2b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.765723  5661 net.cpp:1153] Copying source layer res3a_branch2b/relu Type:ReLU #blobs=0
I1006 03:22:02.765730  5661 net.cpp:1153] Copying source layer res3a_branch2b_res3a_branch2b/relu_0_split Type:Split #blobs=0
I1006 03:22:02.765735  5661 net.cpp:1153] Copying source layer pool3 Type:Pooling #blobs=0
I1006 03:22:02.765741  5661 net.cpp:1153] Copying source layer res4a_branch2a Type:Convolution #blobs=2
I1006 03:22:02.765864  5661 net.cpp:1153] Copying source layer res4a_branch2a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.765899  5661 net.cpp:1153] Copying source layer res4a_branch2a/relu Type:ReLU #blobs=0
I1006 03:22:02.765907  5661 net.cpp:1153] Copying source layer res4a_branch2b Type:Convolution #blobs=2
I1006 03:22:02.765974  5661 net.cpp:1153] Copying source layer res4a_branch2b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.766008  5661 net.cpp:1153] Copying source layer res4a_branch2b/relu Type:ReLU #blobs=0
I1006 03:22:02.766016  5661 net.cpp:1153] Copying source layer pool4 Type:Pooling #blobs=0
I1006 03:22:02.766021  5661 net.cpp:1153] Copying source layer res5a_branch2a Type:Convolution #blobs=2
I1006 03:22:02.766404  5661 net.cpp:1153] Copying source layer res5a_branch2a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.766443  5661 net.cpp:1153] Copying source layer res5a_branch2a/relu Type:ReLU #blobs=0
I1006 03:22:02.766449  5661 net.cpp:1153] Copying source layer res5a_branch2b Type:Convolution #blobs=2
I1006 03:22:02.766621  5661 net.cpp:1153] Copying source layer res5a_branch2b/bn Type:BatchNorm #blobs=5
I1006 03:22:02.766659  5661 net.cpp:1153] Copying source layer res5a_branch2b/relu Type:ReLU #blobs=0
I1006 03:22:02.766664  5661 net.cpp:1153] Copying source layer out5a Type:Convolution #blobs=2
I1006 03:22:02.766717  5661 net.cpp:1153] Copying source layer out5a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.766752  5661 net.cpp:1153] Copying source layer out5a/relu Type:ReLU #blobs=0
I1006 03:22:02.766757  5661 net.cpp:1153] Copying source layer out5a_up2 Type:Deconvolution #blobs=1
I1006 03:22:02.766770  5661 net.cpp:1153] Copying source layer out3a Type:Convolution #blobs=2
I1006 03:22:02.766801  5661 net.cpp:1153] Copying source layer out3a/bn Type:BatchNorm #blobs=5
I1006 03:22:02.766837  5661 net.cpp:1153] Copying source layer out3a/relu Type:ReLU #blobs=0
I1006 03:22:02.766844  5661 net.cpp:1153] Copying source layer out3_out5_combined Type:Eltwise #blobs=0
I1006 03:22:02.766850  5661 net.cpp:1153] Copying source layer ctx_conv1 Type:Convolution #blobs=2
I1006 03:22:02.766881  5661 net.cpp:1153] Copying source layer ctx_conv1/bn Type:BatchNorm #blobs=5
I1006 03:22:02.766916  5661 net.cpp:1153] Copying source layer ctx_conv1/relu Type:ReLU #blobs=0
I1006 03:22:02.766923  5661 net.cpp:1153] Copying source layer ctx_conv2 Type:Convolution #blobs=2
I1006 03:22:02.766955  5661 net.cpp:1153] Copying source layer ctx_conv2/bn Type:BatchNorm #blobs=5
I1006 03:22:02.766994  5661 net.cpp:1153] Copying source layer ctx_conv2/relu Type:ReLU #blobs=0
I1006 03:22:02.767001  5661 net.cpp:1153] Copying source layer ctx_conv3 Type:Convolution #blobs=2
I1006 03:22:02.767031  5661 net.cpp:1153] Copying source layer ctx_conv3/bn Type:BatchNorm #blobs=5
I1006 03:22:02.767067  5661 net.cpp:1153] Copying source layer ctx_conv3/relu Type:ReLU #blobs=0
I1006 03:22:02.767074  5661 net.cpp:1153] Copying source layer ctx_conv4 Type:Convolution #blobs=2
I1006 03:22:02.767105  5661 net.cpp:1153] Copying source layer ctx_conv4/bn Type:BatchNorm #blobs=5
I1006 03:22:02.767132  5661 net.cpp:1153] Copying source layer ctx_conv4/relu Type:ReLU #blobs=0
I1006 03:22:02.767145  5661 net.cpp:1153] Copying source layer ctx_final Type:Convolution #blobs=2
I1006 03:22:02.767160  5661 net.cpp:1153] Copying source layer ctx_final/relu Type:ReLU #blobs=0
I1006 03:22:02.767164  5661 net.cpp:1153] Copying source layer out_deconv_final_up2 Type:Deconvolution #blobs=1
I1006 03:22:02.767175  5661 net.cpp:1153] Copying source layer out_deconv_final_up4 Type:Deconvolution #blobs=1
I1006 03:22:02.767189  5661 net.cpp:1153] Copying source layer out_deconv_final_up8 Type:Deconvolution #blobs=1
I1006 03:22:02.767202  5661 net.cpp:1153] Copying source layer loss Type:SoftmaxWithLoss #blobs=0
I1006 03:22:02.767421  5661 caffe.cpp:260] Starting Optimization
I1006 03:22:02.767441  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 03:22:02.783886  5661 solver.cpp:453] Solving jsegnet21v2_train
I1006 03:22:02.783907  5661 solver.cpp:454] Learning Rate Policy: multistep
I1006 03:22:02.783939  5661 net.cpp:1483] [0] Reserving 10800128 bytes of shared learnable space for type FLOAT
I1006 03:22:02.787210  5661 solver.cpp:269] Initial Test started...
I1006 03:22:02.787230  5661 solver.cpp:538] Iteration 0, Testing net (#0)
I1006 03:22:02.789583  5692 common.cpp:528] NVML initialized, thread 5692
I1006 03:22:02.823479  5692 common.cpp:550] NVML succeeded to set CPU affinity on device 0, thread 5692
I1006 03:22:02.825700  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.744485
I1006 03:22:02.825718  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 03:22:02.825726  5661 solver.cpp:624]     Test net output #2: loss = 1.10466 (* 1 = 1.10466 loss)
I1006 03:22:02.825744  5661 solver.cpp:274] Initial Test completed in 0.0385086s
I1006 03:22:03.462208  5661 solver.cpp:358] Iteration 0 (0.636452 s), loss = 0.0426138
I1006 03:22:03.462236  5661 solver.cpp:376]     Train net output #0: loss = 0.0301087 (* 1 = 0.0301087 loss)
I1006 03:22:03.462249  5661 sgd_solver.cpp:172] Iteration 0, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:22:03.464447  5661 solver.cpp:389] Sparsity after update:
I1006 03:22:03.465443  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 03:22:03.465454  5661 net.cpp:2738] conv1a_param_0(0) 
I1006 03:22:03.465461  5661 net.cpp:2738] conv1b_param_0(0) 
I1006 03:22:03.465466  5661 net.cpp:2738] ctx_conv1_param_0(0) 
I1006 03:22:03.465469  5661 net.cpp:2738] ctx_conv2_param_0(0) 
I1006 03:22:03.465472  5661 net.cpp:2738] ctx_conv3_param_0(0) 
I1006 03:22:03.465476  5661 net.cpp:2738] ctx_conv4_param_0(0) 
I1006 03:22:03.465479  5661 net.cpp:2738] ctx_final_param_0(0) 
I1006 03:22:03.465482  5661 net.cpp:2738] out3a_param_0(0) 
I1006 03:22:03.465485  5661 net.cpp:2738] out5a_param_0(0) 
I1006 03:22:03.465489  5661 net.cpp:2738] res2a_branch2a_param_0(0) 
I1006 03:22:03.465493  5661 net.cpp:2738] res2a_branch2b_param_0(0) 
I1006 03:22:03.465497  5661 net.cpp:2738] res3a_branch2a_param_0(0) 
I1006 03:22:03.465499  5661 net.cpp:2738] res3a_branch2b_param_0(0) 
I1006 03:22:03.465503  5661 net.cpp:2738] res4a_branch2a_param_0(0) 
I1006 03:22:03.465508  5661 net.cpp:2738] res4a_branch2b_param_0(0) 
I1006 03:22:03.465512  5661 net.cpp:2738] res5a_branch2a_param_0(0) 
I1006 03:22:03.465515  5661 net.cpp:2738] res5a_branch2b_param_0(0) 
I1006 03:22:03.465519  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (0/2.69117e+06) 0
I1006 03:22:03.544137  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1a' with space 1.23M 3/1 1 1 3 	(avail 3.82G, req 1.23M)	t: 0 0 3.19
I1006 03:22:03.967442  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'conv1b' with space 1.23M 32/4 6 4 3 	(avail 3.82G, req 1.23M)	t: 0 1.55 2.52
I1006 03:22:04.466707  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2a' with space 1.23M 32/1 6 4 3 	(avail 3.82G, req 1.23M)	t: 0 1.27 3.13
I1006 03:22:04.789865  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res2a_branch2b' with space 1.23M 64/4 6 4 3 	(avail 3.82G, req 1.23M)	t: 0 0.55 1.3
I1006 03:22:05.190199  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2a' with space 0.09G 64/1 6 4 5 	(avail 3.73G, req 0.09G)	t: 0 1.14 1.95
I1006 03:22:05.454198  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res3a_branch2b' with space 0.09G 128/4 6 4 3 	(avail 3.73G, req 0.09G)	t: 0 0.24 0.56
I1006 03:22:05.784356  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2a' with space 0.09G 128/1 6 4 5 	(avail 3.73G, req 0.09G)	t: 0 1.25 0.98
I1006 03:22:06.034296  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res4a_branch2b' with space 0.09G 256/4 6 4 5 	(avail 3.73G, req 0.09G)	t: 0 0.22 0.3
I1006 03:22:06.401160  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2a' with space 0.09G 256/1 0 0 0 	(avail 3.73G, req 0.09G)	t: 0 12.72 11.79
I1006 03:22:06.632591  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'res5a_branch2b' with space 0.09G 512/4 0 0 0 	(avail 3.73G, req 0.09G)	t: 0 1.87 1.48
I1006 03:22:06.852957  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'out5a' with space 0.09G 512/2 0 0 0 	(avail 3.73G, req 0.09G)	t: 0 2.1 1.01
I1006 03:22:07.123471  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'out3a' with space 0.09G 128/2 6 4 3 	(avail 3.73G, req 0.09G)	t: 0 0.43 0.95
I1006 03:22:07.630851  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_conv1' with space 0.09G 64/1 6 4 3 	(avail 3.73G, req 0.09G)	t: 0 0.66 1.55
I1006 03:22:07.740963  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_conv2' with space 0.09G 64/1 0 0 0 	(avail 3.73G, req 0.09G)	t: 0 3 1.82
I1006 03:22:08.138494  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_conv3' with space 0.09G 64/1 0 0 0 	(avail 3.73G, req 0.09G)	t: 0 3.06 1.97
I1006 03:22:08.249456  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_conv4' with space 0.09G 64/1 0 0 0 	(avail 3.73G, req 0.09G)	t: 0 3.07 1.82
I1006 03:22:08.531147  5661 cudnn_conv_layer.cpp:848] [0] Conv Algos (F,BD,BF): 'ctx_final' with space 0.09G 64/1 6 4 3 	(avail 3.73G, req 0.09G)	t: 0 0.25 0.87
I1006 03:22:09.004884  5661 solver.cpp:358] Iteration 1 (5.54276 s), loss = 0.048354
I1006 03:22:09.004909  5661 solver.cpp:376]     Train net output #0: loss = 0.0454869 (* 1 = 0.0454869 loss)
I1006 03:22:09.550546  5661 solver.cpp:358] Iteration 2 (0.545666 s), loss = 0.0505193
I1006 03:22:09.550571  5661 solver.cpp:376]     Train net output #0: loss = 0.0616716 (* 1 = 0.0616716 loss)
I1006 03:23:03.199117  5661 solver.cpp:352] Iteration 100 (1.82668 iter/s, 53.6494s/98 iter), loss = 0.0549144
I1006 03:23:03.212102  5661 solver.cpp:376]     Train net output #0: loss = 0.051381 (* 1 = 0.051381 loss)
I1006 03:23:03.212121  5661 sgd_solver.cpp:172] Iteration 100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:23:48.788281  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:23:48.862927  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:23:58.134713  5661 solver.cpp:352] Iteration 200 (1.82028 iter/s, 54.9365s/100 iter), 1.1/322.7ep, loss = 0.0562458
I1006 03:23:58.134738  5661 solver.cpp:376]     Train net output #0: loss = 0.042126 (* 1 = 0.042126 loss)
I1006 03:23:58.134747  5661 sgd_solver.cpp:172] Iteration 200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:24:52.946565  5661 solver.cpp:352] Iteration 300 (1.82439 iter/s, 54.8128s/100 iter), 1.6/322.7ep, loss = 0.0702962
I1006 03:24:52.947374  5661 solver.cpp:376]     Train net output #0: loss = 0.0702934 (* 1 = 0.0702934 loss)
I1006 03:24:52.947383  5661 sgd_solver.cpp:172] Iteration 300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:25:30.877897  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:25:30.943666  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:25:47.841890  5661 solver.cpp:352] Iteration 400 (1.82162 iter/s, 54.8962s/100 iter), 2.2/322.7ep, loss = 0.0661099
I1006 03:25:47.841914  5661 solver.cpp:376]     Train net output #0: loss = 0.0539966 (* 1 = 0.0539966 loss)
I1006 03:25:47.841922  5661 sgd_solver.cpp:172] Iteration 400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:26:42.746817  5661 solver.cpp:352] Iteration 500 (1.8213 iter/s, 54.9059s/100 iter), 2.7/322.7ep, loss = 0.0773596
I1006 03:26:42.747654  5661 solver.cpp:376]     Train net output #0: loss = 0.0789708 (* 1 = 0.0789708 loss)
I1006 03:26:42.747663  5661 sgd_solver.cpp:172] Iteration 500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:27:12.998693  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:27:13.057575  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:27:37.640821  5661 solver.cpp:352] Iteration 600 (1.82166 iter/s, 54.8949s/100 iter), 3.2/322.7ep, loss = 0.0673295
I1006 03:27:37.640844  5661 solver.cpp:376]     Train net output #0: loss = 0.0563676 (* 1 = 0.0563676 loss)
I1006 03:27:37.640851  5661 sgd_solver.cpp:172] Iteration 600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:28:32.543081  5661 solver.cpp:352] Iteration 700 (1.82139 iter/s, 54.9032s/100 iter), 3.8/322.7ep, loss = 0.0742361
I1006 03:28:32.543885  5661 solver.cpp:376]     Train net output #0: loss = 0.0998394 (* 1 = 0.0998394 loss)
I1006 03:28:32.543896  5661 sgd_solver.cpp:172] Iteration 700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:28:55.123637  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:28:55.176512  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:29:27.584859  5661 solver.cpp:352] Iteration 800 (1.81677 iter/s, 55.0427s/100 iter), 4.3/322.7ep, loss = 0.107656
I1006 03:29:27.585669  5661 solver.cpp:376]     Train net output #0: loss = 0.122808 (* 1 = 0.122808 loss)
I1006 03:29:27.585677  5661 sgd_solver.cpp:172] Iteration 800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:30:22.643059  5661 solver.cpp:352] Iteration 900 (1.81623 iter/s, 55.0592s/100 iter), 4.8/322.7ep, loss = 0.0961337
I1006 03:30:22.643858  5661 solver.cpp:376]     Train net output #0: loss = 0.0747747 (* 1 = 0.0747747 loss)
I1006 03:30:22.643869  5661 sgd_solver.cpp:172] Iteration 900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:30:37.510887  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:30:37.555155  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:31:17.679925  5661 solver.cpp:352] Iteration 1000 (1.81693 iter/s, 55.0378s/100 iter), 5.4/322.7ep, loss = 0.070785
I1006 03:31:17.680071  5661 solver.cpp:376]     Train net output #0: loss = 0.0508444 (* 1 = 0.0508444 loss)
I1006 03:31:17.680080  5661 sgd_solver.cpp:172] Iteration 1000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:31:17.681102  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.6 sparsity_achieved=0 iter=1000
W1006 03:31:17.681118  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 03:31:19.515815  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.242083
W1006 03:31:19.515848  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 03:31:21.947346  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.574219
W1006 03:31:21.947381  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 03:31:25.119206  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.59668
W1006 03:31:25.119244  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 03:31:30.868731  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.574978
W1006 03:31:30.868768  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 03:31:37.388365  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.598958
W1006 03:31:37.388399  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 03:31:46.106707  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.596436
W1006 03:31:46.106742  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 03:31:58.341056  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.599826
W1006 03:31:58.341202  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 03:32:13.827913  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.598877
W1006 03:32:13.827950  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 03:32:35.028589  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.598966
W1006 03:32:35.029844  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 03:32:45.393441  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.599826
W1006 03:32:45.393474  5661 net.cpp:2612] out5a ni=512 no=64
W1006 03:32:46.928390  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.599826
W1006 03:32:46.928418  5661 net.cpp:2612] out3a ni=128 no=64
W1006 03:32:47.848522  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.598958
W1006 03:32:47.848549  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 03:32:49.524127  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.598958
W1006 03:32:49.524163  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 03:32:51.093238  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.598958
W1006 03:32:51.093269  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 03:32:52.599568  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.598958
W1006 03:32:52.599601  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 03:32:53.884879  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.598958
W1006 03:32:53.884909  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 03:32:53.973317  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.291016
I1006 03:32:53.973337  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 03:32:53.975687  5661 solver.cpp:389] Sparsity after update:
I1006 03:32:53.976562  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 03:32:53.976570  5661 net.cpp:2738] conv1a_param_0(0.242) 
I1006 03:32:53.976575  5661 net.cpp:2738] conv1b_param_0(0.574) 
I1006 03:32:53.976578  5661 net.cpp:2738] ctx_conv1_param_0(0.599) 
I1006 03:32:53.976581  5661 net.cpp:2738] ctx_conv2_param_0(0.599) 
I1006 03:32:53.976584  5661 net.cpp:2738] ctx_conv3_param_0(0.599) 
I1006 03:32:53.976588  5661 net.cpp:2738] ctx_conv4_param_0(0.599) 
I1006 03:32:53.976590  5661 net.cpp:2738] ctx_final_param_0(0.291) 
I1006 03:32:53.976593  5661 net.cpp:2738] out3a_param_0(0.599) 
I1006 03:32:53.976595  5661 net.cpp:2738] out5a_param_0(0.6) 
I1006 03:32:53.976598  5661 net.cpp:2738] res2a_branch2a_param_0(0.597) 
I1006 03:32:53.976603  5661 net.cpp:2738] res2a_branch2b_param_0(0.575) 
I1006 03:32:53.976604  5661 net.cpp:2738] res3a_branch2a_param_0(0.599) 
I1006 03:32:53.976608  5661 net.cpp:2738] res3a_branch2b_param_0(0.596) 
I1006 03:32:53.976610  5661 net.cpp:2738] res4a_branch2a_param_0(0.6) 
I1006 03:32:53.976613  5661 net.cpp:2738] res4a_branch2b_param_0(0.599) 
I1006 03:32:53.976616  5661 net.cpp:2738] res5a_branch2a_param_0(0.599) 
I1006 03:32:53.976619  5661 net.cpp:2738] res5a_branch2b_param_0(0.6) 
I1006 03:32:53.976622  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.6101e+06/2.69117e+06) 0.598
I1006 03:33:47.139549  5661 solver.cpp:352] Iteration 1100 (0.669061 iter/s, 149.463s/100 iter), 5.9/322.7ep, loss = 0.0515954
I1006 03:33:47.140291  5661 solver.cpp:376]     Train net output #0: loss = 0.0606634 (* 1 = 0.0606634 loss)
I1006 03:33:47.140300  5661 sgd_solver.cpp:172] Iteration 1100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:33:54.272738  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:33:54.304695  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:34:42.137544  5661 solver.cpp:352] Iteration 1200 (1.8182 iter/s, 54.9995s/100 iter), 6.5/322.7ep, loss = 0.0608599
I1006 03:34:42.137681  5661 solver.cpp:376]     Train net output #0: loss = 0.0698275 (* 1 = 0.0698275 loss)
I1006 03:34:42.137694  5661 sgd_solver.cpp:172] Iteration 1200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:35:36.222232  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:35:36.307291  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:35:36.987871  5661 solver.cpp:352] Iteration 1300 (1.82309 iter/s, 54.8518s/100 iter), 7/322.7ep, loss = 0.0645189
I1006 03:35:36.987896  5661 solver.cpp:376]     Train net output #0: loss = 0.0658926 (* 1 = 0.0658926 loss)
I1006 03:35:36.987905  5661 sgd_solver.cpp:172] Iteration 1300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:36:31.800720  5661 solver.cpp:352] Iteration 1400 (1.82434 iter/s, 54.8143s/100 iter), 7.5/322.7ep, loss = 0.051497
I1006 03:36:31.800886  5661 solver.cpp:376]     Train net output #0: loss = 0.0669212 (* 1 = 0.0669212 loss)
I1006 03:36:31.800899  5661 sgd_solver.cpp:172] Iteration 1400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:37:18.189257  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:37:18.268729  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:37:26.676612  5661 solver.cpp:352] Iteration 1500 (1.82225 iter/s, 54.8773s/100 iter), 8.1/322.7ep, loss = 0.0606051
I1006 03:37:26.676637  5661 solver.cpp:376]     Train net output #0: loss = 0.0582625 (* 1 = 0.0582625 loss)
I1006 03:37:26.676648  5661 sgd_solver.cpp:172] Iteration 1500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:38:21.512675  5661 solver.cpp:352] Iteration 1600 (1.82357 iter/s, 54.8374s/100 iter), 8.6/322.7ep, loss = 0.0599845
I1006 03:38:21.513525  5661 solver.cpp:376]     Train net output #0: loss = 0.0561747 (* 1 = 0.0561747 loss)
I1006 03:38:21.513536  5661 sgd_solver.cpp:172] Iteration 1600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:39:00.365917  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:39:00.434128  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:39:16.530472  5661 solver.cpp:352] Iteration 1700 (1.81755 iter/s, 55.0192s/100 iter), 9.1/322.7ep, loss = 0.0452921
I1006 03:39:16.530498  5661 solver.cpp:376]     Train net output #0: loss = 0.0509928 (* 1 = 0.0509928 loss)
I1006 03:39:16.530505  5661 sgd_solver.cpp:172] Iteration 1700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:40:11.521136  5661 solver.cpp:352] Iteration 1800 (1.81845 iter/s, 54.992s/100 iter), 9.7/322.7ep, loss = 0.0479332
I1006 03:40:11.521306  5661 solver.cpp:376]     Train net output #0: loss = 0.0366876 (* 1 = 0.0366876 loss)
I1006 03:40:11.521313  5661 sgd_solver.cpp:172] Iteration 1800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:40:42.651269  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:40:42.712888  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:41:06.501631  5661 solver.cpp:352] Iteration 1900 (1.81878 iter/s, 54.9818s/100 iter), 10.2/322.7ep, loss = 0.0689045
I1006 03:41:06.501653  5661 solver.cpp:376]     Train net output #0: loss = 0.052348 (* 1 = 0.052348 loss)
I1006 03:41:06.501660  5661 sgd_solver.cpp:172] Iteration 1900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:42:00.930292  5661 solver.cpp:538] Iteration 2000, Testing net (#0)
I1006 03:42:10.047281  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:42:10.058584  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:42:10.148249  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.952627
I1006 03:42:10.148280  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 03:42:10.148291  5661 solver.cpp:624]     Test net output #2: loss = 0.175224 (* 1 = 0.175224 loss)
I1006 03:42:10.148311  5661 solver.cpp:283] Tests completed in 63.6482s
I1006 03:42:10.701660  5661 solver.cpp:352] Iteration 2000 (1.57114 iter/s, 63.6482s/100 iter), 10.8/322.7ep, loss = 0.0567465
I1006 03:42:10.701685  5661 solver.cpp:376]     Train net output #0: loss = 0.0480323 (* 1 = 0.0480323 loss)
I1006 03:42:10.701694  5661 sgd_solver.cpp:172] Iteration 2000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:42:10.702752  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.61 sparsity_achieved=0.598291 iter=2000
W1006 03:42:10.702766  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 03:42:12.493940  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.243333
W1006 03:42:12.493978  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 03:42:15.064985  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.578559
W1006 03:42:15.065021  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 03:42:17.768424  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.60753
W1006 03:42:17.768457  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 03:42:22.470624  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.587782
W1006 03:42:22.470657  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 03:42:26.840747  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.609375
W1006 03:42:26.840782  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 03:42:33.419888  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.607096
W1006 03:42:33.420034  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 03:42:39.388808  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.609375
W1006 03:42:39.388841  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 03:42:51.162602  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.609375
W1006 03:42:51.162642  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 03:43:00.527890  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.608938
W1006 03:43:00.527925  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 03:43:04.090281  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.609375
W1006 03:43:04.090425  5661 net.cpp:2612] out5a ni=512 no=64
W1006 03:43:05.072849  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.609809
W1006 03:43:05.072978  5661 net.cpp:2612] out3a ni=128 no=64
W1006 03:43:05.493675  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.609375
W1006 03:43:05.493758  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 03:43:06.848642  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.609375
W1006 03:43:06.848737  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 03:43:07.965569  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.609375
W1006 03:43:07.965652  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 03:43:09.133853  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.609375
W1006 03:43:09.133935  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 03:43:10.043197  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.609375
W1006 03:43:10.043278  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 03:43:10.125063  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.298394
I1006 03:43:10.125087  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 03:43:10.127431  5661 solver.cpp:389] Sparsity after update:
I1006 03:43:10.128294  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 03:43:10.128301  5661 net.cpp:2738] conv1a_param_0(0.243) 
I1006 03:43:10.128306  5661 net.cpp:2738] conv1b_param_0(0.579) 
I1006 03:43:10.128310  5661 net.cpp:2738] ctx_conv1_param_0(0.609) 
I1006 03:43:10.128314  5661 net.cpp:2738] ctx_conv2_param_0(0.609) 
I1006 03:43:10.128317  5661 net.cpp:2738] ctx_conv3_param_0(0.609) 
I1006 03:43:10.128320  5661 net.cpp:2738] ctx_conv4_param_0(0.609) 
I1006 03:43:10.128324  5661 net.cpp:2738] ctx_final_param_0(0.298) 
I1006 03:43:10.128327  5661 net.cpp:2738] out3a_param_0(0.609) 
I1006 03:43:10.128330  5661 net.cpp:2738] out5a_param_0(0.61) 
I1006 03:43:10.128334  5661 net.cpp:2738] res2a_branch2a_param_0(0.608) 
I1006 03:43:10.128337  5661 net.cpp:2738] res2a_branch2b_param_0(0.588) 
I1006 03:43:10.128340  5661 net.cpp:2738] res3a_branch2a_param_0(0.609) 
I1006 03:43:10.128343  5661 net.cpp:2738] res3a_branch2b_param_0(0.607) 
I1006 03:43:10.128346  5661 net.cpp:2738] res4a_branch2a_param_0(0.609) 
I1006 03:43:10.128350  5661 net.cpp:2738] res4a_branch2b_param_0(0.609) 
I1006 03:43:10.128353  5661 net.cpp:2738] res5a_branch2a_param_0(0.609) 
I1006 03:43:10.128357  5661 net.cpp:2738] res5a_branch2b_param_0(0.609) 
I1006 03:43:10.128360  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.63678e+06/2.69117e+06) 0.608
I1006 03:43:32.187376  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:43:32.250663  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:44:03.672726  5661 solver.cpp:352] Iteration 2100 (0.885164 iter/s, 112.973s/100 iter), 11.3/322.7ep, loss = 0.0458969
I1006 03:44:03.673537  5661 solver.cpp:376]     Train net output #0: loss = 0.050493 (* 1 = 0.050493 loss)
I1006 03:44:03.673549  5661 sgd_solver.cpp:172] Iteration 2100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:44:58.510732  5661 solver.cpp:352] Iteration 2200 (1.82351 iter/s, 54.8391s/100 iter), 11.8/322.7ep, loss = 0.0599788
I1006 03:44:58.511596  5661 solver.cpp:376]     Train net output #0: loss = 0.0427198 (* 1 = 0.0427198 loss)
I1006 03:44:58.511607  5661 sgd_solver.cpp:172] Iteration 2200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:45:14.167774  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:45:14.220379  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:45:24.061281  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 03:45:53.426108  5661 solver.cpp:352] Iteration 2300 (1.82094 iter/s, 54.9165s/100 iter), 12.4/322.7ep, loss = 0.0452127
I1006 03:45:53.426265  5661 solver.cpp:376]     Train net output #0: loss = 0.0385714 (* 1 = 0.0385714 loss)
I1006 03:45:53.426272  5661 sgd_solver.cpp:172] Iteration 2300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:46:48.319038  5661 solver.cpp:352] Iteration 2400 (1.82169 iter/s, 54.8941s/100 iter), 12.9/322.7ep, loss = 0.059745
I1006 03:46:48.319856  5661 solver.cpp:376]     Train net output #0: loss = 0.0720214 (* 1 = 0.0720214 loss)
I1006 03:46:48.319865  5661 sgd_solver.cpp:172] Iteration 2400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:46:56.290885  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:46:56.332458  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:47:43.370537  5661 solver.cpp:352] Iteration 2500 (1.81644 iter/s, 55.0527s/100 iter), 13.4/322.7ep, loss = 0.053609
I1006 03:47:43.371357  5661 solver.cpp:376]     Train net output #0: loss = 0.0628927 (* 1 = 0.0628927 loss)
I1006 03:47:43.371366  5661 sgd_solver.cpp:172] Iteration 2500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:48:38.327970  5661 solver.cpp:352] Iteration 2600 (1.81955 iter/s, 54.9586s/100 iter), 14/322.7ep, loss = 0.0399118
I1006 03:48:38.328119  5661 solver.cpp:376]     Train net output #0: loss = 0.046672 (* 1 = 0.046672 loss)
I1006 03:48:38.328126  5661 sgd_solver.cpp:172] Iteration 2600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:48:38.616091  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:48:38.637020  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:49:33.268302  5661 solver.cpp:352] Iteration 2700 (1.82012 iter/s, 54.9415s/100 iter), 14.5/322.7ep, loss = 0.0596852
I1006 03:49:33.269127  5661 solver.cpp:376]     Train net output #0: loss = 0.0623409 (* 1 = 0.0623409 loss)
I1006 03:49:33.269135  5661 sgd_solver.cpp:172] Iteration 2700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:50:20.619664  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:50:20.706045  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:50:28.237752  5661 solver.cpp:352] Iteration 2800 (1.81915 iter/s, 54.9706s/100 iter), 15.1/322.7ep, loss = 0.0835018
I1006 03:50:28.237777  5661 solver.cpp:376]     Train net output #0: loss = 0.0516234 (* 1 = 0.0516234 loss)
I1006 03:50:28.237785  5661 sgd_solver.cpp:172] Iteration 2800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:51:23.121686  5661 solver.cpp:352] Iteration 2900 (1.82199 iter/s, 54.8851s/100 iter), 15.6/322.7ep, loss = 0.0472541
I1006 03:51:23.122491  5661 solver.cpp:376]     Train net output #0: loss = 0.0406609 (* 1 = 0.0406609 loss)
I1006 03:51:23.122500  5661 sgd_solver.cpp:172] Iteration 2900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:52:02.729358  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:52:02.809597  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:52:18.027040  5661 solver.cpp:352] Iteration 3000 (1.82128 iter/s, 54.9065s/100 iter), 16.1/322.7ep, loss = 0.0446412
I1006 03:52:18.027065  5661 solver.cpp:376]     Train net output #0: loss = 0.0437659 (* 1 = 0.0437659 loss)
I1006 03:52:18.027072  5661 sgd_solver.cpp:172] Iteration 3000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:52:18.028107  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.62 sparsity_achieved=0.608204 iter=3000
W1006 03:52:18.028121  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 03:52:19.826462  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.2525
W1006 03:52:19.826567  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 03:52:22.409966  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.59158
W1006 03:52:22.410095  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 03:52:25.305537  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.618001
W1006 03:52:25.305667  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 03:52:30.660509  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.601454
W1006 03:52:30.660571  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 03:52:35.428243  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.619792
W1006 03:52:35.428419  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 03:52:42.957253  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.617757
W1006 03:52:42.957383  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 03:52:48.535217  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.619792
W1006 03:52:48.535315  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 03:52:59.218041  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.619792
W1006 03:52:59.218152  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 03:53:06.136595  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.618908
W1006 03:53:06.136703  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 03:53:09.325104  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.619792
W1006 03:53:09.325217  5661 net.cpp:2612] out5a ni=512 no=64
W1006 03:53:10.347465  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.619792
W1006 03:53:10.347506  5661 net.cpp:2612] out3a ni=128 no=64
W1006 03:53:10.729269  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.619765
W1006 03:53:10.729354  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 03:53:12.265164  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.619792
W1006 03:53:12.265249  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 03:53:13.453858  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.619792
W1006 03:53:13.453893  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 03:53:14.629330  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.619792
W1006 03:53:14.629410  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 03:53:15.593694  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.619792
W1006 03:53:15.593827  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 03:53:15.683933  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.305556
I1006 03:53:15.684005  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 03:53:15.686380  5661 solver.cpp:389] Sparsity after update:
I1006 03:53:15.687309  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 03:53:15.687317  5661 net.cpp:2738] conv1a_param_0(0.252) 
I1006 03:53:15.687324  5661 net.cpp:2738] conv1b_param_0(0.592) 
I1006 03:53:15.687327  5661 net.cpp:2738] ctx_conv1_param_0(0.62) 
I1006 03:53:15.687330  5661 net.cpp:2738] ctx_conv2_param_0(0.62) 
I1006 03:53:15.687333  5661 net.cpp:2738] ctx_conv3_param_0(0.62) 
I1006 03:53:15.687337  5661 net.cpp:2738] ctx_conv4_param_0(0.62) 
I1006 03:53:15.687340  5661 net.cpp:2738] ctx_final_param_0(0.306) 
I1006 03:53:15.687343  5661 net.cpp:2738] out3a_param_0(0.62) 
I1006 03:53:15.687346  5661 net.cpp:2738] out5a_param_0(0.62) 
I1006 03:53:15.687350  5661 net.cpp:2738] res2a_branch2a_param_0(0.618) 
I1006 03:53:15.687353  5661 net.cpp:2738] res2a_branch2b_param_0(0.601) 
I1006 03:53:15.687356  5661 net.cpp:2738] res3a_branch2a_param_0(0.62) 
I1006 03:53:15.687360  5661 net.cpp:2738] res3a_branch2b_param_0(0.618) 
I1006 03:53:15.687362  5661 net.cpp:2738] res4a_branch2a_param_0(0.62) 
I1006 03:53:15.687366  5661 net.cpp:2738] res4a_branch2b_param_0(0.62) 
I1006 03:53:15.687371  5661 net.cpp:2738] res5a_branch2a_param_0(0.619) 
I1006 03:53:15.687374  5661 net.cpp:2738] res5a_branch2b_param_0(0.62) 
I1006 03:53:15.687378  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.66425e+06/2.69117e+06) 0.618
I1006 03:54:09.192150  5661 solver.cpp:352] Iteration 3100 (0.899547 iter/s, 111.167s/100 iter), 16.7/322.7ep, loss = 0.0496339
I1006 03:54:09.193035  5661 solver.cpp:376]     Train net output #0: loss = 0.0476505 (* 1 = 0.0476505 loss)
I1006 03:54:09.193044  5661 sgd_solver.cpp:172] Iteration 3100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:54:41.052650  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:54:41.126365  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:55:04.022677  5661 solver.cpp:352] Iteration 3200 (1.82377 iter/s, 54.8316s/100 iter), 17.2/322.7ep, loss = 0.0533588
I1006 03:55:04.022701  5661 solver.cpp:376]     Train net output #0: loss = 0.0528835 (* 1 = 0.0528835 loss)
I1006 03:55:04.022707  5661 sgd_solver.cpp:172] Iteration 3200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:55:58.901490  5661 solver.cpp:352] Iteration 3300 (1.82216 iter/s, 54.8799s/100 iter), 17.7/322.7ep, loss = 0.0825388
I1006 03:55:58.902285  5661 solver.cpp:376]     Train net output #0: loss = 0.108931 (* 1 = 0.108931 loss)
I1006 03:55:58.902298  5661 sgd_solver.cpp:172] Iteration 3300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:56:23.102054  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:56:23.168287  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:56:53.762972  5661 solver.cpp:352] Iteration 3400 (1.82274 iter/s, 54.8626s/100 iter), 18.3/322.7ep, loss = 0.0531156
I1006 03:56:53.763773  5661 solver.cpp:376]     Train net output #0: loss = 0.0425745 (* 1 = 0.0425745 loss)
I1006 03:56:53.763782  5661 sgd_solver.cpp:172] Iteration 3400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:57:48.601075  5661 solver.cpp:352] Iteration 3500 (1.82351 iter/s, 54.8392s/100 iter), 18.8/322.7ep, loss = 0.0604145
I1006 03:57:48.601847  5661 solver.cpp:376]     Train net output #0: loss = 0.0771023 (* 1 = 0.0771023 loss)
I1006 03:57:48.601856  5661 sgd_solver.cpp:172] Iteration 3500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:58:05.135109  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:58:05.193471  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:58:43.467973  5661 solver.cpp:352] Iteration 3600 (1.82256 iter/s, 54.868s/100 iter), 19.4/322.7ep, loss = 0.119775
I1006 03:58:43.468792  5661 solver.cpp:376]     Train net output #0: loss = 0.100045 (* 1 = 0.100045 loss)
I1006 03:58:43.468801  5661 sgd_solver.cpp:172] Iteration 3600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:59:38.397914  5661 solver.cpp:352] Iteration 3700 (1.82046 iter/s, 54.931s/100 iter), 19.9/322.7ep, loss = 0.0593291
I1006 03:59:38.398731  5661 solver.cpp:376]     Train net output #0: loss = 0.0510931 (* 1 = 0.0510931 loss)
I1006 03:59:38.398741  5661 sgd_solver.cpp:172] Iteration 3700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 03:59:47.194792  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 03:59:47.250067  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:00:33.257437  5661 solver.cpp:352] Iteration 3800 (1.8228 iter/s, 54.8606s/100 iter), 20.4/322.7ep, loss = 0.0530946
I1006 04:00:33.258257  5661 solver.cpp:376]     Train net output #0: loss = 0.0449912 (* 1 = 0.0449912 loss)
I1006 04:00:33.258266  5661 sgd_solver.cpp:172] Iteration 3800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:01:28.111372  5661 solver.cpp:352] Iteration 3900 (1.82299 iter/s, 54.855s/100 iter), 21/322.7ep, loss = 0.0452397
I1006 04:01:28.112169  5661 solver.cpp:376]     Train net output #0: loss = 0.045882 (* 1 = 0.045882 loss)
I1006 04:01:28.112177  5661 sgd_solver.cpp:172] Iteration 3900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:01:29.228018  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:01:29.272016  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:02:22.529311  5661 solver.cpp:538] Iteration 4000, Testing net (#0)
I1006 04:02:31.431339  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:02:31.442539  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:02:31.531014  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.951034
I1006 04:02:31.531041  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 04:02:31.531050  5661 solver.cpp:624]     Test net output #2: loss = 0.174456 (* 1 = 0.174456 loss)
I1006 04:02:31.531069  5661 solver.cpp:283] Tests completed in 63.421s
I1006 04:02:32.081892  5661 solver.cpp:352] Iteration 4000 (1.57677 iter/s, 63.421s/100 iter), 21.5/322.7ep, loss = 0.059773
I1006 04:02:32.081920  5661 solver.cpp:376]     Train net output #0: loss = 0.0563416 (* 1 = 0.0563416 loss)
I1006 04:02:32.081946  5661 sgd_solver.cpp:172] Iteration 4000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:02:32.083468  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.63 sparsity_achieved=0.618411 iter=4000
W1006 04:02:32.083497  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 04:02:33.913251  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.2525
W1006 04:02:33.913381  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 04:02:36.404721  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.603299
W1006 04:02:36.404826  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 04:02:39.355382  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.628418
W1006 04:02:39.355499  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 04:02:44.626772  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.608615
W1006 04:02:44.626880  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 04:02:48.438664  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.628472
W1006 04:02:48.438760  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 04:02:55.463238  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.628228
W1006 04:02:55.463392  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 04:03:00.640149  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.62934
W1006 04:03:00.640235  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 04:03:09.795842  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.628472
W1006 04:03:09.795929  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 04:03:15.593122  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.628878
W1006 04:03:15.593170  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 04:03:18.505635  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.62934
W1006 04:03:18.505750  5661 net.cpp:2612] out5a ni=512 no=64
W1006 04:03:19.242594  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.629774
W1006 04:03:19.242667  5661 net.cpp:2612] out3a ni=128 no=64
W1006 04:03:19.537273  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.628472
W1006 04:03:19.537300  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 04:03:20.781679  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.628472
W1006 04:03:20.781769  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 04:03:21.775907  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.628472
W1006 04:03:21.775945  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 04:03:22.907665  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.628472
W1006 04:03:22.907707  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 04:03:23.842376  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.628472
W1006 04:03:23.842455  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 04:03:23.934841  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.307075
I1006 04:03:23.934864  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 04:03:23.937242  5661 solver.cpp:389] Sparsity after update:
I1006 04:03:23.938175  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 04:03:23.938184  5661 net.cpp:2738] conv1a_param_0(0.252) 
I1006 04:03:23.938190  5661 net.cpp:2738] conv1b_param_0(0.603) 
I1006 04:03:23.938194  5661 net.cpp:2738] ctx_conv1_param_0(0.628) 
I1006 04:03:23.938197  5661 net.cpp:2738] ctx_conv2_param_0(0.628) 
I1006 04:03:23.938200  5661 net.cpp:2738] ctx_conv3_param_0(0.628) 
I1006 04:03:23.938205  5661 net.cpp:2738] ctx_conv4_param_0(0.628) 
I1006 04:03:23.938207  5661 net.cpp:2738] ctx_final_param_0(0.307) 
I1006 04:03:23.938210  5661 net.cpp:2738] out3a_param_0(0.628) 
I1006 04:03:23.938213  5661 net.cpp:2738] out5a_param_0(0.63) 
I1006 04:03:23.938217  5661 net.cpp:2738] res2a_branch2a_param_0(0.628) 
I1006 04:03:23.938220  5661 net.cpp:2738] res2a_branch2b_param_0(0.609) 
I1006 04:03:23.938225  5661 net.cpp:2738] res3a_branch2a_param_0(0.628) 
I1006 04:03:23.938227  5661 net.cpp:2738] res3a_branch2b_param_0(0.628) 
I1006 04:03:23.938230  5661 net.cpp:2738] res4a_branch2a_param_0(0.629) 
I1006 04:03:23.938235  5661 net.cpp:2738] res4a_branch2b_param_0(0.628) 
I1006 04:03:23.938237  5661 net.cpp:2738] res5a_branch2a_param_0(0.629) 
I1006 04:03:23.938241  5661 net.cpp:2738] res5a_branch2b_param_0(0.629) 
I1006 04:03:23.938243  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.69013e+06/2.69117e+06) 0.628
I1006 04:04:10.992332  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:04:11.013536  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:04:17.558202  5661 solver.cpp:352] Iteration 4100 (0.948064 iter/s, 105.478s/100 iter), 22.1/322.7ep, loss = 0.0414312
I1006 04:04:17.558224  5661 solver.cpp:376]     Train net output #0: loss = 0.0320954 (* 1 = 0.0320954 loss)
I1006 04:04:17.558231  5661 sgd_solver.cpp:172] Iteration 4100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:05:12.475461  5661 solver.cpp:352] Iteration 4200 (1.82089 iter/s, 54.9183s/100 iter), 22.6/322.7ep, loss = 0.0544026
I1006 04:05:12.476272  5661 solver.cpp:376]     Train net output #0: loss = 0.0664138 (* 1 = 0.0664138 loss)
I1006 04:05:12.476282  5661 sgd_solver.cpp:172] Iteration 4200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:05:52.890606  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:05:52.975872  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:06:07.352788  5661 solver.cpp:352] Iteration 4300 (1.82221 iter/s, 54.8784s/100 iter), 23.1/322.7ep, loss = 0.0832534
I1006 04:06:07.352813  5661 solver.cpp:376]     Train net output #0: loss = 0.0462225 (* 1 = 0.0462225 loss)
I1006 04:06:07.352821  5661 sgd_solver.cpp:172] Iteration 4300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:07:02.224469  5661 solver.cpp:352] Iteration 4400 (1.82243 iter/s, 54.8719s/100 iter), 23.7/322.7ep, loss = 0.0475041
I1006 04:07:02.224602  5661 solver.cpp:376]     Train net output #0: loss = 0.0495301 (* 1 = 0.0495301 loss)
I1006 04:07:02.224612  5661 sgd_solver.cpp:172] Iteration 4400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:07:34.921236  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:07:35.000499  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:07:55.198897  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 04:07:57.108966  5661 solver.cpp:352] Iteration 4500 (1.822 iter/s, 54.8847s/100 iter), 24.2/322.7ep, loss = 0.0641093
I1006 04:07:57.108989  5661 solver.cpp:376]     Train net output #0: loss = 0.0878394 (* 1 = 0.0878394 loss)
I1006 04:07:57.108995  5661 sgd_solver.cpp:172] Iteration 4500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:08:52.108700  5661 solver.cpp:352] Iteration 4600 (1.81818 iter/s, 55s/100 iter), 24.7/322.7ep, loss = 0.0452944
I1006 04:08:52.108965  5661 solver.cpp:376]     Train net output #0: loss = 0.0527747 (* 1 = 0.0527747 loss)
I1006 04:08:52.108973  5661 sgd_solver.cpp:172] Iteration 4600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:09:17.098737  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:09:17.173403  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:09:47.050843  5661 solver.cpp:352] Iteration 4700 (1.82008 iter/s, 54.9425s/100 iter), 25.3/322.7ep, loss = 0.0403239
I1006 04:09:47.051712  5661 solver.cpp:376]     Train net output #0: loss = 0.049054 (* 1 = 0.049054 loss)
I1006 04:09:47.051720  5661 sgd_solver.cpp:172] Iteration 4700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:10:42.106988  5661 solver.cpp:352] Iteration 4800 (1.81631 iter/s, 55.0566s/100 iter), 25.8/322.7ep, loss = 0.0473575
I1006 04:10:42.107805  5661 solver.cpp:376]     Train net output #0: loss = 0.0387535 (* 1 = 0.0387535 loss)
I1006 04:10:42.107817  5661 sgd_solver.cpp:172] Iteration 4800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:10:59.464320  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:10:59.538493  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:11:37.032702  5661 solver.cpp:352] Iteration 4900 (1.82063 iter/s, 54.9262s/100 iter), 26.4/322.7ep, loss = 0.0521056
I1006 04:11:37.033557  5661 solver.cpp:376]     Train net output #0: loss = 0.0586973 (* 1 = 0.0586973 loss)
I1006 04:11:37.033567  5661 sgd_solver.cpp:172] Iteration 4900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:12:32.077131  5661 solver.cpp:352] Iteration 5000 (1.8167 iter/s, 55.0449s/100 iter), 26.9/322.7ep, loss = 0.056354
I1006 04:12:32.077395  5661 solver.cpp:376]     Train net output #0: loss = 0.0602167 (* 1 = 0.0602167 loss)
I1006 04:12:32.077409  5661 sgd_solver.cpp:172] Iteration 5000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:12:32.078672  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.64 sparsity_achieved=0.628028 iter=5000
W1006 04:12:32.078691  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 04:12:34.059409  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.2625
W1006 04:12:34.059566  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 04:12:37.003878  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.612847
W1006 04:12:37.004014  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 04:12:40.100342  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.638889
W1006 04:12:40.100437  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 04:12:45.640739  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.620877
W1006 04:12:45.640866  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 04:12:50.332204  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.638889
W1006 04:12:50.332317  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 04:12:57.838923  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.638591
W1006 04:12:57.839030  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 04:13:03.277393  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.639757
W1006 04:13:03.277484  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 04:13:13.325343  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.638889
W1006 04:13:13.325446  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 04:13:18.548089  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.638852
W1006 04:13:18.548192  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 04:13:21.561584  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.639757
W1006 04:13:21.561697  5661 net.cpp:2612] out5a ni=512 no=64
W1006 04:13:22.291914  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.639757
W1006 04:13:22.292040  5661 net.cpp:2612] out3a ni=128 no=64
W1006 04:13:22.599050  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.638889
W1006 04:13:22.599164  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 04:13:23.853443  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.638889
W1006 04:13:23.853520  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 04:13:24.817582  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.638889
W1006 04:13:24.817723  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 04:13:25.877002  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.638889
W1006 04:13:25.877127  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 04:13:26.835986  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.638889
W1006 04:13:26.836122  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 04:13:26.927400  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.27908
I1006 04:13:26.927429  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 04:13:26.929781  5661 solver.cpp:389] Sparsity after update:
I1006 04:13:26.930677  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 04:13:26.930685  5661 net.cpp:2738] conv1a_param_0(0.262) 
I1006 04:13:26.930691  5661 net.cpp:2738] conv1b_param_0(0.613) 
I1006 04:13:26.930694  5661 net.cpp:2738] ctx_conv1_param_0(0.639) 
I1006 04:13:26.930698  5661 net.cpp:2738] ctx_conv2_param_0(0.639) 
I1006 04:13:26.930701  5661 net.cpp:2738] ctx_conv3_param_0(0.639) 
I1006 04:13:26.930704  5661 net.cpp:2738] ctx_conv4_param_0(0.639) 
I1006 04:13:26.930707  5661 net.cpp:2738] ctx_final_param_0(0.279) 
I1006 04:13:26.930711  5661 net.cpp:2738] out3a_param_0(0.639) 
I1006 04:13:26.930713  5661 net.cpp:2738] out5a_param_0(0.64) 
I1006 04:13:26.930716  5661 net.cpp:2738] res2a_branch2a_param_0(0.639) 
I1006 04:13:26.930719  5661 net.cpp:2738] res2a_branch2b_param_0(0.621) 
I1006 04:13:26.930722  5661 net.cpp:2738] res3a_branch2a_param_0(0.639) 
I1006 04:13:26.930725  5661 net.cpp:2738] res3a_branch2b_param_0(0.639) 
I1006 04:13:26.930729  5661 net.cpp:2738] res4a_branch2a_param_0(0.64) 
I1006 04:13:26.930732  5661 net.cpp:2738] res4a_branch2b_param_0(0.639) 
I1006 04:13:26.930735  5661 net.cpp:2738] res5a_branch2a_param_0(0.639) 
I1006 04:13:26.930738  5661 net.cpp:2738] res5a_branch2b_param_0(0.64) 
I1006 04:13:26.930743  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.71741e+06/2.69117e+06) 0.638
I1006 04:13:35.714342  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:13:35.772586  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:14:20.517051  5661 solver.cpp:352] Iteration 5100 (0.922162 iter/s, 108.441s/100 iter), 27.4/322.7ep, loss = 0.0473396
I1006 04:14:20.517854  5661 solver.cpp:376]     Train net output #0: loss = 0.0348692 (* 1 = 0.0348692 loss)
I1006 04:14:20.517863  5661 sgd_solver.cpp:172] Iteration 5100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:15:15.362690  5661 solver.cpp:352] Iteration 5200 (1.82328 iter/s, 54.8462s/100 iter), 28/322.7ep, loss = 0.0564128
I1006 04:15:15.363505  5661 solver.cpp:376]     Train net output #0: loss = 0.052776 (* 1 = 0.052776 loss)
I1006 04:15:15.363513  5661 sgd_solver.cpp:172] Iteration 5200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:15:17.304666  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:15:17.357172  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:16:10.189797  5661 solver.cpp:352] Iteration 5300 (1.82389 iter/s, 54.8277s/100 iter), 28.5/322.7ep, loss = 0.0392944
I1006 04:16:10.190567  5661 solver.cpp:376]     Train net output #0: loss = 0.0354304 (* 1 = 0.0354304 loss)
I1006 04:16:10.190575  5661 sgd_solver.cpp:172] Iteration 5300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:16:59.297319  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:16:59.341996  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:17:05.033236  5661 solver.cpp:352] Iteration 5400 (1.82335 iter/s, 54.8441s/100 iter), 29/322.7ep, loss = 0.0453863
I1006 04:17:05.033259  5661 solver.cpp:376]     Train net output #0: loss = 0.0458475 (* 1 = 0.0458475 loss)
I1006 04:17:05.033267  5661 sgd_solver.cpp:172] Iteration 5400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:17:59.898298  5661 solver.cpp:352] Iteration 5500 (1.82263 iter/s, 54.8657s/100 iter), 29.6/322.7ep, loss = 0.0474299
I1006 04:17:59.899092  5661 solver.cpp:376]     Train net output #0: loss = 0.0558539 (* 1 = 0.0558539 loss)
I1006 04:17:59.899101  5661 sgd_solver.cpp:172] Iteration 5500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:18:41.321338  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:18:41.353302  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:18:54.781772  5661 solver.cpp:352] Iteration 5600 (1.82202 iter/s, 54.8842s/100 iter), 30.1/322.7ep, loss = 0.0448951
I1006 04:18:54.781798  5661 solver.cpp:376]     Train net output #0: loss = 0.0452105 (* 1 = 0.0452105 loss)
I1006 04:18:54.781807  5661 sgd_solver.cpp:172] Iteration 5600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:19:49.703882  5661 solver.cpp:352] Iteration 5700 (1.82074 iter/s, 54.9229s/100 iter), 30.7/322.7ep, loss = 0.0461979
I1006 04:19:49.704668  5661 solver.cpp:376]     Train net output #0: loss = 0.0451254 (* 1 = 0.0451254 loss)
I1006 04:19:49.704676  5661 sgd_solver.cpp:172] Iteration 5700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:20:23.212080  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:20:23.301975  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:20:44.589375  5661 solver.cpp:352] Iteration 5800 (1.82195 iter/s, 54.8863s/100 iter), 31.2/322.7ep, loss = 0.0458078
I1006 04:20:44.589397  5661 solver.cpp:376]     Train net output #0: loss = 0.0452349 (* 1 = 0.0452349 loss)
I1006 04:20:44.589404  5661 sgd_solver.cpp:172] Iteration 5800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:21:39.497486  5661 solver.cpp:352] Iteration 5900 (1.8212 iter/s, 54.9089s/100 iter), 31.7/322.7ep, loss = 0.0512654
I1006 04:21:39.498395  5661 solver.cpp:376]     Train net output #0: loss = 0.0505953 (* 1 = 0.0505953 loss)
I1006 04:21:39.498405  5661 sgd_solver.cpp:172] Iteration 5900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:22:05.400130  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:22:05.485363  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:22:34.066853  5661 solver.cpp:538] Iteration 6000, Testing net (#0)
I1006 04:22:42.888716  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:22:42.905464  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:22:42.986716  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.955082
I1006 04:22:42.986747  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 04:22:42.986758  5661 solver.cpp:624]     Test net output #2: loss = 0.180236 (* 1 = 0.180236 loss)
I1006 04:22:42.986778  5661 solver.cpp:283] Tests completed in 63.4902s
I1006 04:22:43.538205  5661 solver.cpp:352] Iteration 6000 (1.57505 iter/s, 63.4902s/100 iter), 32.3/322.7ep, loss = 0.045652
I1006 04:22:43.538229  5661 solver.cpp:376]     Train net output #0: loss = 0.0513577 (* 1 = 0.0513577 loss)
I1006 04:22:43.538236  5661 sgd_solver.cpp:172] Iteration 6000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:22:43.539269  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.65 sparsity_achieved=0.638166 iter=6000
W1006 04:22:43.539283  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 04:22:45.286021  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.262917
W1006 04:22:45.286144  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 04:22:47.645475  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.614149
W1006 04:22:47.645606  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 04:22:50.339912  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.649251
W1006 04:22:50.340049  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 04:22:55.473635  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.627279
W1006 04:22:55.473811  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 04:22:59.772215  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.649306
W1006 04:22:59.772301  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 04:23:06.891616  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.64898
W1006 04:23:06.891723  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 04:23:11.759179  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.649306
W1006 04:23:11.759281  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 04:23:21.223280  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.649272
W1006 04:23:21.223431  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 04:23:25.830752  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.64882
W1006 04:23:25.830857  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 04:23:28.625748  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.649306
W1006 04:23:28.625833  5661 net.cpp:2612] out5a ni=512 no=64
W1006 04:23:29.338778  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.64974
W1006 04:23:29.338857  5661 net.cpp:2612] out3a ni=128 no=64
W1006 04:23:29.645026  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.649306
W1006 04:23:29.645061  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 04:23:30.974603  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.649306
W1006 04:23:30.974687  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 04:23:32.046325  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.649306
W1006 04:23:32.046406  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 04:23:33.201447  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.649306
W1006 04:23:33.201525  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 04:23:34.202142  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.649306
W1006 04:23:34.202222  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 04:23:34.299677  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.282552
I1006 04:23:34.299701  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 04:23:34.302072  5661 solver.cpp:389] Sparsity after update:
I1006 04:23:34.302996  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 04:23:34.303005  5661 net.cpp:2738] conv1a_param_0(0.263) 
I1006 04:23:34.303010  5661 net.cpp:2738] conv1b_param_0(0.614) 
I1006 04:23:34.303014  5661 net.cpp:2738] ctx_conv1_param_0(0.649) 
I1006 04:23:34.303019  5661 net.cpp:2738] ctx_conv2_param_0(0.649) 
I1006 04:23:34.303021  5661 net.cpp:2738] ctx_conv3_param_0(0.649) 
I1006 04:23:34.303025  5661 net.cpp:2738] ctx_conv4_param_0(0.649) 
I1006 04:23:34.303027  5661 net.cpp:2738] ctx_final_param_0(0.283) 
I1006 04:23:34.303030  5661 net.cpp:2738] out3a_param_0(0.649) 
I1006 04:23:34.303033  5661 net.cpp:2738] out5a_param_0(0.65) 
I1006 04:23:34.303037  5661 net.cpp:2738] res2a_branch2a_param_0(0.649) 
I1006 04:23:34.303040  5661 net.cpp:2738] res2a_branch2b_param_0(0.627) 
I1006 04:23:34.303043  5661 net.cpp:2738] res3a_branch2a_param_0(0.649) 
I1006 04:23:34.303046  5661 net.cpp:2738] res3a_branch2b_param_0(0.649) 
I1006 04:23:34.303050  5661 net.cpp:2738] res4a_branch2a_param_0(0.649) 
I1006 04:23:34.303053  5661 net.cpp:2738] res4a_branch2b_param_0(0.649) 
I1006 04:23:34.303056  5661 net.cpp:2738] res5a_branch2a_param_0(0.649) 
I1006 04:23:34.303059  5661 net.cpp:2738] res5a_branch2b_param_0(0.649) 
I1006 04:23:34.303062  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.74396e+06/2.69117e+06) 0.648
I1006 04:24:27.993572  5661 solver.cpp:352] Iteration 6100 (0.957335 iter/s, 104.457s/100 iter), 32.8/322.7ep, loss = 0.0511135
I1006 04:24:27.994417  5661 solver.cpp:376]     Train net output #0: loss = 0.0580619 (* 1 = 0.0580619 loss)
I1006 04:24:27.994426  5661 sgd_solver.cpp:172] Iteration 6100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:24:46.124349  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:24:46.198920  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:25:22.860110  5661 solver.cpp:352] Iteration 6200 (1.82258 iter/s, 54.8673s/100 iter), 33.3/322.7ep, loss = 0.0721859
I1006 04:25:22.860893  5661 solver.cpp:376]     Train net output #0: loss = 0.061463 (* 1 = 0.061463 loss)
I1006 04:25:22.860901  5661 sgd_solver.cpp:172] Iteration 6200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:26:17.689504  5661 solver.cpp:352] Iteration 6300 (1.82381 iter/s, 54.8302s/100 iter), 33.9/322.7ep, loss = 0.0625022
I1006 04:26:17.690316  5661 solver.cpp:376]     Train net output #0: loss = 0.0734376 (* 1 = 0.0734376 loss)
I1006 04:26:17.690325  5661 sgd_solver.cpp:172] Iteration 6300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:26:28.141263  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:26:28.206701  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:27:12.483911  5661 solver.cpp:352] Iteration 6400 (1.82498 iter/s, 54.7952s/100 iter), 34.4/322.7ep, loss = 0.0439987
I1006 04:27:12.484719  5661 solver.cpp:376]     Train net output #0: loss = 0.0517992 (* 1 = 0.0517992 loss)
I1006 04:27:12.484730  5661 sgd_solver.cpp:172] Iteration 6400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:28:07.303542  5661 solver.cpp:352] Iteration 6500 (1.82414 iter/s, 54.8205s/100 iter), 35/322.7ep, loss = 0.0638137
I1006 04:28:07.304358  5661 solver.cpp:376]     Train net output #0: loss = 0.0577442 (* 1 = 0.0577442 loss)
I1006 04:28:07.304366  5661 sgd_solver.cpp:172] Iteration 6500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:28:10.084856  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:28:10.143882  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:29:02.210016  5661 solver.cpp:352] Iteration 6600 (1.82125 iter/s, 54.9073s/100 iter), 35.5/322.7ep, loss = 0.0531104
I1006 04:29:02.210867  5661 solver.cpp:376]     Train net output #0: loss = 0.0571377 (* 1 = 0.0571377 loss)
I1006 04:29:02.210876  5661 sgd_solver.cpp:172] Iteration 6600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:29:52.130663  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:29:52.183297  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:29:57.039033  5661 solver.cpp:352] Iteration 6700 (1.82382 iter/s, 54.8299s/100 iter), 36/322.7ep, loss = 0.050089
I1006 04:29:57.039057  5661 solver.cpp:376]     Train net output #0: loss = 0.0555372 (* 1 = 0.0555372 loss)
I1006 04:29:57.039063  5661 sgd_solver.cpp:172] Iteration 6700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:30:22.521062  5681 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 04:30:51.904814  5661 solver.cpp:352] Iteration 6800 (1.8226 iter/s, 54.8666s/100 iter), 36.6/322.7ep, loss = 0.0398961
I1006 04:30:51.904835  5661 solver.cpp:376]     Train net output #0: loss = 0.0392022 (* 1 = 0.0392022 loss)
I1006 04:30:51.904842  5661 sgd_solver.cpp:172] Iteration 6800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:31:34.142547  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:31:34.185258  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:31:46.733490  5661 solver.cpp:352] Iteration 6900 (1.82383 iter/s, 54.8296s/100 iter), 37.1/322.7ep, loss = 0.0570531
I1006 04:31:46.733515  5661 solver.cpp:376]     Train net output #0: loss = 0.054425 (* 1 = 0.054425 loss)
I1006 04:31:46.733522  5661 sgd_solver.cpp:172] Iteration 6900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:32:41.663408  5661 solver.cpp:352] Iteration 7000 (1.82047 iter/s, 54.9308s/100 iter), 37.6/322.7ep, loss = 0.0528101
I1006 04:32:41.664199  5661 solver.cpp:376]     Train net output #0: loss = 0.0502562 (* 1 = 0.0502562 loss)
I1006 04:32:41.664208  5661 sgd_solver.cpp:172] Iteration 7000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:32:41.665244  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.66 sparsity_achieved=0.648032 iter=7000
W1006 04:32:41.665259  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 04:32:43.377434  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.264167
W1006 04:32:43.377571  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 04:32:46.300477  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.623698
W1006 04:32:46.300541  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 04:32:49.579227  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.659722
W1006 04:32:49.579365  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 04:32:55.193367  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.638889
W1006 04:32:55.193493  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 04:32:59.788617  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.659722
W1006 04:32:59.788722  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 04:33:06.679517  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.659234
W1006 04:33:06.679623  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 04:33:11.152539  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.659722
W1006 04:33:11.152663  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 04:33:21.660807  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.659722
W1006 04:33:21.661537  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 04:33:26.637248  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.658787
W1006 04:33:26.637298  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 04:33:29.665307  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.659722
W1006 04:33:29.665438  5661 net.cpp:2612] out5a ni=512 no=64
W1006 04:33:30.353301  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.659722
W1006 04:33:30.353426  5661 net.cpp:2612] out3a ni=128 no=64
W1006 04:33:30.677711  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.659722
W1006 04:33:30.677733  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 04:33:32.045758  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.659722
W1006 04:33:32.045809  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 04:33:33.084213  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.659722
W1006 04:33:33.084336  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 04:33:34.245896  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.659722
W1006 04:33:34.245944  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 04:33:35.286717  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.659722
W1006 04:33:35.286801  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 04:33:35.384692  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.284939
I1006 04:33:35.384716  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 04:33:35.387069  5661 solver.cpp:389] Sparsity after update:
I1006 04:33:35.387955  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 04:33:35.387964  5661 net.cpp:2738] conv1a_param_0(0.264) 
I1006 04:33:35.387969  5661 net.cpp:2738] conv1b_param_0(0.624) 
I1006 04:33:35.387972  5661 net.cpp:2738] ctx_conv1_param_0(0.66) 
I1006 04:33:35.387975  5661 net.cpp:2738] ctx_conv2_param_0(0.66) 
I1006 04:33:35.387979  5661 net.cpp:2738] ctx_conv3_param_0(0.66) 
I1006 04:33:35.387981  5661 net.cpp:2738] ctx_conv4_param_0(0.66) 
I1006 04:33:35.387984  5661 net.cpp:2738] ctx_final_param_0(0.285) 
I1006 04:33:35.387987  5661 net.cpp:2738] out3a_param_0(0.66) 
I1006 04:33:35.387990  5661 net.cpp:2738] out5a_param_0(0.66) 
I1006 04:33:35.387993  5661 net.cpp:2738] res2a_branch2a_param_0(0.66) 
I1006 04:33:35.387996  5661 net.cpp:2738] res2a_branch2b_param_0(0.639) 
I1006 04:33:35.388000  5661 net.cpp:2738] res3a_branch2a_param_0(0.66) 
I1006 04:33:35.388002  5661 net.cpp:2738] res3a_branch2b_param_0(0.659) 
I1006 04:33:35.388005  5661 net.cpp:2738] res4a_branch2a_param_0(0.66) 
I1006 04:33:35.388008  5661 net.cpp:2738] res4a_branch2b_param_0(0.66) 
I1006 04:33:35.388011  5661 net.cpp:2738] res5a_branch2a_param_0(0.659) 
I1006 04:33:35.388016  5661 net.cpp:2738] res5a_branch2b_param_0(0.66) 
I1006 04:33:35.388020  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.77135e+06/2.69117e+06) 0.658
I1006 04:34:08.799366  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:34:08.820145  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:34:29.093158  5661 solver.cpp:352] Iteration 7100 (0.930828 iter/s, 107.431s/100 iter), 38.2/322.7ep, loss = 0.0563236
I1006 04:34:29.093183  5661 solver.cpp:376]     Train net output #0: loss = 0.0466886 (* 1 = 0.0466886 loss)
I1006 04:34:29.093194  5661 sgd_solver.cpp:172] Iteration 7100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:35:23.911473  5661 solver.cpp:352] Iteration 7200 (1.82418 iter/s, 54.8191s/100 iter), 38.7/322.7ep, loss = 0.062526
I1006 04:35:23.912273  5661 solver.cpp:376]     Train net output #0: loss = 0.0656298 (* 1 = 0.0656298 loss)
I1006 04:35:23.912282  5661 sgd_solver.cpp:172] Iteration 7200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:35:50.582621  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:35:50.668707  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:36:18.735488  5661 solver.cpp:352] Iteration 7300 (1.82399 iter/s, 54.8249s/100 iter), 39.3/322.7ep, loss = 0.0659232
I1006 04:36:18.736347  5661 solver.cpp:376]     Train net output #0: loss = 0.0718485 (* 1 = 0.0718485 loss)
I1006 04:36:18.736356  5661 sgd_solver.cpp:172] Iteration 7300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:37:13.749997  5661 solver.cpp:352] Iteration 7400 (1.81767 iter/s, 55.0154s/100 iter), 39.8/322.7ep, loss = 0.0482547
I1006 04:37:13.750823  5661 solver.cpp:376]     Train net output #0: loss = 0.0441554 (* 1 = 0.0441554 loss)
I1006 04:37:13.750834  5661 sgd_solver.cpp:172] Iteration 7400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:37:32.755025  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:37:32.839010  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:38:08.856782  5661 solver.cpp:352] Iteration 7500 (1.81463 iter/s, 55.1077s/100 iter), 40.3/322.7ep, loss = 0.046892
I1006 04:38:08.857631  5661 solver.cpp:376]     Train net output #0: loss = 0.0414454 (* 1 = 0.0414454 loss)
I1006 04:38:08.857640  5661 sgd_solver.cpp:172] Iteration 7500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:39:03.880007  5661 solver.cpp:352] Iteration 7600 (1.81739 iter/s, 55.0241s/100 iter), 40.9/322.7ep, loss = 0.0501791
I1006 04:39:03.880812  5661 solver.cpp:376]     Train net output #0: loss = 0.0300697 (* 1 = 0.0300697 loss)
I1006 04:39:03.880822  5661 sgd_solver.cpp:172] Iteration 7600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:39:15.185019  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:39:15.257422  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:39:58.870898  5661 solver.cpp:352] Iteration 7700 (1.81845 iter/s, 54.9918s/100 iter), 41.4/322.7ep, loss = 0.0712933
I1006 04:39:58.871681  5661 solver.cpp:376]     Train net output #0: loss = 0.0453091 (* 1 = 0.0453091 loss)
I1006 04:39:58.871691  5661 sgd_solver.cpp:172] Iteration 7700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:40:53.890393  5661 solver.cpp:352] Iteration 7800 (1.81761 iter/s, 55.0173s/100 iter), 41.9/322.7ep, loss = 0.0478668
I1006 04:40:53.891243  5661 solver.cpp:376]     Train net output #0: loss = 0.0536989 (* 1 = 0.0536989 loss)
I1006 04:40:53.891254  5661 sgd_solver.cpp:172] Iteration 7800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:40:57.516430  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:40:57.584956  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:41:49.024343  5661 solver.cpp:352] Iteration 7900 (1.81388 iter/s, 55.1304s/100 iter), 42.5/322.7ep, loss = 0.0825862
I1006 04:41:49.025162  5661 solver.cpp:376]     Train net output #0: loss = 0.0610843 (* 1 = 0.0610843 loss)
I1006 04:41:49.025171  5661 sgd_solver.cpp:172] Iteration 7900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:42:39.979310  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:42:40.038529  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:42:43.503646  5661 solver.cpp:538] Iteration 8000, Testing net (#0)
I1006 04:42:52.282950  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:42:52.294657  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:42:52.384757  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.950524
I1006 04:42:52.384788  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 04:42:52.384799  5661 solver.cpp:624]     Test net output #2: loss = 0.171749 (* 1 = 0.171749 loss)
I1006 04:42:52.384819  5661 solver.cpp:283] Tests completed in 63.3569s
I1006 04:42:52.935535  5661 solver.cpp:352] Iteration 8000 (1.57836 iter/s, 63.3569s/100 iter), 43/322.7ep, loss = 0.0618197
I1006 04:42:52.935561  5661 solver.cpp:376]     Train net output #0: loss = 0.0586962 (* 1 = 0.0586962 loss)
I1006 04:42:52.935569  5661 sgd_solver.cpp:172] Iteration 8000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:42:52.936610  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.67 sparsity_achieved=0.658209 iter=8000
W1006 04:42:52.936625  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 04:42:54.780794  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.272917
W1006 04:42:54.780917  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 04:42:57.335304  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.630642
W1006 04:42:57.335353  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 04:42:59.920403  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.666667
W1006 04:42:59.920532  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 04:43:05.256386  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.645508
W1006 04:43:05.256505  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 04:43:09.570128  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.668403
W1006 04:43:09.570282  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 04:43:17.304186  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.666423
W1006 04:43:17.304327  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 04:43:22.434057  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.669271
W1006 04:43:22.434192  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 04:43:32.144701  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.668403
W1006 04:43:32.144826  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 04:43:36.877120  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.668754
W1006 04:43:36.877252  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 04:43:40.292627  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.669271
W1006 04:43:40.292752  5661 net.cpp:2612] out5a ni=512 no=64
W1006 04:43:41.057852  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.669705
W1006 04:43:41.057883  5661 net.cpp:2612] out3a ni=128 no=64
W1006 04:43:41.378908  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.668403
W1006 04:43:41.378986  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 04:43:42.769084  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.668403
W1006 04:43:42.769165  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 04:43:43.784327  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.668403
W1006 04:43:43.784407  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 04:43:44.850481  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.668403
W1006 04:43:44.850512  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 04:43:45.768530  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.668403
W1006 04:43:45.768623  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 04:43:45.863085  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.315538
I1006 04:43:45.863111  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 04:43:45.865451  5661 solver.cpp:389] Sparsity after update:
I1006 04:43:45.866319  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 04:43:45.866328  5661 net.cpp:2738] conv1a_param_0(0.273) 
I1006 04:43:45.866333  5661 net.cpp:2738] conv1b_param_0(0.631) 
I1006 04:43:45.866335  5661 net.cpp:2738] ctx_conv1_param_0(0.668) 
I1006 04:43:45.866339  5661 net.cpp:2738] ctx_conv2_param_0(0.668) 
I1006 04:43:45.866343  5661 net.cpp:2738] ctx_conv3_param_0(0.668) 
I1006 04:43:45.866345  5661 net.cpp:2738] ctx_conv4_param_0(0.668) 
I1006 04:43:45.866348  5661 net.cpp:2738] ctx_final_param_0(0.316) 
I1006 04:43:45.866351  5661 net.cpp:2738] out3a_param_0(0.668) 
I1006 04:43:45.866353  5661 net.cpp:2738] out5a_param_0(0.67) 
I1006 04:43:45.866356  5661 net.cpp:2738] res2a_branch2a_param_0(0.667) 
I1006 04:43:45.866359  5661 net.cpp:2738] res2a_branch2b_param_0(0.646) 
I1006 04:43:45.866363  5661 net.cpp:2738] res3a_branch2a_param_0(0.668) 
I1006 04:43:45.866365  5661 net.cpp:2738] res3a_branch2b_param_0(0.666) 
I1006 04:43:45.866369  5661 net.cpp:2738] res4a_branch2a_param_0(0.669) 
I1006 04:43:45.866371  5661 net.cpp:2738] res4a_branch2b_param_0(0.668) 
I1006 04:43:45.866374  5661 net.cpp:2738] res5a_branch2a_param_0(0.669) 
I1006 04:43:45.866377  5661 net.cpp:2738] res5a_branch2b_param_0(0.669) 
I1006 04:43:45.866380  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.79718e+06/2.69117e+06) 0.668
I1006 04:44:39.497419  5661 solver.cpp:352] Iteration 8100 (0.938468 iter/s, 106.557s/100 iter), 43.6/322.7ep, loss = 0.0482242
I1006 04:44:39.498241  5661 solver.cpp:376]     Train net output #0: loss = 0.0458209 (* 1 = 0.0458209 loss)
I1006 04:44:39.498250  5661 sgd_solver.cpp:172] Iteration 8100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:45:22.763733  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:45:22.819216  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:45:34.527844  5661 solver.cpp:352] Iteration 8200 (1.81725 iter/s, 55.0282s/100 iter), 44.1/322.7ep, loss = 0.0547535
I1006 04:45:34.527866  5661 solver.cpp:376]     Train net output #0: loss = 0.0538482 (* 1 = 0.0538482 loss)
I1006 04:45:34.527874  5661 sgd_solver.cpp:172] Iteration 8200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:46:29.555964  5661 solver.cpp:352] Iteration 8300 (1.81732 iter/s, 55.0262s/100 iter), 44.6/322.7ep, loss = 0.0409486
I1006 04:46:29.556138  5661 solver.cpp:376]     Train net output #0: loss = 0.0414571 (* 1 = 0.0414571 loss)
I1006 04:46:29.556149  5661 sgd_solver.cpp:172] Iteration 8300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:47:05.063899  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:47:05.109308  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:47:24.540419  5661 solver.cpp:352] Iteration 8400 (1.81875 iter/s, 54.9828s/100 iter), 45.2/322.7ep, loss = 0.0396688
I1006 04:47:24.540442  5661 solver.cpp:376]     Train net output #0: loss = 0.0355002 (* 1 = 0.0355002 loss)
I1006 04:47:24.540449  5661 sgd_solver.cpp:172] Iteration 8400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:48:19.586738  5661 solver.cpp:352] Iteration 8500 (1.8167 iter/s, 55.0448s/100 iter), 45.7/322.7ep, loss = 0.0340421
I1006 04:48:19.587556  5661 solver.cpp:376]     Train net output #0: loss = 0.0339955 (* 1 = 0.0339955 loss)
I1006 04:48:19.587564  5661 sgd_solver.cpp:172] Iteration 8500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:48:47.392720  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:48:47.413641  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:49:14.611398  5661 solver.cpp:352] Iteration 8600 (1.81741 iter/s, 55.0234s/100 iter), 46.3/322.7ep, loss = 0.0559855
I1006 04:49:14.611554  5661 solver.cpp:376]     Train net output #0: loss = 0.0551755 (* 1 = 0.0551755 loss)
I1006 04:49:14.611563  5661 sgd_solver.cpp:172] Iteration 8600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:50:09.569651  5661 solver.cpp:352] Iteration 8700 (1.8196 iter/s, 54.9571s/100 iter), 46.8/322.7ep, loss = 0.0523322
I1006 04:50:09.570447  5661 solver.cpp:376]     Train net output #0: loss = 0.068775 (* 1 = 0.068775 loss)
I1006 04:50:09.570456  5661 sgd_solver.cpp:172] Iteration 8700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:50:29.367120  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:50:29.453721  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:51:04.436586  5661 solver.cpp:352] Iteration 8800 (1.82262 iter/s, 54.866s/100 iter), 47.3/322.7ep, loss = 0.0555283
I1006 04:51:04.437404  5661 solver.cpp:376]     Train net output #0: loss = 0.0450112 (* 1 = 0.0450112 loss)
I1006 04:51:04.437413  5661 sgd_solver.cpp:172] Iteration 8800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:51:59.332952  5661 solver.cpp:352] Iteration 8900 (1.82164 iter/s, 54.8955s/100 iter), 47.9/322.7ep, loss = 0.0482465
I1006 04:51:59.333775  5661 solver.cpp:376]     Train net output #0: loss = 0.0485696 (* 1 = 0.0485696 loss)
I1006 04:51:59.333783  5661 sgd_solver.cpp:172] Iteration 8900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:52:11.445847  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:52:11.525990  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:52:52.287158  5681 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 04:52:54.209797  5661 solver.cpp:352] Iteration 9000 (1.82229 iter/s, 54.8761s/100 iter), 48.4/322.7ep, loss = 0.0464816
I1006 04:52:54.209822  5661 solver.cpp:376]     Train net output #0: loss = 0.0484338 (* 1 = 0.0484338 loss)
I1006 04:52:54.209830  5661 sgd_solver.cpp:172] Iteration 9000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:52:54.210878  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.68 sparsity_achieved=0.667807 iter=9000
W1006 04:52:54.210892  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 04:52:55.956663  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.272917
W1006 04:52:55.956797  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 04:52:58.794098  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.631944
W1006 04:52:58.794242  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 04:53:01.952024  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.677083
W1006 04:53:01.952157  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 04:53:07.782383  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.651693
W1006 04:53:07.782491  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 04:53:12.630414  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.678819
W1006 04:53:12.630523  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 04:53:21.332295  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.676785
W1006 04:53:21.332418  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 04:53:27.066699  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.679688
W1006 04:53:27.067466  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 04:53:37.736914  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.678819
W1006 04:53:37.737015  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 04:53:42.435168  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.678723
W1006 04:53:42.435276  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 04:53:45.629117  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.679686
W1006 04:53:45.629199  5661 net.cpp:2612] out5a ni=512 no=64
W1006 04:53:46.377053  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.679688
W1006 04:53:46.377081  5661 net.cpp:2612] out3a ni=128 no=64
W1006 04:53:46.695729  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.678819
W1006 04:53:46.695807  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 04:53:48.135627  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.678819
W1006 04:53:48.135721  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 04:53:49.229194  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.678819
W1006 04:53:49.229291  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 04:53:50.438978  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.678819
W1006 04:53:50.439060  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 04:53:51.481853  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.678819
W1006 04:53:51.481933  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 04:53:51.585362  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.294922
I1006 04:53:51.585386  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 04:53:51.587757  5661 solver.cpp:389] Sparsity after update:
I1006 04:53:51.588692  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 04:53:51.588701  5661 net.cpp:2738] conv1a_param_0(0.273) 
I1006 04:53:51.588706  5661 net.cpp:2738] conv1b_param_0(0.632) 
I1006 04:53:51.588709  5661 net.cpp:2738] ctx_conv1_param_0(0.679) 
I1006 04:53:51.588713  5661 net.cpp:2738] ctx_conv2_param_0(0.679) 
I1006 04:53:51.588716  5661 net.cpp:2738] ctx_conv3_param_0(0.679) 
I1006 04:53:51.588719  5661 net.cpp:2738] ctx_conv4_param_0(0.679) 
I1006 04:53:51.588723  5661 net.cpp:2738] ctx_final_param_0(0.295) 
I1006 04:53:51.588726  5661 net.cpp:2738] out3a_param_0(0.679) 
I1006 04:53:51.588729  5661 net.cpp:2738] out5a_param_0(0.68) 
I1006 04:53:51.588732  5661 net.cpp:2738] res2a_branch2a_param_0(0.677) 
I1006 04:53:51.588735  5661 net.cpp:2738] res2a_branch2b_param_0(0.652) 
I1006 04:53:51.588739  5661 net.cpp:2738] res3a_branch2a_param_0(0.679) 
I1006 04:53:51.588742  5661 net.cpp:2738] res3a_branch2b_param_0(0.677) 
I1006 04:53:51.588745  5661 net.cpp:2738] res4a_branch2a_param_0(0.68) 
I1006 04:53:51.588748  5661 net.cpp:2738] res4a_branch2b_param_0(0.679) 
I1006 04:53:51.588752  5661 net.cpp:2738] res5a_branch2a_param_0(0.679) 
I1006 04:53:51.588755  5661 net.cpp:2738] res5a_branch2b_param_0(0.68) 
I1006 04:53:51.588758  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.82439e+06/2.69117e+06) 0.678
I1006 04:54:45.352062  5661 solver.cpp:352] Iteration 9100 (0.899759 iter/s, 111.141s/100 iter), 48.9/322.7ep, loss = 0.048084
I1006 04:54:45.352855  5661 solver.cpp:376]     Train net output #0: loss = 0.0385027 (* 1 = 0.0385027 loss)
I1006 04:54:45.352864  5661 sgd_solver.cpp:172] Iteration 9100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:54:49.793920  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:54:49.866904  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:55:40.316318  5661 solver.cpp:352] Iteration 9200 (1.81938 iter/s, 54.9638s/100 iter), 49.5/322.7ep, loss = 0.0468384
I1006 04:55:40.317126  5661 solver.cpp:376]     Train net output #0: loss = 0.0388725 (* 1 = 0.0388725 loss)
I1006 04:55:40.317136  5661 sgd_solver.cpp:172] Iteration 9200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:56:32.069201  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:56:32.135195  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:56:35.326227  5661 solver.cpp:352] Iteration 9300 (1.81787 iter/s, 55.0095s/100 iter), 50/322.7ep, loss = 0.0540642
I1006 04:56:35.326259  5661 solver.cpp:376]     Train net output #0: loss = 0.0457892 (* 1 = 0.0457892 loss)
I1006 04:56:35.326267  5661 sgd_solver.cpp:172] Iteration 9300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:57:30.314021  5661 solver.cpp:352] Iteration 9400 (1.81859 iter/s, 54.9875s/100 iter), 50.6/322.7ep, loss = 0.0471177
I1006 04:57:30.314806  5661 solver.cpp:376]     Train net output #0: loss = 0.0456787 (* 1 = 0.0456787 loss)
I1006 04:57:30.314815  5661 sgd_solver.cpp:172] Iteration 9400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:58:14.397243  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:58:14.456427  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:58:25.431845  5661 solver.cpp:352] Iteration 9500 (1.8143 iter/s, 55.1176s/100 iter), 51.1/322.7ep, loss = 0.052938
I1006 04:58:25.431870  5661 solver.cpp:376]     Train net output #0: loss = 0.0417114 (* 1 = 0.0417114 loss)
I1006 04:58:25.431877  5661 sgd_solver.cpp:172] Iteration 9500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:59:20.451062  5661 solver.cpp:352] Iteration 9600 (1.81755 iter/s, 55.0191s/100 iter), 51.6/322.7ep, loss = 0.0615846
I1006 04:59:20.451858  5661 solver.cpp:376]     Train net output #0: loss = 0.0733372 (* 1 = 0.0733372 loss)
I1006 04:59:20.451869  5661 sgd_solver.cpp:172] Iteration 9600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 04:59:56.800340  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 04:59:56.856379  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:00:15.429298  5661 solver.cpp:352] Iteration 9700 (1.8189 iter/s, 54.9781s/100 iter), 52.2/322.7ep, loss = 0.0505213
I1006 05:00:15.429322  5661 solver.cpp:376]     Train net output #0: loss = 0.0436175 (* 1 = 0.0436175 loss)
I1006 05:00:15.429330  5661 sgd_solver.cpp:172] Iteration 9700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:01:10.523852  5661 solver.cpp:352] Iteration 9800 (1.81506 iter/s, 55.0945s/100 iter), 52.7/322.7ep, loss = 0.0518244
I1006 05:01:10.524657  5661 solver.cpp:376]     Train net output #0: loss = 0.0587558 (* 1 = 0.0587558 loss)
I1006 05:01:10.524665  5661 sgd_solver.cpp:172] Iteration 9800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:01:39.149757  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:01:39.193516  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:02:05.546860  5661 solver.cpp:352] Iteration 9900 (1.81742 iter/s, 55.023s/100 iter), 53.2/322.7ep, loss = 0.0788761
I1006 05:02:05.547015  5661 solver.cpp:376]     Train net output #0: loss = 0.068482 (* 1 = 0.068482 loss)
I1006 05:02:05.547024  5661 sgd_solver.cpp:172] Iteration 9900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:02:59.999415  5661 solver.cpp:905] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_10000.caffemodel
I1006 05:03:00.022047  5661 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_10000.solverstate
I1006 05:03:00.030318  5661 solver.cpp:538] Iteration 10000, Testing net (#0)
I1006 05:03:08.979075  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:03:08.997323  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:03:09.075501  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.951869
I1006 05:03:09.075528  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 05:03:09.075537  5661 solver.cpp:624]     Test net output #2: loss = 0.169951 (* 1 = 0.169951 loss)
I1006 05:03:09.075554  5661 solver.cpp:283] Tests completed in 63.5288s
I1006 05:03:09.627482  5661 solver.cpp:352] Iteration 10000 (1.57409 iter/s, 63.5288s/100 iter), 53.8/322.7ep, loss = 0.0577665
I1006 05:03:09.627508  5661 solver.cpp:376]     Train net output #0: loss = 0.0531481 (* 1 = 0.0531481 loss)
I1006 05:03:09.627516  5661 sgd_solver.cpp:172] Iteration 10000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:03:09.628598  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.69 sparsity_achieved=0.677918 iter=10000
W1006 05:03:09.628614  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 05:03:11.446549  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.273333
W1006 05:03:11.446674  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 05:03:14.264885  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.640191
W1006 05:03:14.264991  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 05:03:17.759963  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.687392
W1006 05:03:17.760051  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 05:03:23.268893  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.660916
W1006 05:03:23.269004  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 05:03:28.126330  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.689236
W1006 05:03:28.126416  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 05:03:36.965951  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.687229
W1006 05:03:36.966702  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 05:03:42.726090  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.689236
W1006 05:03:42.726182  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 05:03:54.008072  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.689223
W1006 05:03:54.008183  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 05:03:58.778249  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.688693
W1006 05:03:58.778349  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 05:04:02.006176  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.689236
W1006 05:04:02.006273  5661 net.cpp:2612] out5a ni=512 no=64
W1006 05:04:02.764086  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.689663
W1006 05:04:02.764111  5661 net.cpp:2612] out3a ni=128 no=64
W1006 05:04:03.095875  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.689236
W1006 05:04:03.095952  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 05:04:04.601794  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.689236
W1006 05:04:04.601830  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 05:04:05.736235  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.689236
W1006 05:04:05.736313  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 05:04:06.982379  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.689236
W1006 05:04:06.982467  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 05:04:08.078119  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.689236
W1006 05:04:08.078215  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 05:04:08.181267  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.299262
I1006 05:04:08.181289  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 05:04:08.183655  5661 solver.cpp:389] Sparsity after update:
I1006 05:04:08.184586  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 05:04:08.184594  5661 net.cpp:2738] conv1a_param_0(0.273) 
I1006 05:04:08.184600  5661 net.cpp:2738] conv1b_param_0(0.64) 
I1006 05:04:08.184603  5661 net.cpp:2738] ctx_conv1_param_0(0.689) 
I1006 05:04:08.184607  5661 net.cpp:2738] ctx_conv2_param_0(0.689) 
I1006 05:04:08.184610  5661 net.cpp:2738] ctx_conv3_param_0(0.689) 
I1006 05:04:08.184613  5661 net.cpp:2738] ctx_conv4_param_0(0.689) 
I1006 05:04:08.184617  5661 net.cpp:2738] ctx_final_param_0(0.299) 
I1006 05:04:08.184619  5661 net.cpp:2738] out3a_param_0(0.689) 
I1006 05:04:08.184623  5661 net.cpp:2738] out5a_param_0(0.69) 
I1006 05:04:08.184626  5661 net.cpp:2738] res2a_branch2a_param_0(0.687) 
I1006 05:04:08.184629  5661 net.cpp:2738] res2a_branch2b_param_0(0.661) 
I1006 05:04:08.184633  5661 net.cpp:2738] res3a_branch2a_param_0(0.689) 
I1006 05:04:08.184636  5661 net.cpp:2738] res3a_branch2b_param_0(0.687) 
I1006 05:04:08.184639  5661 net.cpp:2738] res4a_branch2a_param_0(0.689) 
I1006 05:04:08.184643  5661 net.cpp:2738] res4a_branch2b_param_0(0.689) 
I1006 05:04:08.184646  5661 net.cpp:2738] res5a_branch2a_param_0(0.689) 
I1006 05:04:08.184649  5661 net.cpp:2738] res5a_branch2b_param_0(0.689) 
I1006 05:04:08.184653  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.85099e+06/2.69117e+06) 0.688
I1006 05:04:27.741922  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:04:27.762910  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:05:01.765522  5661 solver.cpp:352] Iteration 10100 (0.891759 iter/s, 112.138s/100 iter), 54.3/322.7ep, loss = 0.0566247
I1006 05:05:01.766346  5661 solver.cpp:376]     Train net output #0: loss = 0.0540861 (* 1 = 0.0540861 loss)
I1006 05:05:01.766356  5661 sgd_solver.cpp:172] Iteration 10100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:05:56.820051  5661 solver.cpp:352] Iteration 10200 (1.81638 iter/s, 55.0546s/100 iter), 54.9/322.7ep, loss = 0.0647195
I1006 05:05:56.820858  5661 solver.cpp:376]     Train net output #0: loss = 0.0477955 (* 1 = 0.0477955 loss)
I1006 05:05:56.820868  5661 sgd_solver.cpp:172] Iteration 10200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:06:09.770906  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:06:09.858156  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:06:51.807997  5661 solver.cpp:352] Iteration 10300 (1.81858 iter/s, 54.9881s/100 iter), 55.4/322.7ep, loss = 0.0595232
I1006 05:06:51.808807  5661 solver.cpp:376]     Train net output #0: loss = 0.0772679 (* 1 = 0.0772679 loss)
I1006 05:06:51.808817  5661 sgd_solver.cpp:172] Iteration 10300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:07:46.837138  5661 solver.cpp:352] Iteration 10400 (1.81721 iter/s, 55.0293s/100 iter), 55.9/322.7ep, loss = 0.051824
I1006 05:07:46.837954  5661 solver.cpp:376]     Train net output #0: loss = 0.0629986 (* 1 = 0.0629986 loss)
I1006 05:07:46.837963  5661 sgd_solver.cpp:172] Iteration 10400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:07:52.117161  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:07:52.196107  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:08:41.825425  5661 solver.cpp:352] Iteration 10500 (1.81856 iter/s, 54.9885s/100 iter), 56.5/322.7ep, loss = 0.0660239
I1006 05:08:41.826231  5661 solver.cpp:376]     Train net output #0: loss = 0.0574586 (* 1 = 0.0574586 loss)
I1006 05:08:41.826241  5661 sgd_solver.cpp:172] Iteration 10500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:09:34.417834  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:09:34.490475  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:09:36.837694  5661 solver.cpp:352] Iteration 10600 (1.81777 iter/s, 55.0125s/100 iter), 57/322.7ep, loss = 0.0461189
I1006 05:09:36.837718  5661 solver.cpp:376]     Train net output #0: loss = 0.0528639 (* 1 = 0.0528639 loss)
I1006 05:09:36.837723  5661 sgd_solver.cpp:172] Iteration 10600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:10:31.835388  5661 solver.cpp:352] Iteration 10700 (1.81825 iter/s, 54.9979s/100 iter), 57.5/322.7ep, loss = 0.0505085
I1006 05:10:31.836181  5661 solver.cpp:376]     Train net output #0: loss = 0.0405114 (* 1 = 0.0405114 loss)
I1006 05:10:31.836191  5661 sgd_solver.cpp:172] Iteration 10700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:11:16.721433  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:11:16.783387  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:11:26.851001  5661 solver.cpp:352] Iteration 10800 (1.81766 iter/s, 55.0159s/100 iter), 58.1/322.7ep, loss = 0.0506085
I1006 05:11:26.851027  5661 solver.cpp:376]     Train net output #0: loss = 0.0543395 (* 1 = 0.0543395 loss)
I1006 05:11:26.851037  5661 sgd_solver.cpp:172] Iteration 10800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:12:21.820329  5661 solver.cpp:352] Iteration 10900 (1.81919 iter/s, 54.9696s/100 iter), 58.6/322.7ep, loss = 0.0557095
I1006 05:12:21.821147  5661 solver.cpp:376]     Train net output #0: loss = 0.0623649 (* 1 = 0.0623649 loss)
I1006 05:12:21.821157  5661 sgd_solver.cpp:172] Iteration 10900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:12:59.013620  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:12:59.071923  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:13:16.840471  5661 solver.cpp:352] Iteration 11000 (1.81751 iter/s, 55.0204s/100 iter), 59.2/322.7ep, loss = 0.0636979
I1006 05:13:16.840495  5661 solver.cpp:376]     Train net output #0: loss = 0.0667937 (* 1 = 0.0667937 loss)
I1006 05:13:16.840502  5661 sgd_solver.cpp:172] Iteration 11000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:13:16.841514  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.7 sparsity_achieved=0.687803 iter=11000
W1006 05:13:16.841528  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 05:13:18.643193  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.282083
W1006 05:13:18.643373  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 05:13:21.346231  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.647569
W1006 05:13:21.346380  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 05:13:24.888039  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.697483
W1006 05:13:24.888190  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 05:13:30.456287  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.667101
W1006 05:13:30.456336  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 05:13:35.330257  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.699653
W1006 05:13:35.330353  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 05:13:43.531325  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.697401
W1006 05:13:43.531409  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 05:13:48.959823  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.699653
W1006 05:13:48.959936  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 05:14:00.658453  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.699639
W1006 05:14:00.658587  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 05:14:05.703474  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.699653
W1006 05:14:05.703569  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 05:14:09.293946  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.699653
W1006 05:14:09.294054  5661 net.cpp:2612] out5a ni=512 no=64
W1006 05:14:10.201177  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.699653
W1006 05:14:10.201210  5661 net.cpp:2612] out3a ni=128 no=64
W1006 05:14:10.622377  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.699653
W1006 05:14:10.622472  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 05:14:12.564709  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.699653
W1006 05:14:12.564797  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 05:14:13.942898  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.699653
W1006 05:14:13.942981  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 05:14:15.288352  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.699653
W1006 05:14:15.288429  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 05:14:16.431814  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.699653
W1006 05:14:16.431896  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 05:14:16.536478  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.29145
I1006 05:14:16.536501  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 05:14:16.538872  5661 solver.cpp:389] Sparsity after update:
I1006 05:14:16.539800  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 05:14:16.539809  5661 net.cpp:2738] conv1a_param_0(0.282) 
I1006 05:14:16.539814  5661 net.cpp:2738] conv1b_param_0(0.648) 
I1006 05:14:16.539818  5661 net.cpp:2738] ctx_conv1_param_0(0.7) 
I1006 05:14:16.539821  5661 net.cpp:2738] ctx_conv2_param_0(0.7) 
I1006 05:14:16.539824  5661 net.cpp:2738] ctx_conv3_param_0(0.7) 
I1006 05:14:16.539829  5661 net.cpp:2738] ctx_conv4_param_0(0.7) 
I1006 05:14:16.539831  5661 net.cpp:2738] ctx_final_param_0(0.291) 
I1006 05:14:16.539834  5661 net.cpp:2738] out3a_param_0(0.7) 
I1006 05:14:16.539837  5661 net.cpp:2738] out5a_param_0(0.7) 
I1006 05:14:16.539841  5661 net.cpp:2738] res2a_branch2a_param_0(0.697) 
I1006 05:14:16.539844  5661 net.cpp:2738] res2a_branch2b_param_0(0.667) 
I1006 05:14:16.539847  5661 net.cpp:2738] res3a_branch2a_param_0(0.7) 
I1006 05:14:16.539851  5661 net.cpp:2738] res3a_branch2b_param_0(0.697) 
I1006 05:14:16.539855  5661 net.cpp:2738] res4a_branch2a_param_0(0.7) 
I1006 05:14:16.539857  5661 net.cpp:2738] res4a_branch2b_param_0(0.7) 
I1006 05:14:16.539860  5661 net.cpp:2738] res5a_branch2a_param_0(0.7) 
I1006 05:14:16.539865  5661 net.cpp:2738] res5a_branch2b_param_0(0.7) 
I1006 05:14:16.539870  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.87946e+06/2.69117e+06) 0.698
I1006 05:15:10.151010  5661 solver.cpp:352] Iteration 11100 (0.882479 iter/s, 113.317s/100 iter), 59.7/322.7ep, loss = 0.0466483
I1006 05:15:10.151829  5661 solver.cpp:376]     Train net output #0: loss = 0.0545825 (* 1 = 0.0545825 loss)
I1006 05:15:10.151839  5661 sgd_solver.cpp:172] Iteration 11100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:15:39.641583  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:15:39.693683  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:16:05.160538  5661 solver.cpp:352] Iteration 11200 (1.81762 iter/s, 55.0171s/100 iter), 60.2/322.7ep, loss = 0.0448908
I1006 05:16:05.160650  5661 solver.cpp:376]     Train net output #0: loss = 0.0548363 (* 1 = 0.0548363 loss)
I1006 05:16:05.160660  5661 sgd_solver.cpp:172] Iteration 11200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:16:30.739912  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 05:17:00.155645  5661 solver.cpp:352] Iteration 11300 (1.81811 iter/s, 55.0021s/100 iter), 60.8/322.7ep, loss = 0.0609753
I1006 05:17:00.156733  5661 solver.cpp:376]     Train net output #0: loss = 0.0605891 (* 1 = 0.0605891 loss)
I1006 05:17:00.156741  5661 sgd_solver.cpp:172] Iteration 11300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:17:21.920653  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:17:21.964476  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:17:55.079211  5661 solver.cpp:352] Iteration 11400 (1.8205 iter/s, 54.93s/100 iter), 61.3/322.7ep, loss = 0.0590423
I1006 05:17:55.080034  5661 solver.cpp:376]     Train net output #0: loss = 0.0585687 (* 1 = 0.0585687 loss)
I1006 05:17:55.080044  5661 sgd_solver.cpp:172] Iteration 11400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:18:49.935416  5661 solver.cpp:352] Iteration 11500 (1.82275 iter/s, 54.8621s/100 iter), 61.8/322.7ep, loss = 0.0502208
I1006 05:18:49.936197  5661 solver.cpp:376]     Train net output #0: loss = 0.050117 (* 1 = 0.050117 loss)
I1006 05:18:49.936206  5661 sgd_solver.cpp:172] Iteration 11500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:19:03.934166  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:19:03.965837  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:19:44.828438  5661 solver.cpp:352] Iteration 11600 (1.82155 iter/s, 54.8984s/100 iter), 62.4/322.7ep, loss = 0.0491628
I1006 05:19:44.828590  5661 solver.cpp:376]     Train net output #0: loss = 0.0499505 (* 1 = 0.0499505 loss)
I1006 05:19:44.828599  5661 sgd_solver.cpp:172] Iteration 11600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:20:39.659456  5661 solver.cpp:352] Iteration 11700 (1.82362 iter/s, 54.836s/100 iter), 62.9/322.7ep, loss = 0.0497151
I1006 05:20:39.660248  5661 solver.cpp:376]     Train net output #0: loss = 0.0668769 (* 1 = 0.0668769 loss)
I1006 05:20:39.660257  5661 sgd_solver.cpp:172] Iteration 11700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:20:45.751803  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:20:45.835299  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:21:34.510277  5661 solver.cpp:352] Iteration 11800 (1.82297 iter/s, 54.8554s/100 iter), 63.5/322.7ep, loss = 0.064944
I1006 05:21:34.511101  5661 solver.cpp:376]     Train net output #0: loss = 0.0731318 (* 1 = 0.0731318 loss)
I1006 05:21:34.511111  5661 sgd_solver.cpp:172] Iteration 11800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:22:27.859596  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:22:27.939687  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:22:29.453912  5661 solver.cpp:352] Iteration 11900 (1.81991 iter/s, 54.9479s/100 iter), 64/322.7ep, loss = 0.0466356
I1006 05:22:29.453938  5661 solver.cpp:376]     Train net output #0: loss = 0.0407634 (* 1 = 0.0407634 loss)
I1006 05:22:29.453945  5661 sgd_solver.cpp:172] Iteration 11900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:23:23.764794  5661 solver.cpp:538] Iteration 12000, Testing net (#0)
I1006 05:23:32.662040  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:23:32.673817  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:23:32.762188  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.951618
I1006 05:23:32.762217  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 05:23:32.762226  5661 solver.cpp:624]     Test net output #2: loss = 0.175649 (* 1 = 0.175649 loss)
I1006 05:23:32.762243  5661 solver.cpp:283] Tests completed in 63.3129s
I1006 05:23:33.310899  5661 solver.cpp:352] Iteration 12000 (1.57946 iter/s, 63.3129s/100 iter), 64.5/322.7ep, loss = 0.0599787
I1006 05:23:33.310922  5661 solver.cpp:376]     Train net output #0: loss = 0.0617978 (* 1 = 0.0617978 loss)
I1006 05:23:33.310930  5661 sgd_solver.cpp:172] Iteration 12000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:23:33.311887  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.71 sparsity_achieved=0.698379 iter=12000
W1006 05:23:33.311900  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 05:23:35.089860  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.2825
W1006 05:23:35.089910  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 05:23:37.940418  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.654948
W1006 05:23:37.940474  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 05:23:41.202214  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.707737
W1006 05:23:41.202265  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 05:23:47.044997  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.674805
W1006 05:23:47.045138  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 05:23:52.919751  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.708333
W1006 05:23:52.919883  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 05:24:01.741927  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.707357
W1006 05:24:01.742048  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 05:24:07.820935  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.709201
W1006 05:24:07.821039  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 05:24:19.963179  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.708306
W1006 05:24:19.963295  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 05:24:25.275730  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.709633
W1006 05:24:25.275780  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 05:24:29.270298  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.709201
W1006 05:24:29.270429  5661 net.cpp:2612] out5a ni=512 no=64
W1006 05:24:30.316790  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.709635
W1006 05:24:30.316916  5661 net.cpp:2612] out3a ni=128 no=64
W1006 05:24:30.689296  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.708333
W1006 05:24:30.689422  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 05:24:32.390879  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.708333
W1006 05:24:32.390964  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 05:24:33.614882  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.708333
W1006 05:24:33.614917  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 05:24:35.015558  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.708333
W1006 05:24:35.015661  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 05:24:36.314678  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.708333
W1006 05:24:36.314760  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 05:24:36.437847  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.346137
I1006 05:24:36.437876  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 05:24:36.440302  5661 solver.cpp:389] Sparsity after update:
I1006 05:24:36.441401  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 05:24:36.441412  5661 net.cpp:2738] conv1a_param_0(0.282) 
I1006 05:24:36.441418  5661 net.cpp:2738] conv1b_param_0(0.655) 
I1006 05:24:36.441424  5661 net.cpp:2738] ctx_conv1_param_0(0.708) 
I1006 05:24:36.441428  5661 net.cpp:2738] ctx_conv2_param_0(0.708) 
I1006 05:24:36.441433  5661 net.cpp:2738] ctx_conv3_param_0(0.708) 
I1006 05:24:36.441437  5661 net.cpp:2738] ctx_conv4_param_0(0.708) 
I1006 05:24:36.441442  5661 net.cpp:2738] ctx_final_param_0(0.346) 
I1006 05:24:36.441447  5661 net.cpp:2738] out3a_param_0(0.708) 
I1006 05:24:36.441452  5661 net.cpp:2738] out5a_param_0(0.71) 
I1006 05:24:36.441457  5661 net.cpp:2738] res2a_branch2a_param_0(0.708) 
I1006 05:24:36.441462  5661 net.cpp:2738] res2a_branch2b_param_0(0.675) 
I1006 05:24:36.441465  5661 net.cpp:2738] res3a_branch2a_param_0(0.708) 
I1006 05:24:36.441470  5661 net.cpp:2738] res3a_branch2b_param_0(0.707) 
I1006 05:24:36.441474  5661 net.cpp:2738] res4a_branch2a_param_0(0.709) 
I1006 05:24:36.441479  5661 net.cpp:2738] res4a_branch2b_param_0(0.708) 
I1006 05:24:36.441483  5661 net.cpp:2738] res5a_branch2a_param_0(0.71) 
I1006 05:24:36.441488  5661 net.cpp:2738] res5a_branch2b_param_0(0.709) 
I1006 05:24:36.441493  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.90556e+06/2.69117e+06) 0.708
I1006 05:25:20.614576  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:25:20.689409  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:25:29.937750  5661 solver.cpp:352] Iteration 12100 (0.857383 iter/s, 116.634s/100 iter), 65.1/322.7ep, loss = 0.0519076
I1006 05:25:29.937777  5661 solver.cpp:376]     Train net output #0: loss = 0.043239 (* 1 = 0.043239 loss)
I1006 05:25:29.937785  5661 sgd_solver.cpp:172] Iteration 12100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:26:24.868338  5661 solver.cpp:352] Iteration 12200 (1.82038 iter/s, 54.9337s/100 iter), 65.6/322.7ep, loss = 0.0471948
I1006 05:26:24.869154  5661 solver.cpp:376]     Train net output #0: loss = 0.0455946 (* 1 = 0.0455946 loss)
I1006 05:26:24.869164  5661 sgd_solver.cpp:172] Iteration 12200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:27:02.799687  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:27:02.868674  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:27:19.763085  5661 solver.cpp:352] Iteration 12300 (1.82157 iter/s, 54.8977s/100 iter), 66.2/322.7ep, loss = 0.0692549
I1006 05:27:19.763109  5661 solver.cpp:376]     Train net output #0: loss = 0.0725136 (* 1 = 0.0725136 loss)
I1006 05:27:19.763118  5661 sgd_solver.cpp:172] Iteration 12300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:28:14.710394  5661 solver.cpp:352] Iteration 12400 (1.81983 iter/s, 54.9501s/100 iter), 66.7/322.7ep, loss = 0.0622545
I1006 05:28:14.711208  5661 solver.cpp:376]     Train net output #0: loss = 0.0732858 (* 1 = 0.0732858 loss)
I1006 05:28:14.711218  5661 sgd_solver.cpp:172] Iteration 12400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:28:44.928365  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:28:44.987170  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:29:09.610200  5661 solver.cpp:352] Iteration 12500 (1.82141 iter/s, 54.9024s/100 iter), 67.2/322.7ep, loss = 0.0487192
I1006 05:29:09.610224  5661 solver.cpp:376]     Train net output #0: loss = 0.0486469 (* 1 = 0.0486469 loss)
I1006 05:29:09.610230  5661 sgd_solver.cpp:172] Iteration 12500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:30:04.521183  5661 solver.cpp:352] Iteration 12600 (1.82105 iter/s, 54.9135s/100 iter), 67.8/322.7ep, loss = 0.0634081
I1006 05:30:04.522009  5661 solver.cpp:376]     Train net output #0: loss = 0.0550856 (* 1 = 0.0550856 loss)
I1006 05:30:04.522017  5661 sgd_solver.cpp:172] Iteration 12600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:30:27.051868  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:30:27.105010  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:30:59.411679  5661 solver.cpp:352] Iteration 12700 (1.82173 iter/s, 54.8929s/100 iter), 68.3/322.7ep, loss = 0.0615606
I1006 05:30:59.412540  5661 solver.cpp:376]     Train net output #0: loss = 0.0585064 (* 1 = 0.0585064 loss)
I1006 05:30:59.412550  5661 sgd_solver.cpp:172] Iteration 12700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:31:54.371191  5661 solver.cpp:352] Iteration 12800 (1.81945 iter/s, 54.9618s/100 iter), 68.8/322.7ep, loss = 0.0589359
I1006 05:31:54.371975  5661 solver.cpp:376]     Train net output #0: loss = 0.0361117 (* 1 = 0.0361117 loss)
I1006 05:31:54.371985  5661 sgd_solver.cpp:172] Iteration 12800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:32:09.256628  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:32:09.300568  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:32:49.456256  5661 solver.cpp:352] Iteration 12900 (1.8153 iter/s, 55.0872s/100 iter), 69.4/322.7ep, loss = 0.0478543
I1006 05:32:49.457053  5661 solver.cpp:376]     Train net output #0: loss = 0.0339427 (* 1 = 0.0339427 loss)
I1006 05:32:49.457064  5661 sgd_solver.cpp:172] Iteration 12900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:33:44.368378  5661 solver.cpp:352] Iteration 13000 (1.82102 iter/s, 54.9142s/100 iter), 69.9/322.7ep, loss = 0.0498312
I1006 05:33:44.369190  5661 solver.cpp:376]     Train net output #0: loss = 0.0619461 (* 1 = 0.0619461 loss)
I1006 05:33:44.369200  5661 sgd_solver.cpp:172] Iteration 13000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:33:44.370268  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.72 sparsity_achieved=0.70808 iter=13000
W1006 05:33:44.370283  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 05:33:46.230427  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.2825
W1006 05:33:46.230506  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 05:33:49.339875  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.655816
W1006 05:33:49.339988  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 05:33:53.107993  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.717828
W1006 05:33:53.108078  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 05:33:59.477176  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.679796
W1006 05:33:59.477295  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 05:34:05.691118  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.71875
W1006 05:34:05.691224  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 05:34:15.075361  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.717367
W1006 05:34:15.075472  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 05:34:22.236534  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.719618
W1006 05:34:22.236632  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 05:34:35.417490  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.718716
W1006 05:34:35.417603  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 05:34:41.505496  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.718213
W1006 05:34:41.505609  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 05:34:45.653605  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.719618
W1006 05:34:45.653734  5661 net.cpp:2612] out5a ni=512 no=64
W1006 05:34:46.580286  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.719618
W1006 05:34:46.580385  5661 net.cpp:2612] out3a ni=128 no=64
W1006 05:34:46.939684  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.71875
W1006 05:34:46.939769  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 05:34:48.630910  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.71875
W1006 05:34:48.631036  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 05:34:49.873548  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.71875
W1006 05:34:49.873682  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 05:34:51.238948  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.71875
W1006 05:34:51.239030  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 05:34:52.491274  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.71875
W1006 05:34:52.491410  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 05:34:52.598459  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.312717
I1006 05:34:52.598490  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 05:34:52.600844  5661 solver.cpp:389] Sparsity after update:
I1006 05:34:52.601754  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 05:34:52.601763  5661 net.cpp:2738] conv1a_param_0(0.282) 
I1006 05:34:52.601768  5661 net.cpp:2738] conv1b_param_0(0.656) 
I1006 05:34:52.601770  5661 net.cpp:2738] ctx_conv1_param_0(0.719) 
I1006 05:34:52.601774  5661 net.cpp:2738] ctx_conv2_param_0(0.719) 
I1006 05:34:52.601778  5661 net.cpp:2738] ctx_conv3_param_0(0.719) 
I1006 05:34:52.601780  5661 net.cpp:2738] ctx_conv4_param_0(0.719) 
I1006 05:34:52.601783  5661 net.cpp:2738] ctx_final_param_0(0.313) 
I1006 05:34:52.601786  5661 net.cpp:2738] out3a_param_0(0.719) 
I1006 05:34:52.601789  5661 net.cpp:2738] out5a_param_0(0.72) 
I1006 05:34:52.601792  5661 net.cpp:2738] res2a_branch2a_param_0(0.718) 
I1006 05:34:52.601795  5661 net.cpp:2738] res2a_branch2b_param_0(0.68) 
I1006 05:34:52.601799  5661 net.cpp:2738] res3a_branch2a_param_0(0.719) 
I1006 05:34:52.601802  5661 net.cpp:2738] res3a_branch2b_param_0(0.717) 
I1006 05:34:52.601805  5661 net.cpp:2738] res4a_branch2a_param_0(0.72) 
I1006 05:34:52.601809  5661 net.cpp:2738] res4a_branch2b_param_0(0.719) 
I1006 05:34:52.601814  5661 net.cpp:2738] res5a_branch2a_param_0(0.718) 
I1006 05:34:52.601816  5661 net.cpp:2738] res5a_branch2b_param_0(0.72) 
I1006 05:34:52.601821  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.93104e+06/2.69117e+06) 0.718
I1006 05:34:59.147799  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:34:59.169852  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:35:46.462607  5661 solver.cpp:352] Iteration 13100 (0.819013 iter/s, 122.098s/100 iter), 70.5/322.7ep, loss = 0.0733943
I1006 05:35:46.463534  5661 solver.cpp:376]     Train net output #0: loss = 0.0600736 (* 1 = 0.0600736 loss)
I1006 05:35:46.463543  5661 sgd_solver.cpp:172] Iteration 13100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:36:40.522511  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:36:40.610328  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:36:41.293042  5661 solver.cpp:352] Iteration 13200 (1.82375 iter/s, 54.8322s/100 iter), 71/322.7ep, loss = 0.0481369
I1006 05:36:41.293068  5661 solver.cpp:376]     Train net output #0: loss = 0.0354627 (* 1 = 0.0354627 loss)
I1006 05:36:41.293076  5661 sgd_solver.cpp:172] Iteration 13200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:37:36.113931  5661 solver.cpp:352] Iteration 13300 (1.82406 iter/s, 54.8226s/100 iter), 71.5/322.7ep, loss = 0.0545078
I1006 05:37:36.114754  5661 solver.cpp:376]     Train net output #0: loss = 0.0489491 (* 1 = 0.0489491 loss)
I1006 05:37:36.114764  5661 sgd_solver.cpp:172] Iteration 13300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:38:22.515482  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:38:22.595589  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:38:30.961956  5661 solver.cpp:352] Iteration 13400 (1.82316 iter/s, 54.8497s/100 iter), 72.1/322.7ep, loss = 0.056456
I1006 05:38:30.961982  5661 solver.cpp:376]     Train net output #0: loss = 0.0608161 (* 1 = 0.0608161 loss)
I1006 05:38:30.961989  5661 sgd_solver.cpp:172] Iteration 13400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:39:24.101094  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 05:39:26.037804  5661 solver.cpp:352] Iteration 13500 (1.81562 iter/s, 55.0775s/100 iter), 72.6/322.7ep, loss = 0.0595427
I1006 05:39:26.037829  5661 solver.cpp:376]     Train net output #0: loss = 0.0632557 (* 1 = 0.0632557 loss)
I1006 05:39:26.037837  5661 sgd_solver.cpp:172] Iteration 13500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:40:04.789649  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:40:04.865988  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:40:21.015290  5661 solver.cpp:352] Iteration 13600 (1.81887 iter/s, 54.9791s/100 iter), 73.1/322.7ep, loss = 0.0671198
I1006 05:40:21.015319  5661 solver.cpp:376]     Train net output #0: loss = 0.0545357 (* 1 = 0.0545357 loss)
I1006 05:40:21.015328  5661 sgd_solver.cpp:172] Iteration 13600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:41:16.023394  5661 solver.cpp:352] Iteration 13700 (1.81786 iter/s, 55.0097s/100 iter), 73.7/322.7ep, loss = 0.0818224
I1006 05:41:16.024200  5661 solver.cpp:376]     Train net output #0: loss = 0.0507757 (* 1 = 0.0507757 loss)
I1006 05:41:16.024214  5661 sgd_solver.cpp:172] Iteration 13700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:41:47.205868  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:41:47.273861  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:42:11.136826  5661 solver.cpp:352] Iteration 13800 (1.81439 iter/s, 55.115s/100 iter), 74.2/322.7ep, loss = 0.069078
I1006 05:42:11.136849  5661 solver.cpp:376]     Train net output #0: loss = 0.0503784 (* 1 = 0.0503784 loss)
I1006 05:42:11.136857  5661 sgd_solver.cpp:172] Iteration 13800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:43:06.140872  5661 solver.cpp:352] Iteration 13900 (1.818 iter/s, 55.0056s/100 iter), 74.8/322.7ep, loss = 0.0555617
I1006 05:43:06.141690  5661 solver.cpp:376]     Train net output #0: loss = 0.0545643 (* 1 = 0.0545643 loss)
I1006 05:43:06.141700  5661 sgd_solver.cpp:172] Iteration 13900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:43:29.646037  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:43:29.704555  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:44:00.713335  5661 solver.cpp:538] Iteration 14000, Testing net (#0)
I1006 05:44:09.498504  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:44:09.510246  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:44:09.600142  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.955403
I1006 05:44:09.600172  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 05:44:09.600183  5661 solver.cpp:624]     Test net output #2: loss = 0.177506 (* 1 = 0.177506 loss)
I1006 05:44:09.600203  5661 solver.cpp:283] Tests completed in 63.4611s
I1006 05:44:10.151866  5661 solver.cpp:352] Iteration 14000 (1.57577 iter/s, 63.4611s/100 iter), 75.3/322.7ep, loss = 0.0477414
I1006 05:44:10.151891  5661 solver.cpp:376]     Train net output #0: loss = 0.0457456 (* 1 = 0.0457456 loss)
I1006 05:44:10.151899  5661 sgd_solver.cpp:172] Iteration 14000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:44:10.152936  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.73 sparsity_achieved=0.717549 iter=14000
W1006 05:44:10.152951  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 05:44:11.978803  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.29
W1006 05:44:11.978899  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 05:44:14.562577  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.661892
W1006 05:44:14.562683  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 05:44:18.208750  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.727919
W1006 05:44:18.208873  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 05:44:24.614104  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.687283
W1006 05:44:24.614233  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 05:44:31.180631  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.729167
W1006 05:44:31.180768  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 05:44:41.331635  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.727132
W1006 05:44:41.331773  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 05:44:47.918181  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.729167
W1006 05:44:47.918290  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 05:45:01.920857  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.729126
W1006 05:45:01.921612  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 05:45:07.910104  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.7283
W1006 05:45:07.910243  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 05:45:11.775004  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.729167
W1006 05:45:11.775130  5661 net.cpp:2612] out5a ni=512 no=64
W1006 05:45:12.737982  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.729601
W1006 05:45:12.738106  5661 net.cpp:2612] out3a ni=128 no=64
W1006 05:45:13.143615  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.729167
W1006 05:45:13.143697  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 05:45:15.054774  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.729167
W1006 05:45:15.054899  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 05:45:16.408784  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.729167
W1006 05:45:16.408910  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 05:45:17.810490  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.729167
W1006 05:45:17.810564  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 05:45:18.981734  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.729167
W1006 05:45:18.981832  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 05:45:19.079994  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.316623
I1006 05:45:19.080061  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 05:45:19.082401  5661 solver.cpp:389] Sparsity after update:
I1006 05:45:19.083216  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 05:45:19.083223  5661 net.cpp:2738] conv1a_param_0(0.29) 
I1006 05:45:19.083227  5661 net.cpp:2738] conv1b_param_0(0.662) 
I1006 05:45:19.083230  5661 net.cpp:2738] ctx_conv1_param_0(0.729) 
I1006 05:45:19.083233  5661 net.cpp:2738] ctx_conv2_param_0(0.729) 
I1006 05:45:19.083236  5661 net.cpp:2738] ctx_conv3_param_0(0.729) 
I1006 05:45:19.083240  5661 net.cpp:2738] ctx_conv4_param_0(0.729) 
I1006 05:45:19.083242  5661 net.cpp:2738] ctx_final_param_0(0.317) 
I1006 05:45:19.083245  5661 net.cpp:2738] out3a_param_0(0.729) 
I1006 05:45:19.083247  5661 net.cpp:2738] out5a_param_0(0.73) 
I1006 05:45:19.083250  5661 net.cpp:2738] res2a_branch2a_param_0(0.728) 
I1006 05:45:19.083252  5661 net.cpp:2738] res2a_branch2b_param_0(0.687) 
I1006 05:45:19.083256  5661 net.cpp:2738] res3a_branch2a_param_0(0.729) 
I1006 05:45:19.083258  5661 net.cpp:2738] res3a_branch2b_param_0(0.727) 
I1006 05:45:19.083261  5661 net.cpp:2738] res4a_branch2a_param_0(0.729) 
I1006 05:45:19.083263  5661 net.cpp:2738] res4a_branch2b_param_0(0.729) 
I1006 05:45:19.083266  5661 net.cpp:2738] res5a_branch2a_param_0(0.728) 
I1006 05:45:19.083269  5661 net.cpp:2738] res5a_branch2b_param_0(0.729) 
I1006 05:45:19.083271  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.95775e+06/2.69117e+06) 0.727
I1006 05:46:12.330379  5661 solver.cpp:352] Iteration 14100 (0.818455 iter/s, 122.181s/100 iter), 75.8/322.7ep, loss = 0.0569744
I1006 05:46:12.331179  5661 solver.cpp:376]     Train net output #0: loss = 0.0390694 (* 1 = 0.0390694 loss)
I1006 05:46:12.331189  5661 sgd_solver.cpp:172] Iteration 14100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:46:27.970736  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:46:28.026405  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:47:07.119149  5661 solver.cpp:352] Iteration 14200 (1.82515 iter/s, 54.7901s/100 iter), 76.4/322.7ep, loss = 0.0473051
I1006 05:47:07.119922  5661 solver.cpp:376]     Train net output #0: loss = 0.055242 (* 1 = 0.055242 loss)
I1006 05:47:07.119932  5661 sgd_solver.cpp:172] Iteration 14200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:48:02.009830  5661 solver.cpp:352] Iteration 14300 (1.82176 iter/s, 54.8921s/100 iter), 76.9/322.7ep, loss = 0.0470022
I1006 05:48:02.010641  5661 solver.cpp:376]     Train net output #0: loss = 0.049241 (* 1 = 0.049241 loss)
I1006 05:48:02.010650  5661 sgd_solver.cpp:172] Iteration 14300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:48:09.982595  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:48:10.027379  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:48:56.823009  5661 solver.cpp:352] Iteration 14400 (1.82435 iter/s, 54.814s/100 iter), 77.4/322.7ep, loss = 0.0540422
I1006 05:48:56.823926  5661 solver.cpp:376]     Train net output #0: loss = 0.0585034 (* 1 = 0.0585034 loss)
I1006 05:48:56.823936  5661 sgd_solver.cpp:172] Iteration 14400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:49:51.688163  5661 solver.cpp:352] Iteration 14500 (1.82265 iter/s, 54.8651s/100 iter), 78/322.7ep, loss = 0.039706
I1006 05:49:51.688957  5661 solver.cpp:376]     Train net output #0: loss = 0.0319782 (* 1 = 0.0319782 loss)
I1006 05:49:51.688969  5661 sgd_solver.cpp:172] Iteration 14500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:49:51.979620  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:49:52.002596  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:50:46.603731  5661 solver.cpp:352] Iteration 14600 (1.82097 iter/s, 54.9157s/100 iter), 78.5/322.7ep, loss = 0.0564932
I1006 05:50:46.604534  5661 solver.cpp:376]     Train net output #0: loss = 0.0556895 (* 1 = 0.0556895 loss)
I1006 05:50:46.604544  5661 sgd_solver.cpp:172] Iteration 14600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:51:33.829869  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:51:33.912528  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:51:41.439288  5661 solver.cpp:352] Iteration 14700 (1.82363 iter/s, 54.8357s/100 iter), 79.1/322.7ep, loss = 0.0698797
I1006 05:51:41.439313  5661 solver.cpp:376]     Train net output #0: loss = 0.055448 (* 1 = 0.055448 loss)
I1006 05:51:41.439321  5661 sgd_solver.cpp:172] Iteration 14700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:52:36.303709  5661 solver.cpp:352] Iteration 14800 (1.82267 iter/s, 54.8647s/100 iter), 79.6/322.7ep, loss = 0.0511503
I1006 05:52:36.304527  5661 solver.cpp:376]     Train net output #0: loss = 0.0474123 (* 1 = 0.0474123 loss)
I1006 05:52:36.304538  5661 sgd_solver.cpp:172] Iteration 14800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:53:15.954445  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:53:16.034512  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:53:31.250033  5661 solver.cpp:352] Iteration 14900 (1.81995 iter/s, 54.9467s/100 iter), 80.1/322.7ep, loss = 0.0513546
I1006 05:53:31.250058  5661 solver.cpp:376]     Train net output #0: loss = 0.0486039 (* 1 = 0.0486039 loss)
I1006 05:53:31.250066  5661 sgd_solver.cpp:172] Iteration 14900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:54:26.107533  5661 solver.cpp:352] Iteration 15000 (1.82289 iter/s, 54.8579s/100 iter), 80.7/322.7ep, loss = 0.0479618
I1006 05:54:26.108330  5661 solver.cpp:376]     Train net output #0: loss = 0.048361 (* 1 = 0.048361 loss)
I1006 05:54:26.108340  5661 sgd_solver.cpp:172] Iteration 15000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:54:26.109279  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.74 sparsity_achieved=0.727473 iter=15000
W1006 05:54:26.109292  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 05:54:27.805573  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.290417
W1006 05:54:27.805719  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 05:54:30.619690  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.667969
W1006 05:54:30.619817  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 05:54:34.840873  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.738118
W1006 05:54:34.841009  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 05:54:40.614780  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.693142
W1006 05:54:40.614931  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 05:54:47.011864  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.739556
W1006 05:54:47.011986  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 05:54:57.195557  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.736925
W1006 05:54:57.195714  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 05:55:05.171283  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.739583
W1006 05:55:05.171363  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 05:55:20.676031  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.73938
W1006 05:55:20.676148  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 05:55:27.402616  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.738354
W1006 05:55:27.402746  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 05:55:32.190244  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.739583
W1006 05:55:32.190348  5661 net.cpp:2612] out5a ni=512 no=64
W1006 05:55:33.213106  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.739583
W1006 05:55:33.213207  5661 net.cpp:2612] out3a ni=128 no=64
W1006 05:55:33.614818  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.739583
W1006 05:55:33.614894  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 05:55:35.730777  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.739583
W1006 05:55:35.730825  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 05:55:37.485121  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.739583
W1006 05:55:37.485205  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 05:55:39.300832  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.739583
W1006 05:55:39.300912  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 05:55:40.660192  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.739583
W1006 05:55:40.660271  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 05:55:40.770891  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.31467
I1006 05:55:40.770915  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 05:55:40.773277  5661 solver.cpp:389] Sparsity after update:
I1006 05:55:40.774181  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 05:55:40.774189  5661 net.cpp:2738] conv1a_param_0(0.29) 
I1006 05:55:40.774194  5661 net.cpp:2738] conv1b_param_0(0.668) 
I1006 05:55:40.774197  5661 net.cpp:2738] ctx_conv1_param_0(0.74) 
I1006 05:55:40.774201  5661 net.cpp:2738] ctx_conv2_param_0(0.74) 
I1006 05:55:40.774204  5661 net.cpp:2738] ctx_conv3_param_0(0.74) 
I1006 05:55:40.774207  5661 net.cpp:2738] ctx_conv4_param_0(0.74) 
I1006 05:55:40.774210  5661 net.cpp:2738] ctx_final_param_0(0.315) 
I1006 05:55:40.774214  5661 net.cpp:2738] out3a_param_0(0.74) 
I1006 05:55:40.774216  5661 net.cpp:2738] out5a_param_0(0.74) 
I1006 05:55:40.774220  5661 net.cpp:2738] res2a_branch2a_param_0(0.738) 
I1006 05:55:40.774224  5661 net.cpp:2738] res2a_branch2b_param_0(0.693) 
I1006 05:55:40.774226  5661 net.cpp:2738] res3a_branch2a_param_0(0.74) 
I1006 05:55:40.774230  5661 net.cpp:2738] res3a_branch2b_param_0(0.737) 
I1006 05:55:40.774232  5661 net.cpp:2738] res4a_branch2a_param_0(0.74) 
I1006 05:55:40.774235  5661 net.cpp:2738] res4a_branch2b_param_0(0.739) 
I1006 05:55:40.774240  5661 net.cpp:2738] res5a_branch2a_param_0(0.738) 
I1006 05:55:40.774242  5661 net.cpp:2738] res5a_branch2b_param_0(0.74) 
I1006 05:55:40.774245  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (1.98511e+06/2.69117e+06) 0.738
I1006 05:56:11.031239  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:56:11.111138  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:56:33.997171  5661 solver.cpp:352] Iteration 15100 (0.781919 iter/s, 127.891s/100 iter), 81.2/322.7ep, loss = 0.0504646
I1006 05:56:33.997195  5661 solver.cpp:376]     Train net output #0: loss = 0.0552348 (* 1 = 0.0552348 loss)
I1006 05:56:33.997200  5661 sgd_solver.cpp:172] Iteration 15100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:57:28.871397  5661 solver.cpp:352] Iteration 15200 (1.82233 iter/s, 54.8747s/100 iter), 81.7/322.7ep, loss = 0.083169
I1006 05:57:28.872205  5661 solver.cpp:376]     Train net output #0: loss = 0.094363 (* 1 = 0.094363 loss)
I1006 05:57:28.872213  5661 sgd_solver.cpp:172] Iteration 15200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:57:53.043916  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:57:53.118471  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:58:23.679252  5661 solver.cpp:352] Iteration 15300 (1.82454 iter/s, 54.8085s/100 iter), 82.3/322.7ep, loss = 0.0425544
I1006 05:58:23.679427  5661 solver.cpp:376]     Train net output #0: loss = 0.0407122 (* 1 = 0.0407122 loss)
I1006 05:58:23.679436  5661 sgd_solver.cpp:172] Iteration 15300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:59:18.564440  5661 solver.cpp:352] Iteration 15400 (1.82196 iter/s, 54.8859s/100 iter), 82.8/322.7ep, loss = 0.0538205
I1006 05:59:18.565254  5661 solver.cpp:376]     Train net output #0: loss = 0.0576231 (* 1 = 0.0576231 loss)
I1006 05:59:18.565263  5661 sgd_solver.cpp:172] Iteration 15400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 05:59:35.108819  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 05:59:35.167501  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:00:13.570592  5661 solver.cpp:352] Iteration 15500 (1.81795 iter/s, 55.0069s/100 iter), 83.4/322.7ep, loss = 0.061942
I1006 06:00:13.571411  5661 solver.cpp:376]     Train net output #0: loss = 0.0621909 (* 1 = 0.0621909 loss)
I1006 06:00:13.571421  5661 sgd_solver.cpp:172] Iteration 15500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:01:08.488539  5661 solver.cpp:352] Iteration 15600 (1.82087 iter/s, 54.9187s/100 iter), 83.9/322.7ep, loss = 0.0561448
I1006 06:01:08.489351  5661 solver.cpp:376]     Train net output #0: loss = 0.0500492 (* 1 = 0.0500492 loss)
I1006 06:01:08.489362  5661 sgd_solver.cpp:172] Iteration 15600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:01:17.299427  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:01:17.351605  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:02:03.448189  5661 solver.cpp:352] Iteration 15700 (1.81949 iter/s, 54.9604s/100 iter), 84.4/322.7ep, loss = 0.0577806
I1006 06:02:03.449012  5661 solver.cpp:376]     Train net output #0: loss = 0.0491751 (* 1 = 0.0491751 loss)
I1006 06:02:03.449021  5661 sgd_solver.cpp:172] Iteration 15700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:02:29.109938  5681 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 06:02:58.504756  5661 solver.cpp:352] Iteration 15800 (1.81629 iter/s, 55.0574s/100 iter), 85/322.7ep, loss = 0.0488454
I1006 06:02:58.505594  5661 solver.cpp:376]     Train net output #0: loss = 0.0623795 (* 1 = 0.0623795 loss)
I1006 06:02:58.505605  5661 sgd_solver.cpp:172] Iteration 15800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:02:59.632834  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:02:59.676614  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:03:53.520571  5661 solver.cpp:352] Iteration 15900 (1.81763 iter/s, 55.0167s/100 iter), 85.5/322.7ep, loss = 0.052121
I1006 06:03:53.521384  5661 solver.cpp:376]     Train net output #0: loss = 0.0613714 (* 1 = 0.0613714 loss)
I1006 06:03:53.521391  5661 sgd_solver.cpp:172] Iteration 15900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:04:41.972272  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:04:41.994037  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:04:48.012935  5661 solver.cpp:538] Iteration 16000, Testing net (#0)
I1006 06:04:56.919306  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:04:56.930701  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:04:57.018604  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.952917
I1006 06:04:57.018631  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 06:04:57.018641  5661 solver.cpp:624]     Test net output #2: loss = 0.183095 (* 1 = 0.183095 loss)
I1006 06:04:57.018659  5661 solver.cpp:283] Tests completed in 63.4991s
I1006 06:04:57.568049  5661 solver.cpp:352] Iteration 16000 (1.57483 iter/s, 63.4991s/100 iter), 86.1/322.7ep, loss = 0.0535093
I1006 06:04:57.568073  5661 solver.cpp:376]     Train net output #0: loss = 0.0350887 (* 1 = 0.0350887 loss)
I1006 06:04:57.568079  5661 sgd_solver.cpp:172] Iteration 16000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:04:57.569031  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.75 sparsity_achieved=0.737638 iter=16000
W1006 06:04:57.569044  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 06:04:59.448698  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.297917
W1006 06:04:59.448827  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 06:05:02.255478  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.670139
W1006 06:05:02.255610  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 06:05:06.458149  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.7449
W1006 06:05:06.458206  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 06:05:12.444963  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.698676
W1006 06:05:12.445109  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 06:05:18.997474  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.748237
W1006 06:05:18.997622  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 06:05:28.431417  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.74349
W1006 06:05:28.431530  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 06:05:36.935107  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.749132
W1006 06:05:36.935159  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 06:05:51.990301  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.748033
W1006 06:05:51.990399  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 06:05:58.973752  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.748371
W1006 06:05:58.973845  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 06:06:03.285200  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.749132
W1006 06:06:03.285280  5661 net.cpp:2612] out5a ni=512 no=64
W1006 06:06:04.420179  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.749566
W1006 06:06:04.420262  5661 net.cpp:2612] out3a ni=128 no=64
W1006 06:06:04.830644  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.748264
W1006 06:06:04.830670  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 06:06:06.973672  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.748264
W1006 06:06:06.973762  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 06:06:08.533174  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.748264
W1006 06:06:08.533267  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 06:06:10.149904  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.748264
W1006 06:06:10.150004  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 06:06:11.505766  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.748264
W1006 06:06:11.505842  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 06:06:11.610529  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.371962
I1006 06:06:11.610555  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 06:06:11.612923  5661 solver.cpp:389] Sparsity after update:
I1006 06:06:11.613831  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 06:06:11.613839  5661 net.cpp:2738] conv1a_param_0(0.298) 
I1006 06:06:11.613844  5661 net.cpp:2738] conv1b_param_0(0.67) 
I1006 06:06:11.613847  5661 net.cpp:2738] ctx_conv1_param_0(0.748) 
I1006 06:06:11.613852  5661 net.cpp:2738] ctx_conv2_param_0(0.748) 
I1006 06:06:11.613854  5661 net.cpp:2738] ctx_conv3_param_0(0.748) 
I1006 06:06:11.613857  5661 net.cpp:2738] ctx_conv4_param_0(0.748) 
I1006 06:06:11.613860  5661 net.cpp:2738] ctx_final_param_0(0.372) 
I1006 06:06:11.613864  5661 net.cpp:2738] out3a_param_0(0.748) 
I1006 06:06:11.613867  5661 net.cpp:2738] out5a_param_0(0.75) 
I1006 06:06:11.613870  5661 net.cpp:2738] res2a_branch2a_param_0(0.745) 
I1006 06:06:11.613873  5661 net.cpp:2738] res2a_branch2b_param_0(0.699) 
I1006 06:06:11.613876  5661 net.cpp:2738] res3a_branch2a_param_0(0.748) 
I1006 06:06:11.613880  5661 net.cpp:2738] res3a_branch2b_param_0(0.743) 
I1006 06:06:11.613883  5661 net.cpp:2738] res4a_branch2a_param_0(0.749) 
I1006 06:06:11.613886  5661 net.cpp:2738] res4a_branch2b_param_0(0.748) 
I1006 06:06:11.613889  5661 net.cpp:2738] res5a_branch2a_param_0(0.748) 
I1006 06:06:11.613894  5661 net.cpp:2738] res5a_branch2b_param_0(0.749) 
I1006 06:06:11.613898  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.01106e+06/2.69117e+06) 0.747
I1006 06:07:04.891427  5661 solver.cpp:352] Iteration 16100 (0.785391 iter/s, 127.325s/100 iter), 86.6/322.7ep, loss = 0.051927
I1006 06:07:04.892256  5661 solver.cpp:376]     Train net output #0: loss = 0.0596586 (* 1 = 0.0596586 loss)
I1006 06:07:04.892268  5661 sgd_solver.cpp:172] Iteration 16100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:07:45.402684  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:07:45.488160  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:07:59.902348  5661 solver.cpp:352] Iteration 16200 (1.81779 iter/s, 55.0118s/100 iter), 87.1/322.7ep, loss = 0.0520715
I1006 06:07:59.902371  5661 solver.cpp:376]     Train net output #0: loss = 0.0417715 (* 1 = 0.0417715 loss)
I1006 06:07:59.902379  5661 sgd_solver.cpp:172] Iteration 16200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:08:54.907851  5661 solver.cpp:352] Iteration 16300 (1.81797 iter/s, 55.0064s/100 iter), 87.7/322.7ep, loss = 0.0515541
I1006 06:08:54.908002  5661 solver.cpp:376]     Train net output #0: loss = 0.0432405 (* 1 = 0.0432405 loss)
I1006 06:08:54.908011  5661 sgd_solver.cpp:172] Iteration 16300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:09:27.720801  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:09:27.796947  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:09:49.887473  5661 solver.cpp:352] Iteration 16400 (1.81883 iter/s, 54.9805s/100 iter), 88.2/322.7ep, loss = 0.0679611
I1006 06:09:49.887502  5661 solver.cpp:376]     Train net output #0: loss = 0.0708486 (* 1 = 0.0708486 loss)
I1006 06:09:49.887512  5661 sgd_solver.cpp:172] Iteration 16400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:10:44.902602  5661 solver.cpp:352] Iteration 16500 (1.81765 iter/s, 55.0161s/100 iter), 88.7/322.7ep, loss = 0.0481034
I1006 06:10:44.903419  5661 solver.cpp:376]     Train net output #0: loss = 0.0503045 (* 1 = 0.0503045 loss)
I1006 06:10:44.903429  5661 sgd_solver.cpp:172] Iteration 16500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:11:09.972157  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:11:10.044219  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:11:39.910698  5661 solver.cpp:352] Iteration 16600 (1.81788 iter/s, 55.0091s/100 iter), 89.3/322.7ep, loss = 0.0501508
I1006 06:11:39.910856  5661 solver.cpp:376]     Train net output #0: loss = 0.06528 (* 1 = 0.06528 loss)
I1006 06:11:39.910866  5661 sgd_solver.cpp:172] Iteration 16600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:12:34.879050  5661 solver.cpp:352] Iteration 16700 (1.8192 iter/s, 54.9693s/100 iter), 89.8/322.7ep, loss = 0.0646595
I1006 06:12:34.879832  5661 solver.cpp:376]     Train net output #0: loss = 0.0663873 (* 1 = 0.0663873 loss)
I1006 06:12:34.879842  5661 sgd_solver.cpp:172] Iteration 16700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:12:52.204423  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:12:52.269938  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:13:29.808221  5661 solver.cpp:352] Iteration 16800 (1.82049 iter/s, 54.9301s/100 iter), 90.4/322.7ep, loss = 0.0511075
I1006 06:13:29.809067  5661 solver.cpp:376]     Train net output #0: loss = 0.055431 (* 1 = 0.055431 loss)
I1006 06:13:29.809077  5661 sgd_solver.cpp:172] Iteration 16800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:14:24.711457  5661 solver.cpp:352] Iteration 16900 (1.82135 iter/s, 54.9042s/100 iter), 90.9/322.7ep, loss = 0.0501002
I1006 06:14:24.712267  5661 solver.cpp:376]     Train net output #0: loss = 0.05272 (* 1 = 0.05272 loss)
I1006 06:14:24.712277  5661 sgd_solver.cpp:172] Iteration 16900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:14:34.347677  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:14:34.406795  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:15:19.691804  5661 solver.cpp:352] Iteration 17000 (1.8188 iter/s, 54.9813s/100 iter), 91.4/322.7ep, loss = 0.0514454
I1006 06:15:19.692683  5661 solver.cpp:376]     Train net output #0: loss = 0.0516535 (* 1 = 0.0516535 loss)
I1006 06:15:19.692692  5661 sgd_solver.cpp:172] Iteration 17000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:15:19.693645  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.76 sparsity_achieved=0.747283 iter=17000
W1006 06:15:19.693657  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 06:15:21.508499  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.29875
W1006 06:15:21.508657  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 06:15:24.394765  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.675781
W1006 06:15:24.394896  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 06:15:28.796597  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.754612
W1006 06:15:28.796749  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 06:15:35.106597  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.705729
W1006 06:15:35.106720  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 06:15:42.837913  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.758626
W1006 06:15:42.838016  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 06:15:53.491056  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.752767
W1006 06:15:53.491176  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 06:16:03.528424  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.759549
W1006 06:16:03.528542  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 06:16:19.495040  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.75824
W1006 06:16:19.495178  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 06:16:27.283324  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.758792
W1006 06:16:27.283421  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 06:16:32.396133  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.759549
W1006 06:16:32.396227  5661 net.cpp:2612] out5a ni=512 no=64
W1006 06:16:33.538934  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.759983
W1006 06:16:33.539052  5661 net.cpp:2612] out3a ni=128 no=64
W1006 06:16:33.973517  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.758681
W1006 06:16:33.973661  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 06:16:36.137759  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.758681
W1006 06:16:36.137887  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 06:16:37.915081  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.758681
W1006 06:16:37.915215  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 06:16:39.819474  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.758681
W1006 06:16:39.819520  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 06:16:41.358813  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.758681
W1006 06:16:41.358939  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 06:16:41.463596  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.282552
I1006 06:16:41.463620  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 06:16:41.465968  5661 solver.cpp:389] Sparsity after update:
I1006 06:16:41.466830  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 06:16:41.466837  5661 net.cpp:2738] conv1a_param_0(0.299) 
I1006 06:16:41.466842  5661 net.cpp:2738] conv1b_param_0(0.676) 
I1006 06:16:41.466845  5661 net.cpp:2738] ctx_conv1_param_0(0.759) 
I1006 06:16:41.466848  5661 net.cpp:2738] ctx_conv2_param_0(0.759) 
I1006 06:16:41.466851  5661 net.cpp:2738] ctx_conv3_param_0(0.759) 
I1006 06:16:41.466855  5661 net.cpp:2738] ctx_conv4_param_0(0.759) 
I1006 06:16:41.466857  5661 net.cpp:2738] ctx_final_param_0(0.283) 
I1006 06:16:41.466861  5661 net.cpp:2738] out3a_param_0(0.759) 
I1006 06:16:41.466863  5661 net.cpp:2738] out5a_param_0(0.76) 
I1006 06:16:41.466866  5661 net.cpp:2738] res2a_branch2a_param_0(0.755) 
I1006 06:16:41.466869  5661 net.cpp:2738] res2a_branch2b_param_0(0.706) 
I1006 06:16:41.466872  5661 net.cpp:2738] res3a_branch2a_param_0(0.759) 
I1006 06:16:41.466876  5661 net.cpp:2738] res3a_branch2b_param_0(0.753) 
I1006 06:16:41.466878  5661 net.cpp:2738] res4a_branch2a_param_0(0.76) 
I1006 06:16:41.466881  5661 net.cpp:2738] res4a_branch2b_param_0(0.758) 
I1006 06:16:41.466884  5661 net.cpp:2738] res5a_branch2a_param_0(0.759) 
I1006 06:16:41.466887  5661 net.cpp:2738] res5a_branch2b_param_0(0.76) 
I1006 06:16:41.466890  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.03849e+06/2.69117e+06) 0.757
I1006 06:17:34.567438  5661 solver.cpp:352] Iteration 17100 (0.741412 iter/s, 134.878s/100 iter), 92/322.7ep, loss = 0.0462984
I1006 06:17:34.568239  5661 solver.cpp:376]     Train net output #0: loss = 0.0387046 (* 1 = 0.0387046 loss)
I1006 06:17:34.568248  5661 sgd_solver.cpp:172] Iteration 17100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:17:36.503566  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:17:36.558725  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:18:30.500135  5661 solver.cpp:352] Iteration 17200 (1.78783 iter/s, 55.9336s/100 iter), 92.5/322.7ep, loss = 0.0515765
I1006 06:18:30.500941  5661 solver.cpp:376]     Train net output #0: loss = 0.0499919 (* 1 = 0.0499919 loss)
I1006 06:18:30.500952  5661 sgd_solver.cpp:172] Iteration 17200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:19:19.653892  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:19:19.703658  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:19:25.391213  5661 solver.cpp:352] Iteration 17300 (1.82176 iter/s, 54.892s/100 iter), 93/322.7ep, loss = 0.0495651
I1006 06:19:25.391237  5661 solver.cpp:376]     Train net output #0: loss = 0.0485324 (* 1 = 0.0485324 loss)
I1006 06:19:25.391245  5661 sgd_solver.cpp:172] Iteration 17300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:20:20.281687  5661 solver.cpp:352] Iteration 17400 (1.82178 iter/s, 54.8915s/100 iter), 93.6/322.7ep, loss = 0.0482276
I1006 06:20:20.282480  5661 solver.cpp:376]     Train net output #0: loss = 0.0428575 (* 1 = 0.0428575 loss)
I1006 06:20:20.282490  5661 sgd_solver.cpp:172] Iteration 17400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:21:01.690747  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:21:01.712095  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:21:15.149528  5661 solver.cpp:352] Iteration 17500 (1.82253 iter/s, 54.8688s/100 iter), 94.1/322.7ep, loss = 0.0535275
I1006 06:21:15.149551  5661 solver.cpp:376]     Train net output #0: loss = 0.0613664 (* 1 = 0.0613664 loss)
I1006 06:21:15.149559  5661 sgd_solver.cpp:172] Iteration 17500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:22:09.971216  5661 solver.cpp:352] Iteration 17600 (1.82406 iter/s, 54.8227s/100 iter), 94.7/322.7ep, loss = 0.053267
I1006 06:22:09.971987  5661 solver.cpp:376]     Train net output #0: loss = 0.0517132 (* 1 = 0.0517132 loss)
I1006 06:22:09.971998  5661 sgd_solver.cpp:172] Iteration 17600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:22:43.488584  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:22:43.570433  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:23:04.810612  5661 solver.cpp:352] Iteration 17700 (1.82347 iter/s, 54.8404s/100 iter), 95.2/322.7ep, loss = 0.0476528
I1006 06:23:04.810638  5661 solver.cpp:376]     Train net output #0: loss = 0.0451121 (* 1 = 0.0451121 loss)
I1006 06:23:04.810647  5661 sgd_solver.cpp:172] Iteration 17700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:23:59.764035  5661 solver.cpp:352] Iteration 17800 (1.81969 iter/s, 54.9543s/100 iter), 95.7/322.7ep, loss = 0.0449546
I1006 06:23:59.764855  5661 solver.cpp:376]     Train net output #0: loss = 0.0349187 (* 1 = 0.0349187 loss)
I1006 06:23:59.764865  5661 sgd_solver.cpp:172] Iteration 17800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:24:25.600181  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:24:25.679071  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:24:54.644176  5661 solver.cpp:352] Iteration 17900 (1.82212 iter/s, 54.881s/100 iter), 96.3/322.7ep, loss = 0.0492395
I1006 06:24:54.644330  5661 solver.cpp:376]     Train net output #0: loss = 0.0586872 (* 1 = 0.0586872 loss)
I1006 06:24:54.644340  5661 sgd_solver.cpp:172] Iteration 17900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:25:47.587724  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 06:25:48.958873  5661 solver.cpp:538] Iteration 18000, Testing net (#0)
I1006 06:25:57.769702  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:25:57.781268  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:25:57.870275  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.955008
I1006 06:25:57.870304  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 06:25:57.870314  5661 solver.cpp:624]     Test net output #2: loss = 0.177342 (* 1 = 0.177342 loss)
I1006 06:25:57.870333  5661 solver.cpp:283] Tests completed in 63.2272s
I1006 06:25:58.421730  5661 solver.cpp:352] Iteration 18000 (1.5816 iter/s, 63.2272s/100 iter), 96.8/322.7ep, loss = 0.0616331
I1006 06:25:58.421756  5661 solver.cpp:376]     Train net output #0: loss = 0.0740123 (* 1 = 0.0740123 loss)
I1006 06:25:58.421763  5661 sgd_solver.cpp:172] Iteration 18000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:25:58.422814  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.77 sparsity_achieved=0.757474 iter=18000
W1006 06:25:58.422828  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 06:26:00.352535  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.300417
W1006 06:26:00.352670  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 06:26:03.697057  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.679253
W1006 06:26:03.697170  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 06:26:07.990439  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.763835
W1006 06:26:07.990526  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 06:26:13.679821  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.711046
W1006 06:26:13.679945  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 06:26:20.987457  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.768962
W1006 06:26:20.987586  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 06:26:31.374469  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.761529
W1006 06:26:31.374589  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 06:26:41.894258  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.769965
W1006 06:26:41.894358  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 06:26:59.257072  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.768304
W1006 06:26:59.257205  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 06:27:07.909006  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.768783
W1006 06:27:07.909108  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 06:27:13.471338  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.769965
W1006 06:27:13.471426  5661 net.cpp:2612] out5a ni=512 no=64
W1006 06:27:14.700719  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.769965
W1006 06:27:14.700800  5661 net.cpp:2612] out3a ni=128 no=64
W1006 06:27:15.188248  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.769097
W1006 06:27:15.188273  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 06:27:17.624791  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.769097
W1006 06:27:17.624871  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 06:27:19.566915  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.769097
W1006 06:27:19.567016  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 06:27:21.483198  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.769097
W1006 06:27:21.483279  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 06:27:23.100682  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.769097
W1006 06:27:23.100760  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 06:27:23.210711  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.374349
I1006 06:27:23.210733  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 06:27:23.213074  5661 solver.cpp:389] Sparsity after update:
I1006 06:27:23.213937  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 06:27:23.213944  5661 net.cpp:2738] conv1a_param_0(0.3) 
I1006 06:27:23.213949  5661 net.cpp:2738] conv1b_param_0(0.679) 
I1006 06:27:23.213953  5661 net.cpp:2738] ctx_conv1_param_0(0.769) 
I1006 06:27:23.213955  5661 net.cpp:2738] ctx_conv2_param_0(0.769) 
I1006 06:27:23.213958  5661 net.cpp:2738] ctx_conv3_param_0(0.769) 
I1006 06:27:23.213961  5661 net.cpp:2738] ctx_conv4_param_0(0.769) 
I1006 06:27:23.213964  5661 net.cpp:2738] ctx_final_param_0(0.374) 
I1006 06:27:23.213968  5661 net.cpp:2738] out3a_param_0(0.769) 
I1006 06:27:23.213970  5661 net.cpp:2738] out5a_param_0(0.77) 
I1006 06:27:23.213973  5661 net.cpp:2738] res2a_branch2a_param_0(0.764) 
I1006 06:27:23.213976  5661 net.cpp:2738] res2a_branch2b_param_0(0.711) 
I1006 06:27:23.213979  5661 net.cpp:2738] res3a_branch2a_param_0(0.769) 
I1006 06:27:23.213982  5661 net.cpp:2738] res3a_branch2b_param_0(0.762) 
I1006 06:27:23.213985  5661 net.cpp:2738] res4a_branch2a_param_0(0.77) 
I1006 06:27:23.213989  5661 net.cpp:2738] res4a_branch2b_param_0(0.768) 
I1006 06:27:23.213991  5661 net.cpp:2738] res5a_branch2a_param_0(0.769) 
I1006 06:27:23.213994  5661 net.cpp:2738] res5a_branch2b_param_0(0.77) 
I1006 06:27:23.213996  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.06611e+06/2.69117e+06) 0.768
I1006 06:27:39.735427  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:27:39.806183  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:28:16.299684  5661 solver.cpp:352] Iteration 18100 (0.725269 iter/s, 137.88s/100 iter), 97.3/322.7ep, loss = 0.0435551
I1006 06:28:16.299835  5661 solver.cpp:376]     Train net output #0: loss = 0.0337608 (* 1 = 0.0337608 loss)
I1006 06:28:16.299845  5661 sgd_solver.cpp:172] Iteration 18100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:29:11.265938  5661 solver.cpp:352] Iteration 18200 (1.81927 iter/s, 54.9671s/100 iter), 97.9/322.7ep, loss = 0.114905
I1006 06:29:11.266724  5661 solver.cpp:376]     Train net output #0: loss = 0.127033 (* 1 = 0.127033 loss)
I1006 06:29:11.266737  5661 sgd_solver.cpp:172] Iteration 18200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:29:21.763147  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:29:21.832358  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:30:06.124979  5661 solver.cpp:352] Iteration 18300 (1.82282 iter/s, 54.86s/100 iter), 98.4/322.7ep, loss = 0.0513864
I1006 06:30:06.125790  5661 solver.cpp:376]     Train net output #0: loss = 0.0617644 (* 1 = 0.0617644 loss)
I1006 06:30:06.125802  5661 sgd_solver.cpp:172] Iteration 18300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:31:01.165840  5661 solver.cpp:352] Iteration 18400 (1.8168 iter/s, 55.0418s/100 iter), 99/322.7ep, loss = 0.0541611
I1006 06:31:01.166664  5661 solver.cpp:376]     Train net output #0: loss = 0.0511033 (* 1 = 0.0511033 loss)
I1006 06:31:01.166674  5661 sgd_solver.cpp:172] Iteration 18400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:31:03.947563  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:31:04.006564  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:31:56.108207  5661 solver.cpp:352] Iteration 18500 (1.82006 iter/s, 54.9433s/100 iter), 99.5/322.7ep, loss = 0.0675533
I1006 06:31:56.109014  5661 solver.cpp:376]     Train net output #0: loss = 0.0894382 (* 1 = 0.0894382 loss)
I1006 06:31:56.109022  5661 sgd_solver.cpp:172] Iteration 18500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:32:46.052716  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:32:46.108073  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:32:50.964325  5661 solver.cpp:352] Iteration 18600 (1.82292 iter/s, 54.8571s/100 iter), 100/322.7ep, loss = 0.099605
I1006 06:32:50.964347  5661 solver.cpp:376]     Train net output #0: loss = 0.142126 (* 1 = 0.142126 loss)
I1006 06:32:50.964354  5661 sgd_solver.cpp:172] Iteration 18600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:33:46.045048  5661 solver.cpp:352] Iteration 18700 (1.81549 iter/s, 55.0817s/100 iter), 100.6/322.7ep, loss = 0.0493501
I1006 06:33:46.045853  5661 solver.cpp:376]     Train net output #0: loss = 0.0542806 (* 1 = 0.0542806 loss)
I1006 06:33:46.045866  5661 sgd_solver.cpp:172] Iteration 18700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:34:28.570082  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:34:28.613821  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:34:41.173931  5661 solver.cpp:352] Iteration 18800 (1.8139 iter/s, 55.1299s/100 iter), 101.1/322.7ep, loss = 0.0511489
I1006 06:34:41.173961  5661 solver.cpp:376]     Train net output #0: loss = 0.0426683 (* 1 = 0.0426683 loss)
I1006 06:34:41.173970  5661 sgd_solver.cpp:172] Iteration 18800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:35:36.187096  5661 solver.cpp:352] Iteration 18900 (1.81771 iter/s, 55.0141s/100 iter), 101.6/322.7ep, loss = 0.070129
I1006 06:35:36.187902  5661 solver.cpp:376]     Train net output #0: loss = 0.0648308 (* 1 = 0.0648308 loss)
I1006 06:35:36.187912  5661 sgd_solver.cpp:172] Iteration 18900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:36:10.832638  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:36:10.854948  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:36:31.213851  5661 solver.cpp:352] Iteration 19000 (1.81727 iter/s, 55.0277s/100 iter), 102.2/322.7ep, loss = 0.0531226
I1006 06:36:31.213876  5661 solver.cpp:376]     Train net output #0: loss = 0.0482084 (* 1 = 0.0482084 loss)
I1006 06:36:31.213882  5661 sgd_solver.cpp:172] Iteration 19000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:36:31.214881  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.78 sparsity_achieved=0.767736 iter=19000
W1006 06:36:31.214895  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 06:36:33.085175  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.30875
W1006 06:36:33.085322  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 06:36:35.785954  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.68316
W1006 06:36:35.785992  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 06:36:39.981771  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.773492
W1006 06:36:39.981899  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 06:36:45.480119  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.717773
W1006 06:36:45.480253  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 06:36:52.858778  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.779229
W1006 06:36:52.858911  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 06:37:03.521183  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.770155
W1006 06:37:03.521311  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 06:37:13.847434  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.779514
W1006 06:37:13.847537  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 06:37:32.399677  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.778341
W1006 06:37:32.400482  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 06:37:41.410317  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.77876
W1006 06:37:41.410429  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 06:37:47.113164  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.779514
W1006 06:37:47.113286  5661 net.cpp:2612] out5a ni=512 no=64
W1006 06:37:48.314615  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.779948
W1006 06:37:48.314719  5661 net.cpp:2612] out3a ni=128 no=64
W1006 06:37:48.889652  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.779514
W1006 06:37:48.889729  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 06:37:51.555546  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.779514
W1006 06:37:51.555634  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 06:37:53.486521  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.779514
W1006 06:37:53.486601  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 06:37:55.504657  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.779514
W1006 06:37:55.504743  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 06:37:57.319945  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.779514
W1006 06:37:57.320046  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 06:37:57.437314  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.338108
I1006 06:37:57.437342  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 06:37:57.439731  5661 solver.cpp:389] Sparsity after update:
I1006 06:37:57.440687  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 06:37:57.440697  5661 net.cpp:2738] conv1a_param_0(0.309) 
I1006 06:37:57.440703  5661 net.cpp:2738] conv1b_param_0(0.683) 
I1006 06:37:57.440706  5661 net.cpp:2738] ctx_conv1_param_0(0.78) 
I1006 06:37:57.440711  5661 net.cpp:2738] ctx_conv2_param_0(0.78) 
I1006 06:37:57.440713  5661 net.cpp:2738] ctx_conv3_param_0(0.78) 
I1006 06:37:57.440716  5661 net.cpp:2738] ctx_conv4_param_0(0.78) 
I1006 06:37:57.440719  5661 net.cpp:2738] ctx_final_param_0(0.338) 
I1006 06:37:57.440723  5661 net.cpp:2738] out3a_param_0(0.78) 
I1006 06:37:57.440726  5661 net.cpp:2738] out5a_param_0(0.78) 
I1006 06:37:57.440729  5661 net.cpp:2738] res2a_branch2a_param_0(0.773) 
I1006 06:37:57.440732  5661 net.cpp:2738] res2a_branch2b_param_0(0.718) 
I1006 06:37:57.440737  5661 net.cpp:2738] res3a_branch2a_param_0(0.779) 
I1006 06:37:57.440739  5661 net.cpp:2738] res3a_branch2b_param_0(0.77) 
I1006 06:37:57.440742  5661 net.cpp:2738] res4a_branch2a_param_0(0.78) 
I1006 06:37:57.440748  5661 net.cpp:2738] res4a_branch2b_param_0(0.778) 
I1006 06:37:57.440752  5661 net.cpp:2738] res5a_branch2a_param_0(0.779) 
I1006 06:37:57.440757  5661 net.cpp:2738] res5a_branch2b_param_0(0.78) 
I1006 06:37:57.440759  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.09237e+06/2.69117e+06) 0.777
I1006 06:38:50.468219  5661 solver.cpp:352] Iteration 19100 (0.718099 iter/s, 139.256s/100 iter), 102.7/322.7ep, loss = 0.0574006
I1006 06:38:50.469023  5661 solver.cpp:376]     Train net output #0: loss = 0.0546085 (* 1 = 0.0546085 loss)
I1006 06:38:50.469034  5661 sgd_solver.cpp:172] Iteration 19100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:39:17.168342  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:39:17.256911  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:39:45.444772  5661 solver.cpp:352] Iteration 19200 (1.81893 iter/s, 54.9775s/100 iter), 103.3/322.7ep, loss = 0.0645771
I1006 06:39:45.444944  5661 solver.cpp:376]     Train net output #0: loss = 0.0643458 (* 1 = 0.0643458 loss)
I1006 06:39:45.444952  5661 sgd_solver.cpp:172] Iteration 19200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:40:40.289530  5661 solver.cpp:352] Iteration 19300 (1.8233 iter/s, 54.8457s/100 iter), 103.8/322.7ep, loss = 0.0579317
I1006 06:40:40.289671  5661 solver.cpp:376]     Train net output #0: loss = 0.0618806 (* 1 = 0.0618806 loss)
I1006 06:40:40.289680  5661 sgd_solver.cpp:172] Iteration 19300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:40:59.273924  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:40:59.354292  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:41:35.167227  5661 solver.cpp:352] Iteration 19400 (1.8222 iter/s, 54.8787s/100 iter), 104.3/322.7ep, loss = 0.041796
I1006 06:41:35.167985  5661 solver.cpp:376]     Train net output #0: loss = 0.0391972 (* 1 = 0.0391972 loss)
I1006 06:41:35.167994  5661 sgd_solver.cpp:172] Iteration 19400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:42:30.030127  5661 solver.cpp:352] Iteration 19500 (1.82269 iter/s, 54.8639s/100 iter), 104.9/322.7ep, loss = 0.0451842
I1006 06:42:30.030930  5661 solver.cpp:376]     Train net output #0: loss = 0.0343684 (* 1 = 0.0343684 loss)
I1006 06:42:30.030939  5661 sgd_solver.cpp:172] Iteration 19500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:42:41.373802  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:42:41.447679  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:43:25.013411  5661 solver.cpp:352] Iteration 19600 (1.8187 iter/s, 54.9843s/100 iter), 105.4/322.7ep, loss = 0.052667
I1006 06:43:25.014231  5661 solver.cpp:376]     Train net output #0: loss = 0.0578664 (* 1 = 0.0578664 loss)
I1006 06:43:25.014243  5661 sgd_solver.cpp:172] Iteration 19600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:44:20.025846  5661 solver.cpp:352] Iteration 19700 (1.81774 iter/s, 55.0134s/100 iter), 105.9/322.7ep, loss = 0.0521929
I1006 06:44:20.026629  5661 solver.cpp:376]     Train net output #0: loss = 0.0636114 (* 1 = 0.0636114 loss)
I1006 06:44:20.026640  5661 sgd_solver.cpp:172] Iteration 19700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:44:23.638933  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:44:23.705229  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:45:15.038944  5661 solver.cpp:352] Iteration 19800 (1.81772 iter/s, 55.0141s/100 iter), 106.5/322.7ep, loss = 0.0517965
I1006 06:45:15.039731  5661 solver.cpp:376]     Train net output #0: loss = 0.0468206 (* 1 = 0.0468206 loss)
I1006 06:45:15.039741  5661 sgd_solver.cpp:172] Iteration 19800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:46:06.015177  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:46:06.079558  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:46:10.130754  5661 solver.cpp:352] Iteration 19900 (1.81512 iter/s, 55.0928s/100 iter), 107/322.7ep, loss = 0.0446199
I1006 06:46:10.130779  5661 solver.cpp:376]     Train net output #0: loss = 0.0462813 (* 1 = 0.0462813 loss)
I1006 06:46:10.130787  5661 sgd_solver.cpp:172] Iteration 19900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:47:04.553473  5661 solver.cpp:905] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_20000.caffemodel
I1006 06:47:04.564905  5661 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_20000.solverstate
I1006 06:47:04.570632  5661 solver.cpp:538] Iteration 20000, Testing net (#0)
I1006 06:47:13.337211  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:47:13.347431  5687 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 06:47:13.349053  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:47:13.437618  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.952274
I1006 06:47:13.437647  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 06:47:13.437659  5661 solver.cpp:624]     Test net output #2: loss = 0.168633 (* 1 = 0.168633 loss)
I1006 06:47:13.437678  5661 solver.cpp:283] Tests completed in 63.3081s
I1006 06:47:13.988761  5661 solver.cpp:352] Iteration 20000 (1.57958 iter/s, 63.3081s/100 iter), 107.6/322.7ep, loss = 0.0577758
I1006 06:47:13.988785  5661 solver.cpp:376]     Train net output #0: loss = 0.045935 (* 1 = 0.045935 loss)
I1006 06:47:13.988792  5661 sgd_solver.cpp:172] Iteration 20000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:47:13.989820  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.79 sparsity_achieved=0.777496 iter=20000
W1006 06:47:13.989835  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 06:47:15.895611  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.30875
W1006 06:47:15.895740  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 06:47:18.989339  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.686632
W1006 06:47:18.989472  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 06:47:23.824313  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.782986
W1006 06:47:23.824450  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 06:47:30.396272  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.721354
W1006 06:47:30.396402  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 06:47:38.455852  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.789483
W1006 06:47:38.456008  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 06:47:49.271670  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.778293
W1006 06:47:49.271800  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 06:48:02.111817  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.789931
W1006 06:48:02.111944  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 06:48:21.292910  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.787801
W1006 06:48:21.293046  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 06:48:31.798066  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.788734
W1006 06:48:31.798156  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 06:48:38.446513  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.789931
W1006 06:48:38.446626  5661 net.cpp:2612] out5a ni=512 no=64
W1006 06:48:39.831233  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.789931
W1006 06:48:39.831315  5661 net.cpp:2612] out3a ni=128 no=64
W1006 06:48:40.449345  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.789931
W1006 06:48:40.449424  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 06:48:43.332224  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.789931
W1006 06:48:43.332269  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 06:48:45.545011  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.789931
W1006 06:48:45.545105  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 06:48:47.749737  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.789931
W1006 06:48:47.749815  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 06:48:49.760627  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.789931
W1006 06:48:49.760707  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 06:48:49.878098  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.342448
I1006 06:48:49.878123  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 06:48:49.880486  5661 solver.cpp:389] Sparsity after update:
I1006 06:48:49.881422  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 06:48:49.881431  5661 net.cpp:2738] conv1a_param_0(0.309) 
I1006 06:48:49.881436  5661 net.cpp:2738] conv1b_param_0(0.687) 
I1006 06:48:49.881440  5661 net.cpp:2738] ctx_conv1_param_0(0.79) 
I1006 06:48:49.881443  5661 net.cpp:2738] ctx_conv2_param_0(0.79) 
I1006 06:48:49.881448  5661 net.cpp:2738] ctx_conv3_param_0(0.79) 
I1006 06:48:49.881450  5661 net.cpp:2738] ctx_conv4_param_0(0.79) 
I1006 06:48:49.881453  5661 net.cpp:2738] ctx_final_param_0(0.342) 
I1006 06:48:49.881456  5661 net.cpp:2738] out3a_param_0(0.79) 
I1006 06:48:49.881460  5661 net.cpp:2738] out5a_param_0(0.79) 
I1006 06:48:49.881464  5661 net.cpp:2738] res2a_branch2a_param_0(0.783) 
I1006 06:48:49.881466  5661 net.cpp:2738] res2a_branch2b_param_0(0.721) 
I1006 06:48:49.881469  5661 net.cpp:2738] res3a_branch2a_param_0(0.789) 
I1006 06:48:49.881474  5661 net.cpp:2738] res3a_branch2b_param_0(0.778) 
I1006 06:48:49.881476  5661 net.cpp:2738] res4a_branch2a_param_0(0.79) 
I1006 06:48:49.881479  5661 net.cpp:2738] res4a_branch2b_param_0(0.788) 
I1006 06:48:49.881482  5661 net.cpp:2738] res5a_branch2a_param_0(0.789) 
I1006 06:48:49.881489  5661 net.cpp:2738] res5a_branch2b_param_0(0.79) 
I1006 06:48:49.881491  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.11943e+06/2.69117e+06) 0.788
I1006 06:49:31.058202  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:49:31.109958  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:49:42.827571  5661 solver.cpp:352] Iteration 20100 (0.671857 iter/s, 148.841s/100 iter), 108.1/322.7ep, loss = 0.0472418
I1006 06:49:42.827595  5661 solver.cpp:376]     Train net output #0: loss = 0.047052 (* 1 = 0.047052 loss)
I1006 06:49:42.827602  5661 sgd_solver.cpp:172] Iteration 20100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:50:37.759482  5661 solver.cpp:352] Iteration 20200 (1.82041 iter/s, 54.9328s/100 iter), 108.6/322.7ep, loss = 0.0567089
I1006 06:50:37.759630  5661 solver.cpp:376]     Train net output #0: loss = 0.0598258 (* 1 = 0.0598258 loss)
I1006 06:50:37.759640  5661 sgd_solver.cpp:172] Iteration 20200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:51:13.341310  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:51:13.388520  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:51:32.786275  5661 solver.cpp:352] Iteration 20300 (1.81726 iter/s, 55.0278s/100 iter), 109.2/322.7ep, loss = 0.0654144
I1006 06:51:32.786299  5661 solver.cpp:376]     Train net output #0: loss = 0.0415246 (* 1 = 0.0415246 loss)
I1006 06:51:32.786305  5661 sgd_solver.cpp:172] Iteration 20300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:52:27.781980  5661 solver.cpp:352] Iteration 20400 (1.81829 iter/s, 54.9967s/100 iter), 109.7/322.7ep, loss = 0.0399041
I1006 06:52:27.782784  5661 solver.cpp:376]     Train net output #0: loss = 0.0366461 (* 1 = 0.0366461 loss)
I1006 06:52:27.782793  5661 sgd_solver.cpp:172] Iteration 20400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:52:55.642729  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:52:55.663720  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:53:22.810834  5661 solver.cpp:352] Iteration 20500 (1.8172 iter/s, 55.0299s/100 iter), 110.3/322.7ep, loss = 0.0642978
I1006 06:53:22.811808  5661 solver.cpp:376]     Train net output #0: loss = 0.0635761 (* 1 = 0.0635761 loss)
I1006 06:53:22.811820  5661 sgd_solver.cpp:172] Iteration 20500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:54:17.807862  5661 solver.cpp:352] Iteration 20600 (1.81825 iter/s, 54.998s/100 iter), 110.8/322.7ep, loss = 0.055454
I1006 06:54:17.808678  5661 solver.cpp:376]     Train net output #0: loss = 0.0678234 (* 1 = 0.0678234 loss)
I1006 06:54:17.808687  5661 sgd_solver.cpp:172] Iteration 20600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:54:37.593786  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:54:37.679843  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:55:12.667024  5661 solver.cpp:352] Iteration 20700 (1.82282 iter/s, 54.8602s/100 iter), 111.3/322.7ep, loss = 0.0520038
I1006 06:55:12.667831  5661 solver.cpp:376]     Train net output #0: loss = 0.0509156 (* 1 = 0.0509156 loss)
I1006 06:55:12.667840  5661 sgd_solver.cpp:172] Iteration 20700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:56:07.491541  5661 solver.cpp:352] Iteration 20800 (1.82397 iter/s, 54.8255s/100 iter), 111.9/322.7ep, loss = 0.0458095
I1006 06:56:07.492374  5661 solver.cpp:376]     Train net output #0: loss = 0.0479492 (* 1 = 0.0479492 loss)
I1006 06:56:07.492384  5661 sgd_solver.cpp:172] Iteration 20800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:56:19.607128  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:56:19.686671  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:57:02.302528  5661 solver.cpp:352] Iteration 20900 (1.82442 iter/s, 54.812s/100 iter), 112.4/322.7ep, loss = 0.0442238
I1006 06:57:02.303347  5661 solver.cpp:376]     Train net output #0: loss = 0.0374671 (* 1 = 0.0374671 loss)
I1006 06:57:02.303357  5661 sgd_solver.cpp:172] Iteration 20900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:57:57.177250  5661 solver.cpp:352] Iteration 21000 (1.82229 iter/s, 54.8759s/100 iter), 112.9/322.7ep, loss = 0.0465904
I1006 06:57:57.178081  5661 solver.cpp:376]     Train net output #0: loss = 0.0384942 (* 1 = 0.0384942 loss)
I1006 06:57:57.178092  5661 sgd_solver.cpp:172] Iteration 21000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 06:57:57.179277  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.8 sparsity_achieved=0.787551 iter=21000
W1006 06:57:57.179293  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 06:57:59.181799  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.30875
W1006 06:57:59.181933  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 06:58:02.068877  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.691406
W1006 06:58:02.068992  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 06:58:07.247478  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.791775
W1006 06:58:07.247581  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 06:58:13.699096  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.72678
W1006 06:58:13.699201  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 06:58:22.673439  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.797824
W1006 06:58:22.673542  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 06:58:34.447880  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.786106
W1006 06:58:34.447976  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 06:58:48.615561  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.799472
W1006 06:58:48.615641  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 06:59:08.036794  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.795926
W1006 06:59:08.036957  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 06:59:19.909346  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.7987
W1006 06:59:19.909484  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 06:59:27.182648  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.799479
W1006 06:59:27.182776  5661 net.cpp:2612] out5a ni=512 no=64
W1006 06:59:28.772826  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.799913
W1006 06:59:28.772944  5661 net.cpp:2612] out3a ni=128 no=64
W1006 06:59:29.463434  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.798611
W1006 06:59:29.463460  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 06:59:32.298105  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.798611
W1006 06:59:32.298223  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 06:59:34.482524  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.798611
W1006 06:59:34.482576  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 06:59:36.952325  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.798611
W1006 06:59:36.952373  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 06:59:39.089874  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.798611
W1006 06:59:39.089972  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 06:59:39.197675  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.395833
I1006 06:59:39.197698  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 06:59:39.200044  5661 solver.cpp:389] Sparsity after update:
I1006 06:59:39.200913  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 06:59:39.200922  5661 net.cpp:2738] conv1a_param_0(0.309) 
I1006 06:59:39.200927  5661 net.cpp:2738] conv1b_param_0(0.691) 
I1006 06:59:39.200929  5661 net.cpp:2738] ctx_conv1_param_0(0.799) 
I1006 06:59:39.200933  5661 net.cpp:2738] ctx_conv2_param_0(0.799) 
I1006 06:59:39.200937  5661 net.cpp:2738] ctx_conv3_param_0(0.799) 
I1006 06:59:39.200939  5661 net.cpp:2738] ctx_conv4_param_0(0.799) 
I1006 06:59:39.200942  5661 net.cpp:2738] ctx_final_param_0(0.396) 
I1006 06:59:39.200945  5661 net.cpp:2738] out3a_param_0(0.799) 
I1006 06:59:39.200948  5661 net.cpp:2738] out5a_param_0(0.8) 
I1006 06:59:39.200951  5661 net.cpp:2738] res2a_branch2a_param_0(0.792) 
I1006 06:59:39.200954  5661 net.cpp:2738] res2a_branch2b_param_0(0.727) 
I1006 06:59:39.200958  5661 net.cpp:2738] res3a_branch2a_param_0(0.798) 
I1006 06:59:39.200960  5661 net.cpp:2738] res3a_branch2b_param_0(0.786) 
I1006 06:59:39.200963  5661 net.cpp:2738] res4a_branch2a_param_0(0.799) 
I1006 06:59:39.200966  5661 net.cpp:2738] res4a_branch2b_param_0(0.796) 
I1006 06:59:39.200969  5661 net.cpp:2738] res5a_branch2a_param_0(0.799) 
I1006 06:59:39.200973  5661 net.cpp:2738] res5a_branch2b_param_0(0.799) 
I1006 06:59:39.200975  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.14528e+06/2.69117e+06) 0.797
I1006 06:59:43.244314  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 06:59:43.318655  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:00:32.079648  5661 solver.cpp:352] Iteration 21100 (0.645557 iter/s, 154.905s/100 iter), 113.5/322.7ep, loss = 0.0598022
I1006 07:00:32.080438  5661 solver.cpp:376]     Train net output #0: loss = 0.0510043 (* 1 = 0.0510043 loss)
I1006 07:00:32.080447  5661 sgd_solver.cpp:172] Iteration 21100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:01:23.832005  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:01:23.898629  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:01:27.089108  5661 solver.cpp:352] Iteration 21200 (1.81784 iter/s, 55.0105s/100 iter), 114/322.7ep, loss = 0.0581526
I1006 07:01:27.089130  5661 solver.cpp:376]     Train net output #0: loss = 0.0512035 (* 1 = 0.0512035 loss)
I1006 07:01:27.089138  5661 sgd_solver.cpp:172] Iteration 21200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:02:22.011981  5661 solver.cpp:352] Iteration 21300 (1.8207 iter/s, 54.9239s/100 iter), 114.6/322.7ep, loss = 0.0477115
I1006 07:02:22.012800  5661 solver.cpp:376]     Train net output #0: loss = 0.0493025 (* 1 = 0.0493025 loss)
I1006 07:02:22.012809  5661 sgd_solver.cpp:172] Iteration 21300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:03:06.120563  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:03:06.183295  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:03:17.084542  5661 solver.cpp:352] Iteration 21400 (1.81575 iter/s, 55.0736s/100 iter), 115.1/322.7ep, loss = 0.0690257
I1006 07:03:17.084573  5661 solver.cpp:376]     Train net output #0: loss = 0.075029 (* 1 = 0.075029 loss)
I1006 07:03:17.084581  5661 sgd_solver.cpp:172] Iteration 21400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:04:12.045812  5661 solver.cpp:352] Iteration 21500 (1.81943 iter/s, 54.9623s/100 iter), 115.6/322.7ep, loss = 0.0536961
I1006 07:04:12.046665  5661 solver.cpp:376]     Train net output #0: loss = 0.0620164 (* 1 = 0.0620164 loss)
I1006 07:04:12.046679  5661 sgd_solver.cpp:172] Iteration 21500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:04:48.453584  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:04:48.507372  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:05:07.111336  5661 solver.cpp:352] Iteration 21600 (1.81598 iter/s, 55.0666s/100 iter), 116.2/322.7ep, loss = 0.051016
I1006 07:05:07.111361  5661 solver.cpp:376]     Train net output #0: loss = 0.0343383 (* 1 = 0.0343383 loss)
I1006 07:05:07.111367  5661 sgd_solver.cpp:172] Iteration 21600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:06:02.102588  5661 solver.cpp:352] Iteration 21700 (1.81844 iter/s, 54.9923s/100 iter), 116.7/322.7ep, loss = 0.0592148
I1006 07:06:02.103391  5661 solver.cpp:376]     Train net output #0: loss = 0.0675913 (* 1 = 0.0675913 loss)
I1006 07:06:02.103400  5661 sgd_solver.cpp:172] Iteration 21700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:06:30.739517  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:06:30.781394  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:06:57.094668  5661 solver.cpp:352] Iteration 21800 (1.81841 iter/s, 54.9932s/100 iter), 117.2/322.7ep, loss = 0.0756384
I1006 07:06:57.095474  5661 solver.cpp:376]     Train net output #0: loss = 0.0534741 (* 1 = 0.0534741 loss)
I1006 07:06:57.095484  5661 sgd_solver.cpp:172] Iteration 21800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:07:52.076182  5661 solver.cpp:352] Iteration 21900 (1.81876 iter/s, 54.9826s/100 iter), 117.8/322.7ep, loss = 0.0452572
I1006 07:07:52.076997  5661 solver.cpp:376]     Train net output #0: loss = 0.0382027 (* 1 = 0.0382027 loss)
I1006 07:07:52.077008  5661 sgd_solver.cpp:172] Iteration 21900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:08:13.033684  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:08:13.054666  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:08:46.544625  5661 solver.cpp:538] Iteration 22000, Testing net (#0)
I1006 07:08:55.492610  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:08:55.503875  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:08:55.591759  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.949746
I1006 07:08:55.591785  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.99998
I1006 07:08:55.591794  5661 solver.cpp:624]     Test net output #2: loss = 0.177404 (* 1 = 0.177404 loss)
I1006 07:08:55.591811  5661 solver.cpp:283] Tests completed in 63.5169s
I1006 07:08:56.139506  5661 solver.cpp:352] Iteration 22000 (1.57438 iter/s, 63.5169s/100 iter), 118.3/322.7ep, loss = 0.0550677
I1006 07:08:56.139529  5661 solver.cpp:376]     Train net output #0: loss = 0.0516049 (* 1 = 0.0516049 loss)
I1006 07:08:56.139535  5661 sgd_solver.cpp:172] Iteration 22000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:08:56.140435  5661 solver.cpp:979] Finding and applying sparsity: sparsity_target=0.8 sparsity_factor=0.81 sparsity_achieved=0.797155 iter=22000
W1006 07:08:56.140446  5661 net.cpp:2612] conv1a ni=3 no=32
W1006 07:08:57.861730  5661 net.cpp:2674] conv1a ZeroWeightsFraction=0.316667
W1006 07:08:57.861881  5661 net.cpp:2612] conv1b ni=32 no=32
W1006 07:09:00.549580  5661 net.cpp:2674] conv1b ZeroWeightsFraction=0.695312
W1006 07:09:00.549708  5661 net.cpp:2612] res2a_branch2a ni=32 no=64
W1006 07:09:05.941196  5661 net.cpp:2674] res2a_branch2a ZeroWeightsFraction=0.80013
W1006 07:09:05.941252  5661 net.cpp:2612] res2a_branch2b ni=64 no=64
W1006 07:09:12.415901  5661 net.cpp:2674] res2a_branch2b ZeroWeightsFraction=0.730903
W1006 07:09:12.416050  5661 net.cpp:2612] res3a_branch2a ni=64 no=128
W1006 07:09:21.416100  5661 net.cpp:2674] res3a_branch2a ZeroWeightsFraction=0.807712
W1006 07:09:21.416230  5661 net.cpp:2612] res3a_branch2b ni=128 no=128
W1006 07:09:33.011060  5661 net.cpp:2674] res3a_branch2b ZeroWeightsFraction=0.793403
W1006 07:09:33.011168  5661 net.cpp:2612] res4a_branch2a ni=128 no=256
W1006 07:09:48.006389  5661 net.cpp:2674] res4a_branch2a ZeroWeightsFraction=0.809862
W1006 07:09:48.006522  5661 net.cpp:2612] res4a_branch2b ni=256 no=256
W1006 07:10:08.631965  5661 net.cpp:2674] res4a_branch2b ZeroWeightsFraction=0.80481
W1006 07:10:08.632076  5661 net.cpp:2612] res5a_branch2a ni=256 no=512
W1006 07:10:22.449455  5661 net.cpp:2674] res5a_branch2a ZeroWeightsFraction=0.808667
W1006 07:10:22.449579  5661 net.cpp:2612] res5a_branch2b ni=512 no=512
W1006 07:10:31.007840  5661 net.cpp:2674] res5a_branch2b ZeroWeightsFraction=0.809896
W1006 07:10:31.007941  5661 net.cpp:2612] out5a ni=512 no=64
W1006 07:10:32.741955  5661 net.cpp:2674] out5a ZeroWeightsFraction=0.809896
W1006 07:10:32.742035  5661 net.cpp:2612] out3a ni=128 no=64
W1006 07:10:33.586195  5661 net.cpp:2674] out3a ZeroWeightsFraction=0.809028
W1006 07:10:33.586273  5661 net.cpp:2612] ctx_conv1 ni=64 no=64
W1006 07:10:36.747541  5661 net.cpp:2674] ctx_conv1 ZeroWeightsFraction=0.808974
W1006 07:10:36.747643  5661 net.cpp:2612] ctx_conv2 ni=64 no=64
W1006 07:10:39.311002  5661 net.cpp:2674] ctx_conv2 ZeroWeightsFraction=0.809028
W1006 07:10:39.311089  5661 net.cpp:2612] ctx_conv3 ni=64 no=64
W1006 07:10:42.029562  5661 net.cpp:2674] ctx_conv3 ZeroWeightsFraction=0.809001
W1006 07:10:42.029657  5661 net.cpp:2612] ctx_conv4 ni=64 no=64
W1006 07:10:44.321753  5661 net.cpp:2674] ctx_conv4 ZeroWeightsFraction=0.809028
W1006 07:10:44.321849  5661 net.cpp:2612] ctx_final ni=64 no=8
W1006 07:10:44.433713  5661 net.cpp:2674] ctx_final ZeroWeightsFraction=0.353516
I1006 07:10:44.433737  5661 net.cpp:2707] All zero weights of convolution layers are frozen
I1006 07:10:44.436086  5661 solver.cpp:389] Sparsity after update:
I1006 07:10:44.436971  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 07:10:44.436980  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 07:10:44.436985  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 07:10:44.436988  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 07:10:44.436991  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 07:10:44.436995  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 07:10:44.436997  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 07:10:44.437000  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 07:10:44.437005  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 07:10:44.437006  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 07:10:44.437011  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 07:10:44.437013  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 07:10:44.437016  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 07:10:44.437019  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 07:10:44.437022  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 07:10:44.437026  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 07:10:44.437028  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 07:10:44.437031  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 07:10:44.437036  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 07:11:37.178573  5661 solver.cpp:352] Iteration 22100 (0.620957 iter/s, 161.042s/100 iter), 118.9/322.7ep, loss = 0.0699669
I1006 07:11:37.179373  5661 solver.cpp:376]     Train net output #0: loss = 0.0522136 (* 1 = 0.0522136 loss)
I1006 07:11:37.179381  5661 sgd_solver.cpp:172] Iteration 22100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:11:50.195719  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:11:50.279856  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:12:32.190870  5661 solver.cpp:352] Iteration 22200 (1.81774 iter/s, 55.0132s/100 iter), 119.4/322.7ep, loss = 0.0539431
I1006 07:12:32.191689  5661 solver.cpp:376]     Train net output #0: loss = 0.0560283 (* 1 = 0.0560283 loss)
I1006 07:12:32.191704  5661 sgd_solver.cpp:172] Iteration 22200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:12:57.778338  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 07:13:27.179087  5661 solver.cpp:352] Iteration 22300 (1.81854 iter/s, 54.9892s/100 iter), 119.9/322.7ep, loss = 0.0518178
I1006 07:13:27.179872  5661 solver.cpp:376]     Train net output #0: loss = 0.0559471 (* 1 = 0.0559471 loss)
I1006 07:13:27.179882  5661 sgd_solver.cpp:172] Iteration 22300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:13:32.467931  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:13:32.550961  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:14:22.159000  5661 solver.cpp:352] Iteration 22400 (1.81881 iter/s, 54.9809s/100 iter), 120.5/322.7ep, loss = 0.0702412
I1006 07:14:22.159756  5661 solver.cpp:376]     Train net output #0: loss = 0.0593963 (* 1 = 0.0593963 loss)
I1006 07:14:22.159765  5661 sgd_solver.cpp:172] Iteration 22400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:15:14.785266  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:15:14.858157  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:15:17.215231  5661 solver.cpp:352] Iteration 22500 (1.81629 iter/s, 55.0573s/100 iter), 121/322.7ep, loss = 0.0521723
I1006 07:15:17.215255  5661 solver.cpp:376]     Train net output #0: loss = 0.0505006 (* 1 = 0.0505006 loss)
I1006 07:15:17.215262  5661 sgd_solver.cpp:172] Iteration 22500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:16:12.386960  5661 solver.cpp:352] Iteration 22600 (1.81249 iter/s, 55.1728s/100 iter), 121.5/322.7ep, loss = 0.0538179
I1006 07:16:12.387787  5661 solver.cpp:376]     Train net output #0: loss = 0.0451815 (* 1 = 0.0451815 loss)
I1006 07:16:12.387800  5661 sgd_solver.cpp:172] Iteration 22600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:16:57.958699  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:16:58.027479  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:17:08.122404  5661 solver.cpp:352] Iteration 22700 (1.79416 iter/s, 55.7365s/100 iter), 122.1/322.7ep, loss = 0.0607723
I1006 07:17:08.122429  5661 solver.cpp:376]     Train net output #0: loss = 0.0622004 (* 1 = 0.0622004 loss)
I1006 07:17:08.122438  5661 sgd_solver.cpp:172] Iteration 22700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:18:04.115552  5661 solver.cpp:352] Iteration 22800 (1.7859 iter/s, 55.9942s/100 iter), 122.6/322.7ep, loss = 0.0632908
I1006 07:18:04.116341  5661 solver.cpp:376]     Train net output #0: loss = 0.0598848 (* 1 = 0.0598848 loss)
I1006 07:18:04.116351  5661 sgd_solver.cpp:172] Iteration 22800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:18:41.747478  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:18:41.806308  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:18:59.626559  5661 solver.cpp:352] Iteration 22900 (1.80141 iter/s, 55.5121s/100 iter), 123.2/322.7ep, loss = 0.067506
I1006 07:18:59.626582  5661 solver.cpp:376]     Train net output #0: loss = 0.0732622 (* 1 = 0.0732622 loss)
I1006 07:18:59.626590  5661 sgd_solver.cpp:172] Iteration 22900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:19:54.625628  5661 solver.cpp:352] Iteration 23000 (1.81818 iter/s, 55.0001s/100 iter), 123.7/322.7ep, loss = 0.0481361
I1006 07:19:54.626469  5661 solver.cpp:376]     Train net output #0: loss = 0.0520793 (* 1 = 0.0520793 loss)
I1006 07:19:54.626478  5661 sgd_solver.cpp:172] Iteration 23000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:19:54.627732  5661 solver.cpp:389] Sparsity after update:
I1006 07:19:54.628598  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 07:19:54.628607  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 07:19:54.628612  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 07:19:54.628615  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 07:19:54.628618  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 07:19:54.628621  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 07:19:54.628624  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 07:19:54.628628  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 07:19:54.628631  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 07:19:54.628633  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 07:19:54.628636  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 07:19:54.628640  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 07:19:54.628643  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 07:19:54.628646  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 07:19:54.628649  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 07:19:54.628653  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 07:19:54.628655  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 07:19:54.628659  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 07:19:54.628661  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 07:20:24.125564  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:20:24.188444  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:20:49.689504  5661 solver.cpp:352] Iteration 23100 (1.81604 iter/s, 55.0649s/100 iter), 124.2/322.7ep, loss = 0.0502663
I1006 07:20:49.689688  5661 solver.cpp:376]     Train net output #0: loss = 0.0540003 (* 1 = 0.0540003 loss)
I1006 07:20:49.689699  5661 sgd_solver.cpp:172] Iteration 23100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:21:44.772478  5661 solver.cpp:352] Iteration 23200 (1.81541 iter/s, 55.084s/100 iter), 124.8/322.7ep, loss = 0.071009
I1006 07:21:44.772640  5661 solver.cpp:376]     Train net output #0: loss = 0.071863 (* 1 = 0.071863 loss)
I1006 07:21:44.772651  5661 sgd_solver.cpp:172] Iteration 23200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:22:06.528560  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:22:06.571782  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:22:39.809482  5661 solver.cpp:352] Iteration 23300 (1.81692 iter/s, 55.0381s/100 iter), 125.3/322.7ep, loss = 0.0597436
I1006 07:22:39.810307  5661 solver.cpp:376]     Train net output #0: loss = 0.0487358 (* 1 = 0.0487358 loss)
I1006 07:22:39.810317  5661 sgd_solver.cpp:172] Iteration 23300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:23:34.628695  5661 solver.cpp:352] Iteration 23400 (1.82414 iter/s, 54.8203s/100 iter), 125.8/322.7ep, loss = 0.0411966
I1006 07:23:34.629693  5661 solver.cpp:376]     Train net output #0: loss = 0.0437358 (* 1 = 0.0437358 loss)
I1006 07:23:34.629703  5661 sgd_solver.cpp:172] Iteration 23400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:23:48.635550  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:23:48.656476  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:24:29.522027  5661 solver.cpp:352] Iteration 23500 (1.82168 iter/s, 54.8944s/100 iter), 126.4/322.7ep, loss = 0.0488067
I1006 07:24:29.522933  5661 solver.cpp:376]     Train net output #0: loss = 0.0443744 (* 1 = 0.0443744 loss)
I1006 07:24:29.522945  5661 sgd_solver.cpp:172] Iteration 23500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:25:24.387576  5661 solver.cpp:352] Iteration 23600 (1.8226 iter/s, 54.8666s/100 iter), 126.9/322.7ep, loss = 0.0539149
I1006 07:25:24.388392  5661 solver.cpp:376]     Train net output #0: loss = 0.0635122 (* 1 = 0.0635122 loss)
I1006 07:25:24.388401  5661 sgd_solver.cpp:172] Iteration 23600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:25:30.477495  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:25:30.566656  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:26:19.302368  5661 solver.cpp:352] Iteration 23700 (1.82097 iter/s, 54.9159s/100 iter), 127.5/322.7ep, loss = 0.0455671
I1006 07:26:19.303208  5661 solver.cpp:376]     Train net output #0: loss = 0.0460559 (* 1 = 0.0460559 loss)
I1006 07:26:19.303217  5661 sgd_solver.cpp:172] Iteration 23700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:27:12.636291  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:27:12.717051  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:27:14.236287  5661 solver.cpp:352] Iteration 23800 (1.82033 iter/s, 54.935s/100 iter), 128/322.7ep, loss = 0.0539036
I1006 07:27:14.236311  5661 solver.cpp:376]     Train net output #0: loss = 0.0400248 (* 1 = 0.0400248 loss)
I1006 07:27:14.236318  5661 sgd_solver.cpp:172] Iteration 23800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:28:09.134671  5661 solver.cpp:352] Iteration 23900 (1.82151 iter/s, 54.8994s/100 iter), 128.5/322.7ep, loss = 0.0731394
I1006 07:28:09.135483  5661 solver.cpp:376]     Train net output #0: loss = 0.069376 (* 1 = 0.069376 loss)
I1006 07:28:09.135491  5661 sgd_solver.cpp:172] Iteration 23900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:28:54.687997  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:28:54.761147  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:29:03.456749  5661 solver.cpp:538] Iteration 24000, Testing net (#0)
I1006 07:29:12.221767  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:29:12.233820  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:29:12.322613  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.950133
I1006 07:29:12.322638  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 07:29:12.322649  5661 solver.cpp:624]     Test net output #2: loss = 0.165261 (* 1 = 0.165261 loss)
I1006 07:29:12.322669  5661 solver.cpp:283] Tests completed in 63.1892s
I1006 07:29:12.873539  5661 solver.cpp:352] Iteration 24000 (1.58255 iter/s, 63.1892s/100 iter), 129.1/322.7ep, loss = 0.0491195
I1006 07:29:12.873564  5661 solver.cpp:376]     Train net output #0: loss = 0.0405326 (* 1 = 0.0405326 loss)
I1006 07:29:12.873572  5661 sgd_solver.cpp:172] Iteration 24000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:29:12.874874  5661 solver.cpp:389] Sparsity after update:
I1006 07:29:12.875782  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 07:29:12.875789  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 07:29:12.875795  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 07:29:12.875798  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 07:29:12.875802  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 07:29:12.875805  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 07:29:12.875809  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 07:29:12.875813  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 07:29:12.875815  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 07:29:12.875818  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 07:29:12.875823  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 07:29:12.875825  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 07:29:12.875829  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 07:29:12.875833  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 07:29:12.875835  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 07:29:12.875838  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 07:29:12.875843  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 07:29:12.875845  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 07:29:12.875849  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 07:30:07.902276  5661 solver.cpp:352] Iteration 24100 (1.8172 iter/s, 55.0298s/100 iter), 129.6/322.7ep, loss = 0.0394509
I1006 07:30:07.903093  5661 solver.cpp:376]     Train net output #0: loss = 0.0360578 (* 1 = 0.0360578 loss)
I1006 07:30:07.903102  5661 sgd_solver.cpp:172] Iteration 24100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:30:47.191625  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:30:47.257181  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:31:04.900460  5661 solver.cpp:352] Iteration 24200 (1.75441 iter/s, 56.9992s/100 iter), 130.2/322.7ep, loss = 0.0501962
I1006 07:31:04.900485  5661 solver.cpp:376]     Train net output #0: loss = 0.0501792 (* 1 = 0.0501792 loss)
I1006 07:31:04.900492  5661 sgd_solver.cpp:172] Iteration 24200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:32:01.395388  5661 solver.cpp:352] Iteration 24300 (1.77006 iter/s, 56.4954s/100 iter), 130.7/322.7ep, loss = 0.0541285
I1006 07:32:01.396212  5661 solver.cpp:376]     Train net output #0: loss = 0.0549088 (* 1 = 0.0549088 loss)
I1006 07:32:01.396224  5661 sgd_solver.cpp:172] Iteration 24300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:32:31.896813  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:32:31.956871  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:32:57.408771  5661 solver.cpp:352] Iteration 24400 (1.78527 iter/s, 56.0139s/100 iter), 131.2/322.7ep, loss = 0.0568464
I1006 07:32:57.408802  5661 solver.cpp:376]     Train net output #0: loss = 0.0428416 (* 1 = 0.0428416 loss)
I1006 07:32:57.408812  5661 sgd_solver.cpp:172] Iteration 24400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:33:51.534135  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 07:33:53.461468  5661 solver.cpp:352] Iteration 24500 (1.78402 iter/s, 56.0532s/100 iter), 131.8/322.7ep, loss = 0.0540929
I1006 07:33:53.461491  5661 solver.cpp:376]     Train net output #0: loss = 0.0599081 (* 1 = 0.0599081 loss)
I1006 07:33:53.461498  5661 sgd_solver.cpp:172] Iteration 24500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:34:16.145593  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:34:16.198469  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:34:48.865839  5661 solver.cpp:352] Iteration 24600 (1.80489 iter/s, 55.4049s/100 iter), 132.3/322.7ep, loss = 0.0597601
I1006 07:34:48.866649  5661 solver.cpp:376]     Train net output #0: loss = 0.0577042 (* 1 = 0.0577042 loss)
I1006 07:34:48.866659  5661 sgd_solver.cpp:172] Iteration 24600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:35:45.108228  5661 solver.cpp:352] Iteration 24700 (1.778 iter/s, 56.243s/100 iter), 132.8/322.7ep, loss = 0.0652446
I1006 07:35:45.109109  5661 solver.cpp:376]     Train net output #0: loss = 0.053255 (* 1 = 0.053255 loss)
I1006 07:35:45.109122  5661 sgd_solver.cpp:172] Iteration 24700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:36:00.422364  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:36:00.464814  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:36:41.404335  5661 solver.cpp:352] Iteration 24800 (1.7763 iter/s, 56.2967s/100 iter), 133.4/322.7ep, loss = 0.0534153
I1006 07:36:41.405184  5661 solver.cpp:376]     Train net output #0: loss = 0.0305928 (* 1 = 0.0305928 loss)
I1006 07:36:41.405197  5661 sgd_solver.cpp:172] Iteration 24800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:37:37.715203  5661 solver.cpp:352] Iteration 24900 (1.77583 iter/s, 56.3115s/100 iter), 133.9/322.7ep, loss = 0.0686623
I1006 07:37:37.716032  5661 solver.cpp:376]     Train net output #0: loss = 0.0741639 (* 1 = 0.0741639 loss)
I1006 07:37:37.716042  5661 sgd_solver.cpp:172] Iteration 24900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:37:44.957412  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:37:44.978380  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:38:33.662412  5661 solver.cpp:352] Iteration 25000 (1.78738 iter/s, 55.9479s/100 iter), 134.5/322.7ep, loss = 0.0643366
I1006 07:38:33.663209  5661 solver.cpp:376]     Train net output #0: loss = 0.0643099 (* 1 = 0.0643099 loss)
I1006 07:38:33.663220  5661 sgd_solver.cpp:172] Iteration 25000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:38:33.664650  5661 solver.cpp:389] Sparsity after update:
I1006 07:38:33.665608  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 07:38:33.665616  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 07:38:33.665623  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 07:38:33.665627  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 07:38:33.665630  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 07:38:33.665634  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 07:38:33.665637  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 07:38:33.665640  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 07:38:33.665644  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 07:38:33.665647  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 07:38:33.665652  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 07:38:33.665654  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 07:38:33.665658  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 07:38:33.665661  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 07:38:33.665665  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 07:38:33.665668  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 07:38:33.665671  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 07:38:33.665676  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 07:38:33.665679  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 07:39:29.703950  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:39:29.789038  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:39:30.512418  5661 solver.cpp:352] Iteration 25100 (1.75899 iter/s, 56.8507s/100 iter), 135/322.7ep, loss = 0.0549066
I1006 07:39:30.512444  5661 solver.cpp:376]     Train net output #0: loss = 0.0510691 (* 1 = 0.0510691 loss)
I1006 07:39:30.512454  5661 sgd_solver.cpp:172] Iteration 25100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:40:26.978935  5661 solver.cpp:352] Iteration 25200 (1.77094 iter/s, 56.4673s/100 iter), 135.5/322.7ep, loss = 0.0597916
I1006 07:40:26.979797  5661 solver.cpp:376]     Train net output #0: loss = 0.0797277 (* 1 = 0.0797277 loss)
I1006 07:40:26.979809  5661 sgd_solver.cpp:172] Iteration 25200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:41:15.446805  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:41:15.528316  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:41:24.313519  5661 solver.cpp:352] Iteration 25300 (1.74412 iter/s, 57.3354s/100 iter), 136.1/322.7ep, loss = 0.0605302
I1006 07:41:24.313547  5661 solver.cpp:376]     Train net output #0: loss = 0.0546585 (* 1 = 0.0546585 loss)
I1006 07:41:24.313557  5661 sgd_solver.cpp:172] Iteration 25300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:42:20.529389  5661 solver.cpp:352] Iteration 25400 (1.77883 iter/s, 56.2167s/100 iter), 136.6/322.7ep, loss = 0.0444909
I1006 07:42:20.530221  5661 solver.cpp:376]     Train net output #0: loss = 0.0490174 (* 1 = 0.0490174 loss)
I1006 07:42:20.530231  5661 sgd_solver.cpp:172] Iteration 25400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:43:00.729753  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:43:00.802721  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:43:17.489773  5661 solver.cpp:352] Iteration 25500 (1.75558 iter/s, 56.9612s/100 iter), 137.1/322.7ep, loss = 0.0529221
I1006 07:43:17.489801  5661 solver.cpp:376]     Train net output #0: loss = 0.0566812 (* 1 = 0.0566812 loss)
I1006 07:43:17.489810  5661 sgd_solver.cpp:172] Iteration 25500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:44:13.848634  5661 solver.cpp:352] Iteration 25600 (1.77432 iter/s, 56.3597s/100 iter), 137.7/322.7ep, loss = 0.051282
I1006 07:44:13.849519  5661 solver.cpp:376]     Train net output #0: loss = 0.0395307 (* 1 = 0.0395307 loss)
I1006 07:44:13.849530  5661 sgd_solver.cpp:172] Iteration 25600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:44:46.180619  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:44:46.247263  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:45:10.676532  5661 solver.cpp:352] Iteration 25700 (1.75967 iter/s, 56.8288s/100 iter), 138.2/322.7ep, loss = 0.052622
I1006 07:45:10.676560  5661 solver.cpp:376]     Train net output #0: loss = 0.0365969 (* 1 = 0.0365969 loss)
I1006 07:45:10.676569  5661 sgd_solver.cpp:172] Iteration 25700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:46:06.925559  5661 solver.cpp:352] Iteration 25800 (1.77778 iter/s, 56.2499s/100 iter), 138.8/322.7ep, loss = 0.0547296
I1006 07:46:06.925942  5661 solver.cpp:376]     Train net output #0: loss = 0.0584692 (* 1 = 0.0584692 loss)
I1006 07:46:06.925966  5661 sgd_solver.cpp:172] Iteration 25800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:46:31.339146  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:46:31.402415  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:47:04.160218  5661 solver.cpp:352] Iteration 25900 (1.74717 iter/s, 57.2356s/100 iter), 139.3/322.7ep, loss = 0.056576
I1006 07:47:04.161056  5661 solver.cpp:376]     Train net output #0: loss = 0.0721048 (* 1 = 0.0721048 loss)
I1006 07:47:04.161065  5661 sgd_solver.cpp:172] Iteration 25900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:47:59.610546  5661 solver.cpp:538] Iteration 26000, Testing net (#0)
I1006 07:48:08.699949  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:48:08.711946  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:48:08.813428  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.950031
I1006 07:48:08.813459  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999986
I1006 07:48:08.813472  5661 solver.cpp:624]     Test net output #2: loss = 0.20329 (* 1 = 0.20329 loss)
I1006 07:48:08.813493  5661 solver.cpp:283] Tests completed in 64.6543s
I1006 07:48:09.389291  5661 solver.cpp:352] Iteration 26000 (1.54669 iter/s, 64.6543s/100 iter), 139.8/322.7ep, loss = 0.0511781
I1006 07:48:09.389315  5661 solver.cpp:376]     Train net output #0: loss = 0.0371669 (* 1 = 0.0371669 loss)
I1006 07:48:09.389323  5661 sgd_solver.cpp:172] Iteration 26000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:48:09.390666  5661 solver.cpp:389] Sparsity after update:
I1006 07:48:09.391607  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 07:48:09.391615  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 07:48:09.391621  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 07:48:09.391625  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 07:48:09.391628  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 07:48:09.391633  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 07:48:09.391635  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 07:48:09.391638  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 07:48:09.391643  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 07:48:09.391645  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 07:48:09.391649  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 07:48:09.391652  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 07:48:09.391655  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 07:48:09.391659  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 07:48:09.391662  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 07:48:09.391666  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 07:48:09.391671  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 07:48:09.391675  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 07:48:09.391679  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 07:48:25.389689  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:48:25.442571  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:49:05.164626  5661 solver.cpp:352] Iteration 26100 (1.79288 iter/s, 55.7762s/100 iter), 140.4/322.7ep, loss = 0.0486235
I1006 07:49:05.165415  5661 solver.cpp:376]     Train net output #0: loss = 0.0463094 (* 1 = 0.0463094 loss)
I1006 07:49:05.165423  5661 sgd_solver.cpp:172] Iteration 26100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:50:01.609925  5661 solver.cpp:352] Iteration 26200 (1.7716 iter/s, 56.4462s/100 iter), 140.9/322.7ep, loss = 0.0865527
I1006 07:50:01.610774  5661 solver.cpp:376]     Train net output #0: loss = 0.0542266 (* 1 = 0.0542266 loss)
I1006 07:50:01.610782  5661 sgd_solver.cpp:172] Iteration 26200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:50:10.450091  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:50:10.494778  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:50:58.083012  5661 solver.cpp:352] Iteration 26300 (1.77073 iter/s, 56.474s/100 iter), 141.4/322.7ep, loss = 0.0560664
I1006 07:50:58.083799  5661 solver.cpp:376]     Train net output #0: loss = 0.0575979 (* 1 = 0.0575979 loss)
I1006 07:50:58.083809  5661 sgd_solver.cpp:172] Iteration 26300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:51:53.559845  5661 solver.cpp:352] Iteration 26400 (1.80252 iter/s, 55.4778s/100 iter), 142/322.7ep, loss = 0.0495025
I1006 07:51:53.560621  5661 solver.cpp:376]     Train net output #0: loss = 0.0590221 (* 1 = 0.0590221 loss)
I1006 07:51:53.560631  5661 sgd_solver.cpp:172] Iteration 26400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:51:53.846225  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:51:53.877990  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:52:48.830478  5661 solver.cpp:352] Iteration 26500 (1.80925 iter/s, 55.2716s/100 iter), 142.5/322.7ep, loss = 0.0550173
I1006 07:52:48.830646  5661 solver.cpp:376]     Train net output #0: loss = 0.0670246 (* 1 = 0.0670246 loss)
I1006 07:52:48.830655  5661 sgd_solver.cpp:172] Iteration 26500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:53:36.325853  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:53:36.414842  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:53:43.979815  5661 solver.cpp:352] Iteration 26600 (1.81323 iter/s, 55.1503s/100 iter), 143.1/322.7ep, loss = 0.0721181
I1006 07:53:43.979842  5661 solver.cpp:376]     Train net output #0: loss = 0.0640738 (* 1 = 0.0640738 loss)
I1006 07:53:43.979851  5661 sgd_solver.cpp:172] Iteration 26600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:54:39.139256  5661 solver.cpp:352] Iteration 26700 (1.8129 iter/s, 55.1604s/100 iter), 143.6/322.7ep, loss = 0.0590792
I1006 07:54:39.140077  5661 solver.cpp:376]     Train net output #0: loss = 0.0586277 (* 1 = 0.0586277 loss)
I1006 07:54:39.140086  5661 sgd_solver.cpp:172] Iteration 26700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:55:04.801848  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 07:55:18.944273  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:55:19.035537  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:55:34.365869  5661 solver.cpp:352] Iteration 26800 (1.81069 iter/s, 55.2275s/100 iter), 144.1/322.7ep, loss = 0.0619214
I1006 07:55:34.365900  5661 solver.cpp:376]     Train net output #0: loss = 0.0715885 (* 1 = 0.0715885 loss)
I1006 07:55:34.365911  5661 sgd_solver.cpp:172] Iteration 26800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:56:29.588259  5661 solver.cpp:352] Iteration 26900 (1.81083 iter/s, 55.2233s/100 iter), 144.7/322.7ep, loss = 0.0468148
I1006 07:56:29.588423  5661 solver.cpp:376]     Train net output #0: loss = 0.0405299 (* 1 = 0.0405299 loss)
I1006 07:56:29.588433  5661 sgd_solver.cpp:172] Iteration 26900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:57:01.640183  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:57:01.715255  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:57:24.773771  5661 solver.cpp:352] Iteration 27000 (1.81204 iter/s, 55.1864s/100 iter), 145.2/322.7ep, loss = 0.0489908
I1006 07:57:24.773795  5661 solver.cpp:376]     Train net output #0: loss = 0.0547089 (* 1 = 0.0547089 loss)
I1006 07:57:24.773802  5661 sgd_solver.cpp:172] Iteration 27000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:57:24.775099  5661 solver.cpp:389] Sparsity after update:
I1006 07:57:24.775997  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 07:57:24.776005  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 07:57:24.776011  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 07:57:24.776015  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 07:57:24.776017  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 07:57:24.776021  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 07:57:24.776024  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 07:57:24.776027  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 07:57:24.776031  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 07:57:24.776033  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 07:57:24.776036  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 07:57:24.776039  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 07:57:24.776043  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 07:57:24.776046  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 07:57:24.776049  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 07:57:24.776052  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 07:57:24.776055  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 07:57:24.776058  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 07:57:24.776062  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 07:58:20.003224  5661 solver.cpp:352] Iteration 27100 (1.8106 iter/s, 55.2304s/100 iter), 145.7/322.7ep, loss = 0.0681883
I1006 07:58:20.004109  5661 solver.cpp:376]     Train net output #0: loss = 0.0614509 (* 1 = 0.0614509 loss)
I1006 07:58:20.004122  5661 sgd_solver.cpp:172] Iteration 27100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 07:58:44.340292  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:58:44.407598  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 07:59:15.195338  5661 solver.cpp:352] Iteration 27200 (1.81182 iter/s, 55.193s/100 iter), 146.3/322.7ep, loss = 0.0516929
I1006 07:59:15.195503  5661 solver.cpp:376]     Train net output #0: loss = 0.0391898 (* 1 = 0.0391898 loss)
I1006 07:59:15.195513  5661 sgd_solver.cpp:172] Iteration 27200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:00:10.417373  5661 solver.cpp:352] Iteration 27300 (1.81084 iter/s, 55.223s/100 iter), 146.8/322.7ep, loss = 0.0490685
I1006 08:00:10.417544  5661 solver.cpp:376]     Train net output #0: loss = 0.0514829 (* 1 = 0.0514829 loss)
I1006 08:00:10.417553  5661 sgd_solver.cpp:172] Iteration 27300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:00:27.036406  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:00:27.098532  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:01:05.605464  5661 solver.cpp:352] Iteration 27400 (1.81196 iter/s, 55.189s/100 iter), 147.4/322.7ep, loss = 0.0639396
I1006 08:01:05.606279  5661 solver.cpp:376]     Train net output #0: loss = 0.0586526 (* 1 = 0.0586526 loss)
I1006 08:01:05.606289  5661 sgd_solver.cpp:172] Iteration 27400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:02:00.890872  5661 solver.cpp:352] Iteration 27500 (1.80877 iter/s, 55.2863s/100 iter), 147.9/322.7ep, loss = 0.053195
I1006 08:02:00.891682  5661 solver.cpp:376]     Train net output #0: loss = 0.0509708 (* 1 = 0.0509708 loss)
I1006 08:02:00.891692  5661 sgd_solver.cpp:172] Iteration 27500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:02:09.753343  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:02:09.806571  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:02:56.087146  5661 solver.cpp:352] Iteration 27600 (1.81169 iter/s, 55.1972s/100 iter), 148.4/322.7ep, loss = 0.0655889
I1006 08:02:56.087975  5661 solver.cpp:376]     Train net output #0: loss = 0.0607078 (* 1 = 0.0607078 loss)
I1006 08:02:56.087985  5661 sgd_solver.cpp:172] Iteration 27600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:03:51.146020  5661 solver.cpp:352] Iteration 27700 (1.81621 iter/s, 55.0598s/100 iter), 149/322.7ep, loss = 0.0478634
I1006 08:03:51.146817  5661 solver.cpp:376]     Train net output #0: loss = 0.0416087 (* 1 = 0.0416087 loss)
I1006 08:03:51.146827  5661 sgd_solver.cpp:172] Iteration 27700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:03:52.267482  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:03:52.313282  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:04:46.198951  5661 solver.cpp:352] Iteration 27800 (1.8164 iter/s, 55.0538s/100 iter), 149.5/322.7ep, loss = 0.0533792
I1006 08:04:46.199767  5661 solver.cpp:376]     Train net output #0: loss = 0.0471671 (* 1 = 0.0471671 loss)
I1006 08:04:46.199776  5661 sgd_solver.cpp:172] Iteration 27800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:05:34.734623  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:05:34.756350  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:05:41.346987  5661 solver.cpp:352] Iteration 27900 (1.81327 iter/s, 55.149s/100 iter), 150.1/322.7ep, loss = 0.048486
I1006 08:05:41.347012  5661 solver.cpp:376]     Train net output #0: loss = 0.0455949 (* 1 = 0.0455949 loss)
I1006 08:05:41.347019  5661 sgd_solver.cpp:172] Iteration 27900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:06:35.856884  5661 solver.cpp:538] Iteration 28000, Testing net (#0)
I1006 08:06:45.062871  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:06:45.075790  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:06:45.163148  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.94733
I1006 08:06:45.163174  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 1
I1006 08:06:45.163184  5661 solver.cpp:624]     Test net output #2: loss = 0.169296 (* 1 = 0.169296 loss)
I1006 08:06:45.163203  5661 solver.cpp:283] Tests completed in 63.8174s
I1006 08:06:45.715092  5661 solver.cpp:352] Iteration 28000 (1.56697 iter/s, 63.8174s/100 iter), 150.6/322.7ep, loss = 0.0567251
I1006 08:06:45.715116  5661 solver.cpp:376]     Train net output #0: loss = 0.0558051 (* 1 = 0.0558051 loss)
I1006 08:06:45.715123  5661 sgd_solver.cpp:172] Iteration 28000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:06:45.716444  5661 solver.cpp:389] Sparsity after update:
I1006 08:06:45.717346  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 08:06:45.717355  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 08:06:45.717360  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 08:06:45.717363  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 08:06:45.717367  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 08:06:45.717370  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 08:06:45.717373  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 08:06:45.717376  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 08:06:45.717380  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 08:06:45.717382  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 08:06:45.717386  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 08:06:45.717389  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 08:06:45.717392  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 08:06:45.717396  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 08:06:45.717398  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 08:06:45.717401  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 08:06:45.717406  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 08:06:45.717408  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 08:06:45.717411  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 08:07:26.288378  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:07:26.369128  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:07:40.893643  5661 solver.cpp:352] Iteration 28100 (1.81227 iter/s, 55.1796s/100 iter), 151.1/322.7ep, loss = 0.0690043
I1006 08:07:40.893671  5661 solver.cpp:376]     Train net output #0: loss = 0.071659 (* 1 = 0.071659 loss)
I1006 08:07:40.893679  5661 sgd_solver.cpp:172] Iteration 28100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:08:35.968909  5661 solver.cpp:352] Iteration 28200 (1.81566 iter/s, 55.0763s/100 iter), 151.7/322.7ep, loss = 0.0642831
I1006 08:08:35.969789  5661 solver.cpp:376]     Train net output #0: loss = 0.0789828 (* 1 = 0.0789828 loss)
I1006 08:08:35.969800  5661 sgd_solver.cpp:172] Iteration 28200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:09:08.781261  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:09:08.861479  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:09:30.999364  5661 solver.cpp:352] Iteration 28300 (1.81714 iter/s, 55.0315s/100 iter), 152.2/322.7ep, loss = 0.0530024
I1006 08:09:30.999388  5661 solver.cpp:376]     Train net output #0: loss = 0.0613482 (* 1 = 0.0613482 loss)
I1006 08:09:30.999397  5661 sgd_solver.cpp:172] Iteration 28300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:10:26.113747  5661 solver.cpp:352] Iteration 28400 (1.81437 iter/s, 55.1154s/100 iter), 152.7/322.7ep, loss = 0.052722
I1006 08:10:26.114573  5661 solver.cpp:376]     Train net output #0: loss = 0.0686348 (* 1 = 0.0686348 loss)
I1006 08:10:26.114583  5661 sgd_solver.cpp:172] Iteration 28400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:10:51.285218  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:10:51.361609  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:11:21.675472  5661 solver.cpp:352] Iteration 28500 (1.79977 iter/s, 55.5628s/100 iter), 153.3/322.7ep, loss = 0.0443287
I1006 08:11:21.676270  5661 solver.cpp:376]     Train net output #0: loss = 0.0520081 (* 1 = 0.0520081 loss)
I1006 08:11:21.676278  5661 sgd_solver.cpp:172] Iteration 28500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:12:18.163476  5661 solver.cpp:352] Iteration 28600 (1.77025 iter/s, 56.4891s/100 iter), 153.8/322.7ep, loss = 0.0478515
I1006 08:12:18.164347  5661 solver.cpp:376]     Train net output #0: loss = 0.0389861 (* 1 = 0.0389861 loss)
I1006 08:12:18.164360  5661 sgd_solver.cpp:172] Iteration 28600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:12:36.659073  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:12:36.728196  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:13:15.205318  5661 solver.cpp:352] Iteration 28700 (1.75307 iter/s, 57.0429s/100 iter), 154.4/322.7ep, loss = 0.0540605
I1006 08:13:15.205581  5661 solver.cpp:376]     Train net output #0: loss = 0.0426174 (* 1 = 0.0426174 loss)
I1006 08:13:15.205600  5661 sgd_solver.cpp:172] Iteration 28700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:14:12.452919  5661 solver.cpp:352] Iteration 28800 (1.74677 iter/s, 57.2487s/100 iter), 154.9/322.7ep, loss = 0.0604054
I1006 08:14:12.453747  5661 solver.cpp:376]     Train net output #0: loss = 0.061819 (* 1 = 0.061819 loss)
I1006 08:14:12.453758  5661 sgd_solver.cpp:172] Iteration 28800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:14:22.207609  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:14:22.268157  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:15:09.527169  5661 solver.cpp:352] Iteration 28900 (1.75207 iter/s, 57.0753s/100 iter), 155.4/322.7ep, loss = 0.04037
I1006 08:15:09.527969  5661 solver.cpp:376]     Train net output #0: loss = 0.0385815 (* 1 = 0.0385815 loss)
I1006 08:15:09.527979  5661 sgd_solver.cpp:172] Iteration 28900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:16:04.032186  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 08:16:05.970952  5661 solver.cpp:352] Iteration 29000 (1.77164 iter/s, 56.4448s/100 iter), 156/322.7ep, loss = 0.0468408
I1006 08:16:05.970983  5661 solver.cpp:376]     Train net output #0: loss = 0.0377438 (* 1 = 0.0377438 loss)
I1006 08:16:05.970993  5661 sgd_solver.cpp:172] Iteration 29000, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:16:05.972476  5661 solver.cpp:389] Sparsity after update:
I1006 08:16:05.974771  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 08:16:05.974782  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 08:16:05.974789  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 08:16:05.974793  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 08:16:05.974797  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 08:16:05.974802  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 08:16:05.974805  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 08:16:05.974809  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 08:16:05.974813  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 08:16:05.974817  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 08:16:05.974822  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 08:16:05.974825  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 08:16:05.974830  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 08:16:05.974834  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 08:16:05.974838  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 08:16:05.974841  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 08:16:05.974845  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 08:16:05.974849  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 08:16:05.974856  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 08:16:07.958039  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:16:08.011474  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:17:00.974615  5661 solver.cpp:352] Iteration 29100 (1.81803 iter/s, 55.0047s/100 iter), 156.5/322.7ep, loss = 0.0475581
I1006 08:17:00.975486  5661 solver.cpp:376]     Train net output #0: loss = 0.0311073 (* 1 = 0.0311073 loss)
I1006 08:17:00.975495  5661 sgd_solver.cpp:172] Iteration 29100, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:17:50.196728  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:17:50.245501  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:17:55.961632  5661 solver.cpp:352] Iteration 29200 (1.81858 iter/s, 54.988s/100 iter), 157/322.7ep, loss = 0.050151
I1006 08:17:55.961658  5661 solver.cpp:376]     Train net output #0: loss = 0.0551617 (* 1 = 0.0551617 loss)
I1006 08:17:55.961666  5661 sgd_solver.cpp:172] Iteration 29200, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:18:50.967599  5661 solver.cpp:352] Iteration 29300 (1.81795 iter/s, 55.007s/100 iter), 157.6/322.7ep, loss = 0.042816
I1006 08:18:50.968406  5661 solver.cpp:376]     Train net output #0: loss = 0.0443688 (* 1 = 0.0443688 loss)
I1006 08:18:50.968416  5661 sgd_solver.cpp:172] Iteration 29300, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:19:32.544178  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:19:32.571789  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:19:46.000341  5661 solver.cpp:352] Iteration 29400 (1.81707 iter/s, 55.0337s/100 iter), 158.1/322.7ep, loss = 0.0446932
I1006 08:19:46.000372  5661 solver.cpp:376]     Train net output #0: loss = 0.0381034 (* 1 = 0.0381034 loss)
I1006 08:19:46.000382  5661 sgd_solver.cpp:172] Iteration 29400, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:20:41.028182  5661 solver.cpp:352] Iteration 29500 (1.81723 iter/s, 55.0288s/100 iter), 158.7/322.7ep, loss = 0.0454621
I1006 08:20:41.028995  5661 solver.cpp:376]     Train net output #0: loss = 0.0418374 (* 1 = 0.0418374 loss)
I1006 08:20:41.029006  5661 sgd_solver.cpp:172] Iteration 29500, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:21:14.652688  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:21:14.737607  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:21:36.043629  5661 solver.cpp:352] Iteration 29600 (1.81764 iter/s, 55.0164s/100 iter), 159.2/322.7ep, loss = 0.0624
I1006 08:21:36.043655  5661 solver.cpp:376]     Train net output #0: loss = 0.0622154 (* 1 = 0.0622154 loss)
I1006 08:21:36.043663  5661 sgd_solver.cpp:172] Iteration 29600, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:22:31.052909  5661 solver.cpp:352] Iteration 29700 (1.81784 iter/s, 55.0102s/100 iter), 159.7/322.7ep, loss = 0.0661407
I1006 08:22:31.053722  5661 solver.cpp:376]     Train net output #0: loss = 0.0847535 (* 1 = 0.0847535 loss)
I1006 08:22:31.053731  5661 sgd_solver.cpp:172] Iteration 29700, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:22:56.975786  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:22:57.054772  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:23:26.072414  5661 solver.cpp:352] Iteration 29800 (1.81751 iter/s, 55.0205s/100 iter), 160.3/322.7ep, loss = 0.0603693
I1006 08:23:26.072580  5661 solver.cpp:376]     Train net output #0: loss = 0.0693011 (* 1 = 0.0693011 loss)
I1006 08:23:26.072589  5661 sgd_solver.cpp:172] Iteration 29800, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:24:21.138586  5661 solver.cpp:352] Iteration 29900 (1.81597 iter/s, 55.0671s/100 iter), 160.8/322.7ep, loss = 0.0610748
I1006 08:24:21.139395  5661 solver.cpp:376]     Train net output #0: loss = 0.0651066 (* 1 = 0.0651066 loss)
I1006 08:24:21.139405  5661 sgd_solver.cpp:172] Iteration 29900, lr = 0.01, m = 0.9, wd = 1e-05, gs = 1
I1006 08:24:39.341650  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:24:39.414353  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:25:15.664615  5661 solver.cpp:905] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_30000.caffemodel
I1006 08:25:15.679190  5661 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_30000.solverstate
I1006 08:25:15.685261  5661 solver.cpp:538] Iteration 30000, Testing net (#0)
I1006 08:25:24.575845  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:25:24.587008  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:25:24.683305  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.955449
I1006 08:25:24.683332  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 08:25:24.683341  5661 solver.cpp:624]     Test net output #2: loss = 0.166942 (* 1 = 0.166942 loss)
I1006 08:25:24.683358  5661 solver.cpp:283] Tests completed in 63.5459s
I1006 08:25:25.078222  5692 sgd_solver.cpp:50] MultiStep Status: Iteration 30000, step = 1
I1006 08:25:25.237833  5661 solver.cpp:352] Iteration 30000 (1.57367 iter/s, 63.5459s/100 iter), 161.3/322.7ep, loss = 0.0492195
I1006 08:25:25.237849  5661 solver.cpp:376]     Train net output #0: loss = 0.0346511 (* 1 = 0.0346511 loss)
I1006 08:25:25.237855  5661 sgd_solver.cpp:172] Iteration 30000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:25:25.239087  5661 solver.cpp:389] Sparsity after update:
I1006 08:25:25.239938  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 08:25:25.239946  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 08:25:25.239950  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 08:25:25.239953  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 08:25:25.239958  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 08:25:25.239960  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 08:25:25.239964  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 08:25:25.239966  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 08:25:25.239969  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 08:25:25.239971  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 08:25:25.239974  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 08:25:25.239977  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 08:25:25.239980  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 08:25:25.239984  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 08:25:25.239986  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 08:25:25.239989  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 08:25:25.239992  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 08:25:25.239995  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 08:25:25.239998  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 08:26:20.160254  5661 solver.cpp:352] Iteration 30100 (1.82072 iter/s, 54.9234s/100 iter), 161.9/322.7ep, loss = 0.0561527
I1006 08:26:20.161067  5661 solver.cpp:376]     Train net output #0: loss = 0.0622469 (* 1 = 0.0622469 loss)
I1006 08:26:20.161075  5661 sgd_solver.cpp:172] Iteration 30100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:26:30.682627  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:26:30.747933  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:27:15.159555  5661 solver.cpp:352] Iteration 30200 (1.81817 iter/s, 55.0002s/100 iter), 162.4/322.7ep, loss = 0.0552739
I1006 08:27:15.160456  5661 solver.cpp:376]     Train net output #0: loss = 0.0586741 (* 1 = 0.0586741 loss)
I1006 08:27:15.160471  5661 sgd_solver.cpp:172] Iteration 30200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:28:10.232058  5661 solver.cpp:352] Iteration 30300 (1.81576 iter/s, 55.0734s/100 iter), 163/322.7ep, loss = 0.0530064
I1006 08:28:10.232859  5661 solver.cpp:376]     Train net output #0: loss = 0.0596522 (* 1 = 0.0596522 loss)
I1006 08:28:10.232868  5661 sgd_solver.cpp:172] Iteration 30300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:28:13.025171  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:28:13.083892  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:29:05.244377  5661 solver.cpp:352] Iteration 30400 (1.81774 iter/s, 55.0133s/100 iter), 163.5/322.7ep, loss = 0.0425295
I1006 08:29:05.245170  5661 solver.cpp:376]     Train net output #0: loss = 0.0456201 (* 1 = 0.0456201 loss)
I1006 08:29:05.245179  5661 sgd_solver.cpp:172] Iteration 30400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:29:55.328572  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:29:55.379391  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:30:00.291522  5661 solver.cpp:352] Iteration 30500 (1.81659 iter/s, 55.0481s/100 iter), 164/322.7ep, loss = 0.0527168
I1006 08:30:00.291553  5661 solver.cpp:376]     Train net output #0: loss = 0.0561103 (* 1 = 0.0561103 loss)
I1006 08:30:00.291563  5661 sgd_solver.cpp:172] Iteration 30500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:30:55.299919  5661 solver.cpp:352] Iteration 30600 (1.81787 iter/s, 55.0093s/100 iter), 164.6/322.7ep, loss = 0.0449191
I1006 08:30:55.300727  5661 solver.cpp:376]     Train net output #0: loss = 0.0567217 (* 1 = 0.0567217 loss)
I1006 08:30:55.300736  5661 sgd_solver.cpp:172] Iteration 30600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:31:37.652817  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:31:37.697597  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:31:50.341478  5661 solver.cpp:352] Iteration 30700 (1.81678 iter/s, 55.0425s/100 iter), 165.1/322.7ep, loss = 0.0454442
I1006 08:31:50.341506  5661 solver.cpp:376]     Train net output #0: loss = 0.0396252 (* 1 = 0.0396252 loss)
I1006 08:31:50.341513  5661 sgd_solver.cpp:172] Iteration 30700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:32:45.348924  5661 solver.cpp:352] Iteration 30800 (1.81791 iter/s, 55.0084s/100 iter), 165.6/322.7ep, loss = 0.0495395
I1006 08:32:45.349722  5661 solver.cpp:376]     Train net output #0: loss = 0.0540924 (* 1 = 0.0540924 loss)
I1006 08:32:45.349732  5661 sgd_solver.cpp:172] Iteration 30800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:33:20.041488  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:33:20.063264  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:33:40.376947  5661 solver.cpp:352] Iteration 30900 (1.81723 iter/s, 55.029s/100 iter), 166.2/322.7ep, loss = 0.0421988
I1006 08:33:40.376971  5661 solver.cpp:376]     Train net output #0: loss = 0.0310203 (* 1 = 0.0310203 loss)
I1006 08:33:40.376977  5661 sgd_solver.cpp:172] Iteration 30900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:34:35.388612  5661 solver.cpp:352] Iteration 31000 (1.81777 iter/s, 55.0126s/100 iter), 166.7/322.7ep, loss = 0.0536677
I1006 08:34:35.389422  5661 solver.cpp:376]     Train net output #0: loss = 0.048856 (* 1 = 0.048856 loss)
I1006 08:34:35.389431  5661 sgd_solver.cpp:172] Iteration 31000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:34:35.390702  5661 solver.cpp:389] Sparsity after update:
I1006 08:34:35.392207  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 08:34:35.392215  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 08:34:35.392220  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 08:34:35.392223  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 08:34:35.392226  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 08:34:35.392230  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 08:34:35.392232  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 08:34:35.392235  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 08:34:35.392238  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 08:34:35.392241  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 08:34:35.392244  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 08:34:35.392247  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 08:34:35.392251  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 08:34:35.392253  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 08:34:35.392257  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 08:34:35.392261  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 08:34:35.392263  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 08:34:35.392266  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 08:34:35.392269  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 08:35:02.150326  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:35:02.233533  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:35:30.423295  5661 solver.cpp:352] Iteration 31100 (1.81701 iter/s, 55.0356s/100 iter), 167.3/322.7ep, loss = 0.0580536
I1006 08:35:30.423454  5661 solver.cpp:376]     Train net output #0: loss = 0.0617651 (* 1 = 0.0617651 loss)
I1006 08:35:30.423465  5661 sgd_solver.cpp:172] Iteration 31100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:36:25.435421  5661 solver.cpp:352] Iteration 31200 (1.81775 iter/s, 55.0131s/100 iter), 167.8/322.7ep, loss = 0.0427714
I1006 08:36:25.436233  5661 solver.cpp:376]     Train net output #0: loss = 0.0444631 (* 1 = 0.0444631 loss)
I1006 08:36:25.436244  5661 sgd_solver.cpp:172] Iteration 31200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:36:44.479832  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:36:44.562266  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:36:51.084323  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 08:37:20.480185  5661 solver.cpp:352] Iteration 31300 (1.81667 iter/s, 55.0457s/100 iter), 168.3/322.7ep, loss = 0.0511368
I1006 08:37:20.480979  5661 solver.cpp:376]     Train net output #0: loss = 0.0431768 (* 1 = 0.0431768 loss)
I1006 08:37:20.480988  5661 sgd_solver.cpp:172] Iteration 31300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:38:15.554513  5661 solver.cpp:352] Iteration 31400 (1.8157 iter/s, 55.0753s/100 iter), 168.9/322.7ep, loss = 0.0440224
I1006 08:38:15.555310  5661 solver.cpp:376]     Train net output #0: loss = 0.0314612 (* 1 = 0.0314612 loss)
I1006 08:38:15.555320  5661 sgd_solver.cpp:172] Iteration 31400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:38:26.883476  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:38:26.957476  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:39:10.495527  5661 solver.cpp:352] Iteration 31500 (1.8201 iter/s, 54.9419s/100 iter), 169.4/322.7ep, loss = 0.0393219
I1006 08:39:10.496322  5661 solver.cpp:376]     Train net output #0: loss = 0.0366902 (* 1 = 0.0366902 loss)
I1006 08:39:10.496331  5661 sgd_solver.cpp:172] Iteration 31500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:40:05.360702  5661 solver.cpp:352] Iteration 31600 (1.82262 iter/s, 54.8661s/100 iter), 169.9/322.7ep, loss = 0.0513638
I1006 08:40:05.361517  5661 solver.cpp:376]     Train net output #0: loss = 0.0603378 (* 1 = 0.0603378 loss)
I1006 08:40:05.361526  5661 sgd_solver.cpp:172] Iteration 31600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:40:08.978091  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:40:09.046773  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:41:00.306661  5661 solver.cpp:352] Iteration 31700 (1.81994 iter/s, 54.9468s/100 iter), 170.5/322.7ep, loss = 0.0524593
I1006 08:41:00.307483  5661 solver.cpp:376]     Train net output #0: loss = 0.0414221 (* 1 = 0.0414221 loss)
I1006 08:41:00.307495  5661 sgd_solver.cpp:172] Iteration 31700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:41:51.195761  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:41:51.255102  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:41:55.292528  5661 solver.cpp:352] Iteration 31800 (1.81862 iter/s, 54.9868s/100 iter), 171/322.7ep, loss = 0.0451421
I1006 08:41:55.292552  5661 solver.cpp:376]     Train net output #0: loss = 0.0485428 (* 1 = 0.0485428 loss)
I1006 08:41:55.292560  5661 sgd_solver.cpp:172] Iteration 31800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:42:50.157032  5661 solver.cpp:352] Iteration 31900 (1.82264 iter/s, 54.8654s/100 iter), 171.6/322.7ep, loss = 0.0430451
I1006 08:42:50.157846  5661 solver.cpp:376]     Train net output #0: loss = 0.0404752 (* 1 = 0.0404752 loss)
I1006 08:42:50.157855  5661 sgd_solver.cpp:172] Iteration 31900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:43:33.314616  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:43:33.367985  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:43:44.555121  5661 solver.cpp:538] Iteration 32000, Testing net (#0)
I1006 08:43:53.377144  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:43:53.387290  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:43:53.482594  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.957975
I1006 08:43:53.482622  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 08:43:53.482631  5661 solver.cpp:624]     Test net output #2: loss = 0.156556 (* 1 = 0.156556 loss)
I1006 08:43:53.482650  5661 solver.cpp:283] Tests completed in 63.3267s
I1006 08:43:54.032393  5661 solver.cpp:352] Iteration 32000 (1.57911 iter/s, 63.3267s/100 iter), 172.1/322.7ep, loss = 0.0468298
I1006 08:43:54.032415  5661 solver.cpp:376]     Train net output #0: loss = 0.0459381 (* 1 = 0.0459381 loss)
I1006 08:43:54.032423  5661 sgd_solver.cpp:172] Iteration 32000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:43:54.033679  5661 solver.cpp:389] Sparsity after update:
I1006 08:43:54.034541  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 08:43:54.034549  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 08:43:54.034554  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 08:43:54.034559  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 08:43:54.034561  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 08:43:54.034564  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 08:43:54.034567  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 08:43:54.034569  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 08:43:54.034572  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 08:43:54.034575  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 08:43:54.034579  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 08:43:54.034581  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 08:43:54.034584  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 08:43:54.034587  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 08:43:54.034590  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 08:43:54.034593  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 08:43:54.034596  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 08:43:54.034600  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 08:43:54.034602  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 08:44:48.987321  5661 solver.cpp:352] Iteration 32100 (1.81964 iter/s, 54.9558s/100 iter), 172.6/322.7ep, loss = 0.0469392
I1006 08:44:48.988138  5661 solver.cpp:376]     Train net output #0: loss = 0.0437829 (* 1 = 0.0437829 loss)
I1006 08:44:48.988150  5661 sgd_solver.cpp:172] Iteration 32100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:45:24.406301  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:45:24.452229  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:45:43.930115  5661 solver.cpp:352] Iteration 32200 (1.82004 iter/s, 54.9437s/100 iter), 173.2/322.7ep, loss = 0.0676532
I1006 08:45:43.930140  5661 solver.cpp:376]     Train net output #0: loss = 0.0422203 (* 1 = 0.0422203 loss)
I1006 08:45:43.930145  5661 sgd_solver.cpp:172] Iteration 32200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:46:38.795948  5661 solver.cpp:352] Iteration 32300 (1.8226 iter/s, 54.8667s/100 iter), 173.7/322.7ep, loss = 0.0411392
I1006 08:46:38.796798  5661 solver.cpp:376]     Train net output #0: loss = 0.0407179 (* 1 = 0.0407179 loss)
I1006 08:46:38.796808  5661 sgd_solver.cpp:172] Iteration 32300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:47:06.525806  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:47:06.546815  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:47:33.840993  5661 solver.cpp:352] Iteration 32400 (1.81666 iter/s, 55.046s/100 iter), 174.3/322.7ep, loss = 0.0476853
I1006 08:47:33.841817  5661 solver.cpp:376]     Train net output #0: loss = 0.0481623 (* 1 = 0.0481623 loss)
I1006 08:47:33.841827  5661 sgd_solver.cpp:172] Iteration 32400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:48:28.927467  5661 solver.cpp:352] Iteration 32500 (1.8153 iter/s, 55.0874s/100 iter), 174.8/322.7ep, loss = 0.0373722
I1006 08:48:28.928287  5661 solver.cpp:376]     Train net output #0: loss = 0.0429887 (* 1 = 0.0429887 loss)
I1006 08:48:28.928297  5661 sgd_solver.cpp:172] Iteration 32500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:48:48.826153  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:48:48.916712  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:49:24.065237  5661 solver.cpp:352] Iteration 32600 (1.81361 iter/s, 55.1387s/100 iter), 175.3/322.7ep, loss = 0.0446498
I1006 08:49:24.066059  5661 solver.cpp:376]     Train net output #0: loss = 0.0468821 (* 1 = 0.0468821 loss)
I1006 08:49:24.066068  5661 sgd_solver.cpp:172] Iteration 32600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:50:19.295035  5661 solver.cpp:352] Iteration 32700 (1.81059 iter/s, 55.2307s/100 iter), 175.9/322.7ep, loss = 0.0387837
I1006 08:50:19.295850  5661 solver.cpp:376]     Train net output #0: loss = 0.0440284 (* 1 = 0.0440284 loss)
I1006 08:50:19.295859  5661 sgd_solver.cpp:172] Iteration 32700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:50:31.489604  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:50:31.572018  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:51:14.450202  5661 solver.cpp:352] Iteration 32800 (1.81304 iter/s, 55.1561s/100 iter), 176.4/322.7ep, loss = 0.0451778
I1006 08:51:14.451051  5661 solver.cpp:376]     Train net output #0: loss = 0.0398774 (* 1 = 0.0398774 loss)
I1006 08:51:14.451063  5661 sgd_solver.cpp:172] Iteration 32800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:52:09.509991  5661 solver.cpp:352] Iteration 32900 (1.81618 iter/s, 55.0607s/100 iter), 176.9/322.7ep, loss = 0.0373779
I1006 08:52:09.510159  5661 solver.cpp:376]     Train net output #0: loss = 0.0364555 (* 1 = 0.0364555 loss)
I1006 08:52:09.510167  5661 sgd_solver.cpp:172] Iteration 32900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:52:13.969755  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:52:14.041800  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:53:04.562749  5661 solver.cpp:352] Iteration 33000 (1.81641 iter/s, 55.0537s/100 iter), 177.5/322.7ep, loss = 0.0394952
I1006 08:53:04.562882  5661 solver.cpp:376]     Train net output #0: loss = 0.0372656 (* 1 = 0.0372656 loss)
I1006 08:53:04.562891  5661 sgd_solver.cpp:172] Iteration 33000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:53:04.564142  5661 solver.cpp:389] Sparsity after update:
I1006 08:53:04.566495  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 08:53:04.566504  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 08:53:04.566509  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 08:53:04.566512  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 08:53:04.566516  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 08:53:04.566519  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 08:53:04.566522  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 08:53:04.566525  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 08:53:04.566529  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 08:53:04.566531  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 08:53:04.566534  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 08:53:04.566537  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 08:53:04.566540  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 08:53:04.566543  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 08:53:04.566547  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 08:53:04.566550  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 08:53:04.566553  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 08:53:04.566556  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 08:53:04.566560  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 08:53:56.374430  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:53:56.440337  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:53:59.648419  5661 solver.cpp:352] Iteration 33100 (1.81532 iter/s, 55.0866s/100 iter), 178/322.7ep, loss = 0.0522353
I1006 08:53:59.648447  5661 solver.cpp:376]     Train net output #0: loss = 0.039344 (* 1 = 0.039344 loss)
I1006 08:53:59.648454  5661 sgd_solver.cpp:172] Iteration 33100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:54:54.733932  5661 solver.cpp:352] Iteration 33200 (1.81533 iter/s, 55.0865s/100 iter), 178.6/322.7ep, loss = 0.0370812
I1006 08:54:54.734097  5661 solver.cpp:376]     Train net output #0: loss = 0.0362616 (* 1 = 0.0362616 loss)
I1006 08:54:54.734107  5661 sgd_solver.cpp:172] Iteration 33200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:55:38.788928  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:55:38.851804  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:55:49.746934  5661 solver.cpp:352] Iteration 33300 (1.81772 iter/s, 55.014s/100 iter), 179.1/322.7ep, loss = 0.0473817
I1006 08:55:49.746958  5661 solver.cpp:376]     Train net output #0: loss = 0.040882 (* 1 = 0.040882 loss)
I1006 08:55:49.746965  5661 sgd_solver.cpp:172] Iteration 33300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:56:44.650746  5661 solver.cpp:352] Iteration 33400 (1.82133 iter/s, 54.9048s/100 iter), 179.6/322.7ep, loss = 0.05569
I1006 08:56:44.651588  5661 solver.cpp:376]     Train net output #0: loss = 0.0582929 (* 1 = 0.0582929 loss)
I1006 08:56:44.651597  5661 sgd_solver.cpp:172] Iteration 33400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:57:20.890961  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:57:20.944051  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:57:37.683364  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 08:57:39.613704  5661 solver.cpp:352] Iteration 33500 (1.81937 iter/s, 54.9639s/100 iter), 180.2/322.7ep, loss = 0.0527852
I1006 08:57:39.613731  5661 solver.cpp:376]     Train net output #0: loss = 0.0412179 (* 1 = 0.0412179 loss)
I1006 08:57:39.613739  5661 sgd_solver.cpp:172] Iteration 33500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:58:34.463142  5661 solver.cpp:352] Iteration 33600 (1.82314 iter/s, 54.8504s/100 iter), 180.7/322.7ep, loss = 0.0444272
I1006 08:58:34.463902  5661 solver.cpp:376]     Train net output #0: loss = 0.0438094 (* 1 = 0.0438094 loss)
I1006 08:58:34.463918  5661 sgd_solver.cpp:172] Iteration 33600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 08:59:03.048053  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:59:03.091651  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 08:59:29.431442  5661 solver.cpp:352] Iteration 33700 (1.8192 iter/s, 54.9693s/100 iter), 181.2/322.7ep, loss = 0.0555434
I1006 08:59:29.432261  5661 solver.cpp:376]     Train net output #0: loss = 0.0629054 (* 1 = 0.0629054 loss)
I1006 08:59:29.432271  5661 sgd_solver.cpp:172] Iteration 33700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:00:24.346928  5661 solver.cpp:352] Iteration 33800 (1.82095 iter/s, 54.9165s/100 iter), 181.8/322.7ep, loss = 0.0545028
I1006 09:00:24.347781  5661 solver.cpp:376]     Train net output #0: loss = 0.0547327 (* 1 = 0.0547327 loss)
I1006 09:00:24.347791  5661 sgd_solver.cpp:172] Iteration 33800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:00:45.233916  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:00:45.256880  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:01:19.305768  5661 solver.cpp:352] Iteration 33900 (1.81951 iter/s, 54.9598s/100 iter), 182.3/322.7ep, loss = 0.0504454
I1006 09:01:19.306571  5661 solver.cpp:376]     Train net output #0: loss = 0.050082 (* 1 = 0.050082 loss)
I1006 09:01:19.306579  5661 sgd_solver.cpp:172] Iteration 33900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:02:13.665637  5661 solver.cpp:538] Iteration 34000, Testing net (#0)
I1006 09:02:22.463739  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:02:22.474828  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:02:22.569476  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.958182
I1006 09:02:22.569504  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 09:02:22.569512  5661 solver.cpp:624]     Test net output #2: loss = 0.15775 (* 1 = 0.15775 loss)
I1006 09:02:22.569530  5661 solver.cpp:283] Tests completed in 63.2649s
I1006 09:02:23.118801  5661 solver.cpp:352] Iteration 34000 (1.58066 iter/s, 63.2649s/100 iter), 182.9/322.7ep, loss = 0.0562844
I1006 09:02:23.118822  5661 solver.cpp:376]     Train net output #0: loss = 0.0529648 (* 1 = 0.0529648 loss)
I1006 09:02:23.118830  5661 sgd_solver.cpp:172] Iteration 34000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:02:23.120070  5661 solver.cpp:389] Sparsity after update:
I1006 09:02:23.120923  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 09:02:23.120930  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 09:02:23.120935  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 09:02:23.120939  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 09:02:23.120941  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 09:02:23.120944  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 09:02:23.120947  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 09:02:23.120950  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 09:02:23.120954  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 09:02:23.120957  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 09:02:23.120960  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 09:02:23.120963  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 09:02:23.120966  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 09:02:23.120970  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 09:02:23.120972  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 09:02:23.120975  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 09:02:23.120978  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 09:02:23.120981  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 09:02:23.120985  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 09:02:36.101347  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:02:36.186874  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:03:18.109438  5661 solver.cpp:352] Iteration 34100 (1.81846 iter/s, 54.9916s/100 iter), 183.4/322.7ep, loss = 0.0407935
I1006 09:03:18.110250  5661 solver.cpp:376]     Train net output #0: loss = 0.044583 (* 1 = 0.044583 loss)
I1006 09:03:18.110261  5661 sgd_solver.cpp:172] Iteration 34100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:04:13.083693  5661 solver.cpp:352] Iteration 34200 (1.819 iter/s, 54.9752s/100 iter), 183.9/322.7ep, loss = 0.0399522
I1006 09:04:13.084470  5661 solver.cpp:376]     Train net output #0: loss = 0.0412726 (* 1 = 0.0412726 loss)
I1006 09:04:13.084478  5661 sgd_solver.cpp:172] Iteration 34200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:04:18.358613  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:04:18.439440  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:05:08.059238  5661 solver.cpp:352] Iteration 34300 (1.81896 iter/s, 54.9765s/100 iter), 184.5/322.7ep, loss = 0.0532726
I1006 09:05:08.060111  5661 solver.cpp:376]     Train net output #0: loss = 0.0505638 (* 1 = 0.0505638 loss)
I1006 09:05:08.060120  5661 sgd_solver.cpp:172] Iteration 34300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:06:00.496953  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:06:00.570570  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:06:02.938621  5661 solver.cpp:352] Iteration 34400 (1.82215 iter/s, 54.8803s/100 iter), 185/322.7ep, loss = 0.0391396
I1006 09:06:02.938645  5661 solver.cpp:376]     Train net output #0: loss = 0.0430733 (* 1 = 0.0430733 loss)
I1006 09:06:02.938652  5661 sgd_solver.cpp:172] Iteration 34400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:06:57.834955  5661 solver.cpp:352] Iteration 34500 (1.82158 iter/s, 54.8973s/100 iter), 185.5/322.7ep, loss = 0.0469969
I1006 09:06:57.835775  5661 solver.cpp:376]     Train net output #0: loss = 0.0360071 (* 1 = 0.0360071 loss)
I1006 09:06:57.835784  5661 sgd_solver.cpp:172] Iteration 34500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:07:42.595777  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:07:42.664963  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:07:52.733954  5661 solver.cpp:352] Iteration 34600 (1.8215 iter/s, 54.8999s/100 iter), 186.1/322.7ep, loss = 0.0464393
I1006 09:07:52.733979  5661 solver.cpp:376]     Train net output #0: loss = 0.0345316 (* 1 = 0.0345316 loss)
I1006 09:07:52.733988  5661 sgd_solver.cpp:172] Iteration 34600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:08:47.629660  5661 solver.cpp:352] Iteration 34700 (1.82161 iter/s, 54.8966s/100 iter), 186.6/322.7ep, loss = 0.0537098
I1006 09:08:47.630481  5661 solver.cpp:376]     Train net output #0: loss = 0.0567254 (* 1 = 0.0567254 loss)
I1006 09:08:47.630491  5661 sgd_solver.cpp:172] Iteration 34700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:09:24.764271  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:09:24.823431  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:09:42.575237  5661 solver.cpp:352] Iteration 34800 (1.81995 iter/s, 54.9465s/100 iter), 187.2/322.7ep, loss = 0.0502597
I1006 09:09:42.575260  5661 solver.cpp:376]     Train net output #0: loss = 0.0593524 (* 1 = 0.0593524 loss)
I1006 09:09:42.575268  5661 sgd_solver.cpp:172] Iteration 34800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:10:37.492871  5661 solver.cpp:352] Iteration 34900 (1.82088 iter/s, 54.9185s/100 iter), 187.7/322.7ep, loss = 0.0466994
I1006 09:10:37.493698  5661 solver.cpp:376]     Train net output #0: loss = 0.0521933 (* 1 = 0.0521933 loss)
I1006 09:10:37.493710  5661 sgd_solver.cpp:172] Iteration 34900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:11:06.915122  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:11:06.970921  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:11:32.456147  5661 solver.cpp:352] Iteration 35000 (1.81937 iter/s, 54.9642s/100 iter), 188.2/322.7ep, loss = 0.0412177
I1006 09:11:32.456990  5661 solver.cpp:376]     Train net output #0: loss = 0.0443598 (* 1 = 0.0443598 loss)
I1006 09:11:32.457000  5661 sgd_solver.cpp:172] Iteration 35000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:11:32.458380  5661 solver.cpp:389] Sparsity after update:
I1006 09:11:32.460613  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 09:11:32.460623  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 09:11:32.460629  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 09:11:32.460633  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 09:11:32.460636  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 09:11:32.460640  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 09:11:32.460644  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 09:11:32.460646  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 09:11:32.460650  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 09:11:32.460654  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 09:11:32.460656  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 09:11:32.460660  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 09:11:32.460664  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 09:11:32.460667  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 09:11:32.460670  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 09:11:32.460675  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 09:11:32.460677  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 09:11:32.460681  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 09:11:32.460685  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 09:12:27.368556  5661 solver.cpp:352] Iteration 35100 (1.82105 iter/s, 54.9133s/100 iter), 188.8/322.7ep, loss = 0.0435088
I1006 09:12:27.369335  5661 solver.cpp:376]     Train net output #0: loss = 0.0415493 (* 1 = 0.0415493 loss)
I1006 09:12:27.369345  5661 sgd_solver.cpp:172] Iteration 35100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:12:49.157291  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:12:49.199757  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:13:22.423920  5661 solver.cpp:352] Iteration 35200 (1.81632 iter/s, 55.0563s/100 iter), 189.3/322.7ep, loss = 0.0489846
I1006 09:13:22.424739  5661 solver.cpp:376]     Train net output #0: loss = 0.0479239 (* 1 = 0.0479239 loss)
I1006 09:13:22.424747  5661 sgd_solver.cpp:172] Iteration 35200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:14:17.479405  5661 solver.cpp:352] Iteration 35300 (1.81631 iter/s, 55.0567s/100 iter), 189.8/322.7ep, loss = 0.037596
I1006 09:14:17.480198  5661 solver.cpp:376]     Train net output #0: loss = 0.0372657 (* 1 = 0.0372657 loss)
I1006 09:14:17.480208  5661 sgd_solver.cpp:172] Iteration 35300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:14:31.528165  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:14:31.550388  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:15:12.468274  5661 solver.cpp:352] Iteration 35400 (1.81851 iter/s, 54.99s/100 iter), 190.4/322.7ep, loss = 0.0471411
I1006 09:15:12.469089  5661 solver.cpp:376]     Train net output #0: loss = 0.0479546 (* 1 = 0.0479546 loss)
I1006 09:15:12.469099  5661 sgd_solver.cpp:172] Iteration 35400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:16:07.513036  5661 solver.cpp:352] Iteration 35500 (1.81667 iter/s, 55.0459s/100 iter), 190.9/322.7ep, loss = 0.0383323
I1006 09:16:07.513833  5661 solver.cpp:376]     Train net output #0: loss = 0.0459405 (* 1 = 0.0459405 loss)
I1006 09:16:07.513841  5661 sgd_solver.cpp:172] Iteration 35500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:16:13.683290  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:16:13.769197  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:17:02.581085  5661 solver.cpp:352] Iteration 35600 (1.8159 iter/s, 55.0692s/100 iter), 191.5/322.7ep, loss = 0.0467992
I1006 09:17:02.581892  5661 solver.cpp:376]     Train net output #0: loss = 0.0420314 (* 1 = 0.0420314 loss)
I1006 09:17:02.581900  5661 sgd_solver.cpp:172] Iteration 35600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:17:56.026077  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:17:56.103638  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:17:57.631814  5661 solver.cpp:352] Iteration 35700 (1.81647 iter/s, 55.0518s/100 iter), 192/322.7ep, loss = 0.041351
I1006 09:17:57.631836  5661 solver.cpp:376]     Train net output #0: loss = 0.0336918 (* 1 = 0.0336918 loss)
I1006 09:17:57.631842  5661 sgd_solver.cpp:172] Iteration 35700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:18:23.222717  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 09:18:52.682204  5661 solver.cpp:352] Iteration 35800 (1.81648 iter/s, 55.0515s/100 iter), 192.5/322.7ep, loss = 0.0465254
I1006 09:18:52.683008  5661 solver.cpp:376]     Train net output #0: loss = 0.0474569 (* 1 = 0.0474569 loss)
I1006 09:18:52.683018  5661 sgd_solver.cpp:172] Iteration 35800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:19:38.401684  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:19:38.475219  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:19:47.731585  5661 solver.cpp:352] Iteration 35900 (1.81651 iter/s, 55.0505s/100 iter), 193.1/322.7ep, loss = 0.0460034
I1006 09:19:47.731611  5661 solver.cpp:376]     Train net output #0: loss = 0.0377834 (* 1 = 0.0377834 loss)
I1006 09:19:47.731619  5661 sgd_solver.cpp:172] Iteration 35900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:20:42.201985  5661 solver.cpp:538] Iteration 36000, Testing net (#0)
I1006 09:20:51.032634  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:20:51.045233  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:20:51.139075  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.95765
I1006 09:20:51.139101  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 09:20:51.139111  5661 solver.cpp:624]     Test net output #2: loss = 0.167511 (* 1 = 0.167511 loss)
I1006 09:20:51.139128  5661 solver.cpp:283] Tests completed in 63.4088s
I1006 09:20:51.688379  5661 solver.cpp:352] Iteration 36000 (1.57707 iter/s, 63.4088s/100 iter), 193.6/322.7ep, loss = 0.0409921
I1006 09:20:51.688401  5661 solver.cpp:376]     Train net output #0: loss = 0.0447122 (* 1 = 0.0447122 loss)
I1006 09:20:51.688407  5661 sgd_solver.cpp:172] Iteration 36000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:20:51.689664  5661 solver.cpp:389] Sparsity after update:
I1006 09:20:51.690518  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 09:20:51.690526  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 09:20:51.690531  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 09:20:51.690534  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 09:20:51.690537  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 09:20:51.690541  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 09:20:51.690543  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 09:20:51.690546  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 09:20:51.690549  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 09:20:51.690552  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 09:20:51.690556  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 09:20:51.690558  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 09:20:51.690562  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 09:20:51.690564  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 09:20:51.690567  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 09:20:51.690570  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 09:20:51.690573  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 09:20:51.690577  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 09:20:51.690579  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 09:21:29.655452  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:21:29.732578  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:21:46.645438  5661 solver.cpp:352] Iteration 36100 (1.81957 iter/s, 54.9581s/100 iter), 194.2/322.7ep, loss = 0.0452299
I1006 09:21:46.645462  5661 solver.cpp:376]     Train net output #0: loss = 0.0543654 (* 1 = 0.0543654 loss)
I1006 09:21:46.645469  5661 sgd_solver.cpp:172] Iteration 36100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:22:41.524170  5661 solver.cpp:352] Iteration 36200 (1.82216 iter/s, 54.8798s/100 iter), 194.7/322.7ep, loss = 0.0398929
I1006 09:22:41.524994  5661 solver.cpp:376]     Train net output #0: loss = 0.0350873 (* 1 = 0.0350873 loss)
I1006 09:22:41.525003  5661 sgd_solver.cpp:172] Iteration 36200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:23:11.842710  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:23:11.905794  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:23:36.563098  5661 solver.cpp:352] Iteration 36300 (1.81686 iter/s, 55.04s/100 iter), 195.2/322.7ep, loss = 0.0389655
I1006 09:23:36.563124  5661 solver.cpp:376]     Train net output #0: loss = 0.0410591 (* 1 = 0.0410591 loss)
I1006 09:23:36.563133  5661 sgd_solver.cpp:172] Iteration 36300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:24:31.497243  5661 solver.cpp:352] Iteration 36400 (1.82033 iter/s, 54.9352s/100 iter), 195.8/322.7ep, loss = 0.0534837
I1006 09:24:31.498116  5661 solver.cpp:376]     Train net output #0: loss = 0.062078 (* 1 = 0.062078 loss)
I1006 09:24:31.498126  5661 sgd_solver.cpp:172] Iteration 36400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:24:54.025549  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:24:54.078765  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:25:26.383976  5661 solver.cpp:352] Iteration 36500 (1.8219 iter/s, 54.8878s/100 iter), 196.3/322.7ep, loss = 0.0515787
I1006 09:25:26.384801  5661 solver.cpp:376]     Train net output #0: loss = 0.0441077 (* 1 = 0.0441077 loss)
I1006 09:25:26.384810  5661 sgd_solver.cpp:172] Iteration 36500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:26:21.414186  5661 solver.cpp:352] Iteration 36600 (1.81715 iter/s, 55.0313s/100 iter), 196.8/322.7ep, loss = 0.0482685
I1006 09:26:21.414989  5661 solver.cpp:376]     Train net output #0: loss = 0.0390199 (* 1 = 0.0390199 loss)
I1006 09:26:21.414999  5661 sgd_solver.cpp:172] Iteration 36600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:26:36.265945  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:26:36.311429  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:27:16.350569  5661 solver.cpp:352] Iteration 36700 (1.82025 iter/s, 54.9375s/100 iter), 197.4/322.7ep, loss = 0.0437994
I1006 09:27:16.351370  5661 solver.cpp:376]     Train net output #0: loss = 0.0252406 (* 1 = 0.0252406 loss)
I1006 09:27:16.351379  5661 sgd_solver.cpp:172] Iteration 36700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:28:11.326375  5661 solver.cpp:352] Iteration 36800 (1.81895 iter/s, 54.9769s/100 iter), 197.9/322.7ep, loss = 0.0413326
I1006 09:28:11.327221  5661 solver.cpp:376]     Train net output #0: loss = 0.0473758 (* 1 = 0.0473758 loss)
I1006 09:28:11.327230  5661 sgd_solver.cpp:172] Iteration 36800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:28:18.485260  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:28:18.506278  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:29:06.325192  5661 solver.cpp:352] Iteration 36900 (1.81819 iter/s, 54.9999s/100 iter), 198.5/322.7ep, loss = 0.0464419
I1006 09:29:06.326002  5661 solver.cpp:376]     Train net output #0: loss = 0.04354 (* 1 = 0.04354 loss)
I1006 09:29:06.326010  5661 sgd_solver.cpp:172] Iteration 36900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:30:00.412221  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:30:00.499074  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:30:01.189545  5661 solver.cpp:352] Iteration 37000 (1.82264 iter/s, 54.8654s/100 iter), 199/322.7ep, loss = 0.044047
I1006 09:30:01.189570  5661 solver.cpp:376]     Train net output #0: loss = 0.0320753 (* 1 = 0.0320753 loss)
I1006 09:30:01.189579  5661 sgd_solver.cpp:172] Iteration 37000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:30:01.190865  5661 solver.cpp:389] Sparsity after update:
I1006 09:30:01.191754  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 09:30:01.191762  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 09:30:01.191767  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 09:30:01.191771  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 09:30:01.191774  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 09:30:01.191777  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 09:30:01.191781  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 09:30:01.191784  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 09:30:01.191787  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 09:30:01.191790  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 09:30:01.191793  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 09:30:01.191797  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 09:30:01.191800  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 09:30:01.191803  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 09:30:01.191807  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 09:30:01.191810  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 09:30:01.191813  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 09:30:01.191817  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 09:30:01.191819  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 09:30:56.087040  5661 solver.cpp:352] Iteration 37100 (1.82154 iter/s, 54.8986s/100 iter), 199.5/322.7ep, loss = 0.0443491
I1006 09:30:56.087904  5661 solver.cpp:376]     Train net output #0: loss = 0.050057 (* 1 = 0.050057 loss)
I1006 09:30:56.087914  5661 sgd_solver.cpp:172] Iteration 37100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:31:42.549669  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:31:42.631178  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:31:51.013808  5661 solver.cpp:352] Iteration 37200 (1.82057 iter/s, 54.9278s/100 iter), 200.1/322.7ep, loss = 0.0416658
I1006 09:31:51.013833  5661 solver.cpp:376]     Train net output #0: loss = 0.0347449 (* 1 = 0.0347449 loss)
I1006 09:31:51.013841  5661 sgd_solver.cpp:172] Iteration 37200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:32:45.932875  5661 solver.cpp:352] Iteration 37300 (1.82083 iter/s, 54.9201s/100 iter), 200.6/322.7ep, loss = 0.0480548
I1006 09:32:45.933692  5661 solver.cpp:376]     Train net output #0: loss = 0.0454347 (* 1 = 0.0454347 loss)
I1006 09:32:45.933702  5661 sgd_solver.cpp:172] Iteration 37300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:33:24.645568  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:33:24.722339  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:33:40.797385  5661 solver.cpp:352] Iteration 37400 (1.82264 iter/s, 54.8655s/100 iter), 201.1/322.7ep, loss = 0.0410495
I1006 09:33:40.797410  5661 solver.cpp:376]     Train net output #0: loss = 0.0458289 (* 1 = 0.0458289 loss)
I1006 09:33:40.797416  5661 sgd_solver.cpp:172] Iteration 37400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:34:35.636108  5661 solver.cpp:352] Iteration 37500 (1.8235 iter/s, 54.8397s/100 iter), 201.7/322.7ep, loss = 0.0559958
I1006 09:34:35.636917  5661 solver.cpp:376]     Train net output #0: loss = 0.056472 (* 1 = 0.056472 loss)
I1006 09:34:35.636926  5661 sgd_solver.cpp:172] Iteration 37500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:35:06.777843  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:35:06.843436  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:35:30.666097  5661 solver.cpp:352] Iteration 37600 (1.81716 iter/s, 55.031s/100 iter), 202.2/322.7ep, loss = 0.045158
I1006 09:35:30.666121  5661 solver.cpp:376]     Train net output #0: loss = 0.0394919 (* 1 = 0.0394919 loss)
I1006 09:35:30.666128  5661 sgd_solver.cpp:172] Iteration 37600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:36:25.682039  5661 solver.cpp:352] Iteration 37700 (1.81762 iter/s, 55.0169s/100 iter), 202.8/322.7ep, loss = 0.0541355
I1006 09:36:25.682819  5661 solver.cpp:376]     Train net output #0: loss = 0.0609665 (* 1 = 0.0609665 loss)
I1006 09:36:25.682828  5661 sgd_solver.cpp:172] Iteration 37700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:36:49.136355  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:36:49.195669  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:37:20.717394  5661 solver.cpp:352] Iteration 37800 (1.81698 iter/s, 55.0363s/100 iter), 203.3/322.7ep, loss = 0.0496719
I1006 09:37:20.718197  5661 solver.cpp:376]     Train net output #0: loss = 0.0458631 (* 1 = 0.0458631 loss)
I1006 09:37:20.718209  5661 sgd_solver.cpp:172] Iteration 37800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:38:15.783192  5661 solver.cpp:352] Iteration 37900 (1.81598 iter/s, 55.0668s/100 iter), 203.8/322.7ep, loss = 0.0386498
I1006 09:38:15.784035  5661 solver.cpp:376]     Train net output #0: loss = 0.0372327 (* 1 = 0.0372327 loss)
I1006 09:38:15.784044  5661 sgd_solver.cpp:172] Iteration 37900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:38:31.505086  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:38:31.556818  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:39:08.763072  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 09:39:10.141692  5661 solver.cpp:538] Iteration 38000, Testing net (#0)
I1006 09:39:19.146955  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:39:19.158893  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:39:19.254748  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.958403
I1006 09:39:19.254777  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 09:39:19.254786  5661 solver.cpp:624]     Test net output #2: loss = 0.162556 (* 1 = 0.162556 loss)
I1006 09:39:19.254803  5661 solver.cpp:283] Tests completed in 63.4728s
I1006 09:39:19.803879  5661 solver.cpp:352] Iteration 38000 (1.57548 iter/s, 63.4728s/100 iter), 204.4/322.7ep, loss = 0.0441821
I1006 09:39:19.803901  5661 solver.cpp:376]     Train net output #0: loss = 0.0426269 (* 1 = 0.0426269 loss)
I1006 09:39:19.803907  5661 sgd_solver.cpp:172] Iteration 38000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:39:19.805143  5661 solver.cpp:389] Sparsity after update:
I1006 09:39:19.805969  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 09:39:19.805976  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 09:39:19.805981  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 09:39:19.805984  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 09:39:19.805987  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 09:39:19.805990  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 09:39:19.805994  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 09:39:19.805996  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 09:39:19.805999  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 09:39:19.806002  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 09:39:19.806005  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 09:39:19.806008  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 09:39:19.806011  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 09:39:19.806015  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 09:39:19.806018  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 09:39:19.806020  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 09:39:19.806023  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 09:39:19.806026  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 09:39:19.806030  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 09:40:14.662966  5661 solver.cpp:352] Iteration 38100 (1.82282 iter/s, 54.8601s/100 iter), 204.9/322.7ep, loss = 0.0584247
I1006 09:40:14.663754  5661 solver.cpp:376]     Train net output #0: loss = 0.0705729 (* 1 = 0.0705729 loss)
I1006 09:40:14.663763  5661 sgd_solver.cpp:172] Iteration 38100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:40:22.646669  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:40:22.688916  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:41:09.527622  5661 solver.cpp:352] Iteration 38200 (1.82263 iter/s, 54.8656s/100 iter), 205.4/322.7ep, loss = 0.0470045
I1006 09:41:09.528437  5661 solver.cpp:376]     Train net output #0: loss = 0.0580483 (* 1 = 0.0580483 loss)
I1006 09:41:09.528447  5661 sgd_solver.cpp:172] Iteration 38200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:42:04.379570  5661 solver.cpp:352] Iteration 38300 (1.82306 iter/s, 54.8529s/100 iter), 206/322.7ep, loss = 0.039454
I1006 09:42:04.380401  5661 solver.cpp:376]     Train net output #0: loss = 0.0417488 (* 1 = 0.0417488 loss)
I1006 09:42:04.380412  5661 sgd_solver.cpp:172] Iteration 38300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:42:04.677294  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:42:04.699507  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:42:59.470021  5661 solver.cpp:352] Iteration 38400 (1.81516 iter/s, 55.0915s/100 iter), 206.5/322.7ep, loss = 0.0644095
I1006 09:42:59.470861  5661 solver.cpp:376]     Train net output #0: loss = 0.0551793 (* 1 = 0.0551793 loss)
I1006 09:42:59.470871  5661 sgd_solver.cpp:172] Iteration 38400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:43:46.873584  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:43:46.959817  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:43:54.511068  5661 solver.cpp:352] Iteration 38500 (1.81679 iter/s, 55.0421s/100 iter), 207.1/322.7ep, loss = 0.0602107
I1006 09:43:54.511091  5661 solver.cpp:376]     Train net output #0: loss = 0.0541404 (* 1 = 0.0541404 loss)
I1006 09:43:54.511099  5661 sgd_solver.cpp:172] Iteration 38500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:44:49.541993  5661 solver.cpp:352] Iteration 38600 (1.81713 iter/s, 55.0319s/100 iter), 207.6/322.7ep, loss = 0.0442015
I1006 09:44:49.542801  5661 solver.cpp:376]     Train net output #0: loss = 0.0479987 (* 1 = 0.0479987 loss)
I1006 09:44:49.542811  5661 sgd_solver.cpp:172] Iteration 38600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:45:29.225044  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:45:29.304034  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:45:44.574967  5661 solver.cpp:352] Iteration 38700 (1.81706 iter/s, 55.034s/100 iter), 208.1/322.7ep, loss = 0.0521353
I1006 09:45:44.574996  5661 solver.cpp:376]     Train net output #0: loss = 0.0485104 (* 1 = 0.0485104 loss)
I1006 09:45:44.575004  5661 sgd_solver.cpp:172] Iteration 38700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:46:39.764022  5661 solver.cpp:352] Iteration 38800 (1.81192 iter/s, 55.1901s/100 iter), 208.7/322.7ep, loss = 0.0427569
I1006 09:46:39.764828  5661 solver.cpp:376]     Train net output #0: loss = 0.0382644 (* 1 = 0.0382644 loss)
I1006 09:46:39.764837  5661 sgd_solver.cpp:172] Iteration 38800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:47:11.743099  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:47:11.814956  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:47:34.760524  5661 solver.cpp:352] Iteration 38900 (1.81826 iter/s, 54.9975s/100 iter), 209.2/322.7ep, loss = 0.0536167
I1006 09:47:34.760548  5661 solver.cpp:376]     Train net output #0: loss = 0.0501996 (* 1 = 0.0501996 loss)
I1006 09:47:34.760556  5661 sgd_solver.cpp:172] Iteration 38900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:48:29.935402  5661 solver.cpp:352] Iteration 39000 (1.81239 iter/s, 55.1758s/100 iter), 209.7/322.7ep, loss = 0.0523857
I1006 09:48:29.936211  5661 solver.cpp:376]     Train net output #0: loss = 0.0620608 (* 1 = 0.0620608 loss)
I1006 09:48:29.936221  5661 sgd_solver.cpp:172] Iteration 39000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:48:29.937587  5661 solver.cpp:389] Sparsity after update:
I1006 09:48:29.939882  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 09:48:29.939893  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 09:48:29.939899  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 09:48:29.939903  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 09:48:29.939906  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 09:48:29.939909  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 09:48:29.939913  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 09:48:29.939916  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 09:48:29.939919  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 09:48:29.939923  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 09:48:29.939926  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 09:48:29.939929  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 09:48:29.939934  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 09:48:29.939936  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 09:48:29.939939  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 09:48:29.939944  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 09:48:29.939947  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 09:48:29.939950  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 09:48:29.939954  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 09:48:54.197831  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:48:54.264784  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:49:24.937791  5661 solver.cpp:352] Iteration 39100 (1.81807 iter/s, 55.0033s/100 iter), 210.3/322.7ep, loss = 0.0356482
I1006 09:49:24.938660  5661 solver.cpp:376]     Train net output #0: loss = 0.033207 (* 1 = 0.033207 loss)
I1006 09:49:24.938669  5661 sgd_solver.cpp:172] Iteration 39100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:50:20.102607  5661 solver.cpp:352] Iteration 39200 (1.81272 iter/s, 55.1658s/100 iter), 210.8/322.7ep, loss = 0.0385147
I1006 09:50:20.103417  5661 solver.cpp:376]     Train net output #0: loss = 0.0322902 (* 1 = 0.0322902 loss)
I1006 09:50:20.103427  5661 sgd_solver.cpp:172] Iteration 39200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:50:36.706523  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:50:36.769187  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:51:15.320557  5661 solver.cpp:352] Iteration 39300 (1.81097 iter/s, 55.2189s/100 iter), 211.4/322.7ep, loss = 0.0516063
I1006 09:51:15.321357  5661 solver.cpp:376]     Train net output #0: loss = 0.0503961 (* 1 = 0.0503961 loss)
I1006 09:51:15.321367  5661 sgd_solver.cpp:172] Iteration 39300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:52:10.395773  5661 solver.cpp:352] Iteration 39400 (1.81567 iter/s, 55.0762s/100 iter), 211.9/322.7ep, loss = 0.0445153
I1006 09:52:10.396620  5661 solver.cpp:376]     Train net output #0: loss = 0.0451308 (* 1 = 0.0451308 loss)
I1006 09:52:10.396631  5661 sgd_solver.cpp:172] Iteration 39400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:52:19.212065  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:52:19.265861  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:53:05.444784  5661 solver.cpp:352] Iteration 39500 (1.81653 iter/s, 55.05s/100 iter), 212.4/322.7ep, loss = 0.0462597
I1006 09:53:05.444945  5661 solver.cpp:376]     Train net output #0: loss = 0.0439482 (* 1 = 0.0439482 loss)
I1006 09:53:05.444957  5661 sgd_solver.cpp:172] Iteration 39500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:54:00.509053  5661 solver.cpp:352] Iteration 39600 (1.81603 iter/s, 55.0652s/100 iter), 213/322.7ep, loss = 0.0386536
I1006 09:54:00.509217  5661 solver.cpp:376]     Train net output #0: loss = 0.0367288 (* 1 = 0.0367288 loss)
I1006 09:54:00.509224  5661 sgd_solver.cpp:172] Iteration 39600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:54:01.631171  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:54:01.673261  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:54:55.391798  5661 solver.cpp:352] Iteration 39700 (1.82203 iter/s, 54.8837s/100 iter), 213.5/322.7ep, loss = 0.0503974
I1006 09:54:55.392594  5661 solver.cpp:376]     Train net output #0: loss = 0.0564364 (* 1 = 0.0564364 loss)
I1006 09:54:55.392602  5661 sgd_solver.cpp:172] Iteration 39700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:55:43.759950  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:55:43.787535  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:55:50.359975  5661 solver.cpp:352] Iteration 39800 (1.8192 iter/s, 54.9692s/100 iter), 214.1/322.7ep, loss = 0.0407484
I1006 09:55:50.359999  5661 solver.cpp:376]     Train net output #0: loss = 0.0409293 (* 1 = 0.0409293 loss)
I1006 09:55:50.360008  5661 sgd_solver.cpp:172] Iteration 39800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:56:45.319746  5661 solver.cpp:352] Iteration 39900 (1.81948 iter/s, 54.9608s/100 iter), 214.6/322.7ep, loss = 0.0477055
I1006 09:56:45.320564  5661 solver.cpp:376]     Train net output #0: loss = 0.0538031 (* 1 = 0.0538031 loss)
I1006 09:56:45.320574  5661 sgd_solver.cpp:172] Iteration 39900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:57:25.719775  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:57:25.806766  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:57:39.728729  5661 solver.cpp:905] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_40000.caffemodel
I1006 09:57:39.741560  5661 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_40000.solverstate
I1006 09:57:39.748920  5661 solver.cpp:538] Iteration 40000, Testing net (#0)
I1006 09:57:48.573726  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:57:48.585038  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:57:48.592757  5686 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 09:57:48.682003  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.95772
I1006 09:57:48.682031  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 09:57:48.682041  5661 solver.cpp:624]     Test net output #2: loss = 0.166609 (* 1 = 0.166609 loss)
I1006 09:57:48.682060  5661 solver.cpp:283] Tests completed in 63.3635s
I1006 09:57:49.233552  5661 solver.cpp:352] Iteration 40000 (1.5782 iter/s, 63.3635s/100 iter), 215.1/322.7ep, loss = 0.0485313
I1006 09:57:49.233577  5661 solver.cpp:376]     Train net output #0: loss = 0.0417226 (* 1 = 0.0417226 loss)
I1006 09:57:49.233585  5661 sgd_solver.cpp:172] Iteration 40000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:57:49.234900  5661 solver.cpp:389] Sparsity after update:
I1006 09:57:49.235810  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 09:57:49.235818  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 09:57:49.235823  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 09:57:49.235827  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 09:57:49.235831  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 09:57:49.235834  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 09:57:49.235837  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 09:57:49.235841  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 09:57:49.235844  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 09:57:49.235847  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 09:57:49.235851  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 09:57:49.235854  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 09:57:49.235857  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 09:57:49.235860  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 09:57:49.235864  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 09:57:49.235867  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 09:57:49.235870  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 09:57:49.235874  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 09:57:49.235877  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 09:58:44.140636  5661 solver.cpp:352] Iteration 40100 (1.82123 iter/s, 54.9081s/100 iter), 215.7/322.7ep, loss = 0.0403045
I1006 09:58:44.141463  5661 solver.cpp:376]     Train net output #0: loss = 0.0390836 (* 1 = 0.0390836 loss)
I1006 09:58:44.141474  5661 sgd_solver.cpp:172] Iteration 40100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 09:59:17.006691  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:59:17.087373  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 09:59:39.308593  5661 solver.cpp:352] Iteration 40200 (1.81261 iter/s, 55.1689s/100 iter), 216.2/322.7ep, loss = 0.0559213
I1006 09:59:39.308621  5661 solver.cpp:376]     Train net output #0: loss = 0.07509 (* 1 = 0.07509 loss)
I1006 09:59:39.308630  5661 sgd_solver.cpp:172] Iteration 40200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:00:34.223682  5661 solver.cpp:352] Iteration 40300 (1.82096 iter/s, 54.9161s/100 iter), 216.7/322.7ep, loss = 0.0469087
I1006 10:00:34.224524  5661 solver.cpp:376]     Train net output #0: loss = 0.0613131 (* 1 = 0.0613131 loss)
I1006 10:00:34.224534  5661 sgd_solver.cpp:172] Iteration 40300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:00:59.246129  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:00:59.318461  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:01:29.159168  5661 solver.cpp:352] Iteration 40400 (1.82028 iter/s, 54.9365s/100 iter), 217.3/322.7ep, loss = 0.0409727
I1006 10:01:29.159320  5661 solver.cpp:376]     Train net output #0: loss = 0.0435546 (* 1 = 0.0435546 loss)
I1006 10:01:29.159328  5661 sgd_solver.cpp:172] Iteration 40400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:02:24.096712  5661 solver.cpp:352] Iteration 40500 (1.82022 iter/s, 54.9385s/100 iter), 217.8/322.7ep, loss = 0.0419593
I1006 10:02:24.097515  5661 solver.cpp:376]     Train net output #0: loss = 0.0390273 (* 1 = 0.0390273 loss)
I1006 10:02:24.097523  5661 sgd_solver.cpp:172] Iteration 40500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:02:41.436177  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:02:41.502178  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:03:19.036844  5661 solver.cpp:352] Iteration 40600 (1.82013 iter/s, 54.9411s/100 iter), 218.4/322.7ep, loss = 0.0418429
I1006 10:03:19.037663  5661 solver.cpp:376]     Train net output #0: loss = 0.0352495 (* 1 = 0.0352495 loss)
I1006 10:03:19.037673  5661 sgd_solver.cpp:172] Iteration 40600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:04:13.974604  5661 solver.cpp:352] Iteration 40700 (1.82021 iter/s, 54.9388s/100 iter), 218.9/322.7ep, loss = 0.048668
I1006 10:04:13.975430  5661 solver.cpp:376]     Train net output #0: loss = 0.0546132 (* 1 = 0.0546132 loss)
I1006 10:04:13.975438  5661 sgd_solver.cpp:172] Iteration 40700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:04:23.625423  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:04:23.688599  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:05:08.920799  5661 solver.cpp:352] Iteration 40800 (1.81993 iter/s, 54.9472s/100 iter), 219.4/322.7ep, loss = 0.0411652
I1006 10:05:08.921593  5661 solver.cpp:376]     Train net output #0: loss = 0.0420367 (* 1 = 0.0420367 loss)
I1006 10:05:08.921603  5661 sgd_solver.cpp:172] Iteration 40800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:06:03.810940  5661 solver.cpp:352] Iteration 40900 (1.82179 iter/s, 54.8911s/100 iter), 220/322.7ep, loss = 0.0380115
I1006 10:06:03.811743  5661 solver.cpp:376]     Train net output #0: loss = 0.030037 (* 1 = 0.030037 loss)
I1006 10:06:03.811750  5661 sgd_solver.cpp:172] Iteration 40900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:06:05.769915  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:06:05.822748  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:06:58.827915  5661 solver.cpp:352] Iteration 41000 (1.81759 iter/s, 55.018s/100 iter), 220.5/322.7ep, loss = 0.0386191
I1006 10:06:58.828717  5661 solver.cpp:376]     Train net output #0: loss = 0.0349441 (* 1 = 0.0349441 loss)
I1006 10:06:58.828727  5661 sgd_solver.cpp:172] Iteration 41000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:06:58.829958  5661 solver.cpp:389] Sparsity after update:
I1006 10:06:58.832056  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 10:06:58.832064  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 10:06:58.832069  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 10:06:58.832072  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 10:06:58.832075  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 10:06:58.832078  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 10:06:58.832082  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 10:06:58.832084  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 10:06:58.832087  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 10:06:58.832090  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 10:06:58.832093  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 10:06:58.832096  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 10:06:58.832099  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 10:06:58.832103  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 10:06:58.832106  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 10:06:58.832109  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 10:06:58.832113  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 10:06:58.832115  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 10:06:58.832118  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 10:07:48.138681  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:07:48.184104  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:07:53.908864  5661 solver.cpp:352] Iteration 41100 (1.81548 iter/s, 55.082s/100 iter), 221/322.7ep, loss = 0.0354542
I1006 10:07:53.908888  5661 solver.cpp:376]     Train net output #0: loss = 0.0388028 (* 1 = 0.0388028 loss)
I1006 10:07:53.908895  5661 sgd_solver.cpp:172] Iteration 41100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:08:48.891553  5661 solver.cpp:352] Iteration 41200 (1.81872 iter/s, 54.9837s/100 iter), 221.6/322.7ep, loss = 0.0480286
I1006 10:08:48.892385  5661 solver.cpp:376]     Train net output #0: loss = 0.0503832 (* 1 = 0.0503832 loss)
I1006 10:08:48.892396  5661 sgd_solver.cpp:172] Iteration 41200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:09:30.442674  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:09:30.463754  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:09:43.906179  5661 solver.cpp:352] Iteration 41300 (1.81766 iter/s, 55.0156s/100 iter), 222.1/322.7ep, loss = 0.0431879
I1006 10:09:43.906204  5661 solver.cpp:376]     Train net output #0: loss = 0.0453604 (* 1 = 0.0453604 loss)
I1006 10:09:43.906213  5661 sgd_solver.cpp:172] Iteration 41300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:10:39.019492  5661 solver.cpp:352] Iteration 41400 (1.81441 iter/s, 55.1143s/100 iter), 222.7/322.7ep, loss = 0.0518427
I1006 10:10:39.020303  5661 solver.cpp:376]     Train net output #0: loss = 0.0577658 (* 1 = 0.0577658 loss)
I1006 10:10:39.020313  5661 sgd_solver.cpp:172] Iteration 41400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:11:12.776690  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:11:12.864724  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:11:34.275207  5661 solver.cpp:352] Iteration 41500 (1.80973 iter/s, 55.2567s/100 iter), 223.2/322.7ep, loss = 0.0445514
I1006 10:11:34.275233  5661 solver.cpp:376]     Train net output #0: loss = 0.0472696 (* 1 = 0.0472696 loss)
I1006 10:11:34.275240  5661 sgd_solver.cpp:172] Iteration 41500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:12:29.496592  5661 solver.cpp:352] Iteration 41600 (1.81086 iter/s, 55.2224s/100 iter), 223.7/322.7ep, loss = 0.0460622
I1006 10:12:29.497397  5661 solver.cpp:376]     Train net output #0: loss = 0.047981 (* 1 = 0.047981 loss)
I1006 10:12:29.497406  5661 sgd_solver.cpp:172] Iteration 41600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:12:55.416476  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:12:55.495219  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:13:24.523550  5661 solver.cpp:352] Iteration 41700 (1.81726 iter/s, 55.028s/100 iter), 224.3/322.7ep, loss = 0.0509994
I1006 10:13:24.524353  5661 solver.cpp:376]     Train net output #0: loss = 0.0577373 (* 1 = 0.0577373 loss)
I1006 10:13:24.524360  5661 sgd_solver.cpp:172] Iteration 41700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:14:19.646471  5661 solver.cpp:352] Iteration 41800 (1.81409 iter/s, 55.1239s/100 iter), 224.8/322.7ep, loss = 0.0557127
I1006 10:14:19.647296  5661 solver.cpp:376]     Train net output #0: loss = 0.0682588 (* 1 = 0.0682588 loss)
I1006 10:14:19.647305  5661 sgd_solver.cpp:172] Iteration 41800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:14:37.932639  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:14:38.008402  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:15:14.882261  5661 solver.cpp:352] Iteration 41900 (1.81039 iter/s, 55.2368s/100 iter), 225.3/322.7ep, loss = 0.044525
I1006 10:15:14.883196  5661 solver.cpp:376]     Train net output #0: loss = 0.0347051 (* 1 = 0.0347051 loss)
I1006 10:15:14.883206  5661 sgd_solver.cpp:172] Iteration 41900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:16:09.565342  5661 solver.cpp:538] Iteration 42000, Testing net (#0)
I1006 10:16:18.396545  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:16:18.407843  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:16:18.505219  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.958334
I1006 10:16:18.505249  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 10:16:18.505259  5661 solver.cpp:624]     Test net output #2: loss = 0.159924 (* 1 = 0.159924 loss)
I1006 10:16:18.505278  5661 solver.cpp:283] Tests completed in 63.6242s
I1006 10:16:19.056543  5661 solver.cpp:352] Iteration 42000 (1.57173 iter/s, 63.6242s/100 iter), 225.9/322.7ep, loss = 0.0457977
I1006 10:16:19.056568  5661 solver.cpp:376]     Train net output #0: loss = 0.0465107 (* 1 = 0.0465107 loss)
I1006 10:16:19.056576  5661 sgd_solver.cpp:172] Iteration 42000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:16:19.057904  5661 solver.cpp:389] Sparsity after update:
I1006 10:16:19.058830  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 10:16:19.058837  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 10:16:19.058843  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 10:16:19.058846  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 10:16:19.058851  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 10:16:19.058853  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 10:16:19.058856  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 10:16:19.058861  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 10:16:19.058863  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 10:16:19.058866  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 10:16:19.058869  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 10:16:19.058873  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 10:16:19.058876  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 10:16:19.058881  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 10:16:19.058883  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 10:16:19.058887  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 10:16:19.058890  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 10:16:19.058893  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 10:16:19.058897  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 10:16:29.557991  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:16:29.626742  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:17:14.193368  5661 solver.cpp:352] Iteration 42100 (1.81364 iter/s, 55.1378s/100 iter), 226.4/322.7ep, loss = 0.0411067
I1006 10:17:14.194186  5661 solver.cpp:376]     Train net output #0: loss = 0.0437828 (* 1 = 0.0437828 loss)
I1006 10:17:14.194196  5661 sgd_solver.cpp:172] Iteration 42100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:18:09.277720  5661 solver.cpp:352] Iteration 42200 (1.81536 iter/s, 55.0854s/100 iter), 227/322.7ep, loss = 0.0474213
I1006 10:18:09.278529  5661 solver.cpp:376]     Train net output #0: loss = 0.0521773 (* 1 = 0.0521773 loss)
I1006 10:18:09.278538  5661 sgd_solver.cpp:172] Iteration 42200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:18:12.078886  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:18:12.138602  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:18:34.880610  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 10:19:04.380868  5661 solver.cpp:352] Iteration 42300 (1.81475 iter/s, 55.1041s/100 iter), 227.5/322.7ep, loss = 0.0440874
I1006 10:19:04.381012  5661 solver.cpp:376]     Train net output #0: loss = 0.0506525 (* 1 = 0.0506525 loss)
I1006 10:19:04.381022  5661 sgd_solver.cpp:172] Iteration 42300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:19:54.423779  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:19:54.477005  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:19:59.391829  5661 solver.cpp:352] Iteration 42400 (1.81779 iter/s, 55.012s/100 iter), 228/322.7ep, loss = 0.0477384
I1006 10:19:59.391860  5661 solver.cpp:376]     Train net output #0: loss = 0.0528735 (* 1 = 0.0528735 loss)
I1006 10:19:59.391870  5661 sgd_solver.cpp:172] Iteration 42400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:20:54.542969  5661 solver.cpp:352] Iteration 42500 (1.81317 iter/s, 55.1521s/100 iter), 228.6/322.7ep, loss = 0.0420864
I1006 10:20:54.543805  5661 solver.cpp:376]     Train net output #0: loss = 0.0431834 (* 1 = 0.0431834 loss)
I1006 10:20:54.543817  5661 sgd_solver.cpp:172] Iteration 42500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:21:36.904085  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:21:36.949606  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:21:49.589331  5661 solver.cpp:352] Iteration 42600 (1.81662 iter/s, 55.0474s/100 iter), 229.1/322.7ep, loss = 0.0396846
I1006 10:21:49.589352  5661 solver.cpp:376]     Train net output #0: loss = 0.035496 (* 1 = 0.035496 loss)
I1006 10:21:49.589359  5661 sgd_solver.cpp:172] Iteration 42600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:22:44.721606  5661 solver.cpp:352] Iteration 42700 (1.81378 iter/s, 55.1334s/100 iter), 229.6/322.7ep, loss = 0.0518383
I1006 10:22:44.722431  5661 solver.cpp:376]     Train net output #0: loss = 0.0467136 (* 1 = 0.0467136 loss)
I1006 10:22:44.722443  5661 sgd_solver.cpp:172] Iteration 42700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:23:19.562517  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:23:19.591938  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:23:39.946903  5661 solver.cpp:352] Iteration 42800 (1.81073 iter/s, 55.2264s/100 iter), 230.2/322.7ep, loss = 0.0613149
I1006 10:23:39.946928  5661 solver.cpp:376]     Train net output #0: loss = 0.0442734 (* 1 = 0.0442734 loss)
I1006 10:23:39.946935  5661 sgd_solver.cpp:172] Iteration 42800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:24:34.963336  5661 solver.cpp:352] Iteration 42900 (1.8176 iter/s, 55.0175s/100 iter), 230.7/322.7ep, loss = 0.0599298
I1006 10:24:34.964141  5661 solver.cpp:376]     Train net output #0: loss = 0.0480918 (* 1 = 0.0480918 loss)
I1006 10:24:34.964152  5661 sgd_solver.cpp:172] Iteration 42900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:25:01.739784  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:25:01.828166  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:25:30.060613  5661 solver.cpp:352] Iteration 43000 (1.81493 iter/s, 55.0984s/100 iter), 231.3/322.7ep, loss = 0.0598429
I1006 10:25:30.061411  5661 solver.cpp:376]     Train net output #0: loss = 0.0651003 (* 1 = 0.0651003 loss)
I1006 10:25:30.061421  5661 sgd_solver.cpp:172] Iteration 43000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:25:30.062804  5661 solver.cpp:389] Sparsity after update:
I1006 10:25:30.065135  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 10:25:30.065150  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 10:25:30.065157  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 10:25:30.065163  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 10:25:30.065168  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 10:25:30.065173  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 10:25:30.065179  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 10:25:30.065184  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 10:25:30.065189  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 10:25:30.065194  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 10:25:30.065199  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 10:25:30.065207  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 10:25:30.065214  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 10:25:30.065220  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 10:25:30.065227  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 10:25:30.065232  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 10:25:30.065239  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 10:25:30.065245  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 10:25:30.065250  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 10:26:25.178133  5661 solver.cpp:352] Iteration 43100 (1.81427 iter/s, 55.1186s/100 iter), 231.8/322.7ep, loss = 0.0526497
I1006 10:26:25.178946  5661 solver.cpp:376]     Train net output #0: loss = 0.0594213 (* 1 = 0.0594213 loss)
I1006 10:26:25.178956  5661 sgd_solver.cpp:172] Iteration 43100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:26:44.236451  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:26:44.317965  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:27:20.245132  5661 solver.cpp:352] Iteration 43200 (1.81593 iter/s, 55.0681s/100 iter), 232.3/322.7ep, loss = 0.04133
I1006 10:27:20.245954  5661 solver.cpp:376]     Train net output #0: loss = 0.0385099 (* 1 = 0.0385099 loss)
I1006 10:27:20.245965  5661 sgd_solver.cpp:172] Iteration 43200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:28:15.281536  5661 solver.cpp:352] Iteration 43300 (1.81694 iter/s, 55.0375s/100 iter), 232.9/322.7ep, loss = 0.0446106
I1006 10:28:15.282320  5661 solver.cpp:376]     Train net output #0: loss = 0.0404567 (* 1 = 0.0404567 loss)
I1006 10:28:15.282328  5661 sgd_solver.cpp:172] Iteration 43300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:28:26.672704  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:28:26.745870  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:29:10.365839  5661 solver.cpp:352] Iteration 43400 (1.81536 iter/s, 55.0854s/100 iter), 233.4/322.7ep, loss = 0.0388967
I1006 10:29:10.366647  5661 solver.cpp:376]     Train net output #0: loss = 0.0379026 (* 1 = 0.0379026 loss)
I1006 10:29:10.366657  5661 sgd_solver.cpp:172] Iteration 43400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:30:05.423010  5661 solver.cpp:352] Iteration 43500 (1.81626 iter/s, 55.0583s/100 iter), 233.9/322.7ep, loss = 0.0456629
I1006 10:30:05.423821  5661 solver.cpp:376]     Train net output #0: loss = 0.0511672 (* 1 = 0.0511672 loss)
I1006 10:30:05.423832  5661 sgd_solver.cpp:172] Iteration 43500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:30:09.054283  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:30:09.120659  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:31:00.441656  5661 solver.cpp:352] Iteration 43600 (1.81753 iter/s, 55.0197s/100 iter), 234.5/322.7ep, loss = 0.0441127
I1006 10:31:00.442476  5661 solver.cpp:376]     Train net output #0: loss = 0.0388563 (* 1 = 0.0388563 loss)
I1006 10:31:00.442484  5661 sgd_solver.cpp:172] Iteration 43600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:31:51.420269  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:31:51.480185  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:31:55.555394  5661 solver.cpp:352] Iteration 43700 (1.81439 iter/s, 55.1148s/100 iter), 235/322.7ep, loss = 0.0359266
I1006 10:31:55.555425  5661 solver.cpp:376]     Train net output #0: loss = 0.027598 (* 1 = 0.027598 loss)
I1006 10:31:55.555434  5661 sgd_solver.cpp:172] Iteration 43700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:32:50.522455  5661 solver.cpp:352] Iteration 43800 (1.81924 iter/s, 54.9681s/100 iter), 235.6/322.7ep, loss = 0.0360471
I1006 10:32:50.523248  5661 solver.cpp:376]     Train net output #0: loss = 0.0326282 (* 1 = 0.0326282 loss)
I1006 10:32:50.523257  5661 sgd_solver.cpp:172] Iteration 43800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:33:33.774968  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:33:33.827539  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:33:45.577363  5661 solver.cpp:352] Iteration 43900 (1.81633 iter/s, 55.0559s/100 iter), 236.1/322.7ep, loss = 0.0468112
I1006 10:33:45.577386  5661 solver.cpp:376]     Train net output #0: loss = 0.0471663 (* 1 = 0.0471663 loss)
I1006 10:33:45.577394  5661 sgd_solver.cpp:172] Iteration 43900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:34:40.004936  5661 solver.cpp:538] Iteration 44000, Testing net (#0)
I1006 10:34:48.848744  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:34:48.860061  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:34:48.959362  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.957887
I1006 10:34:48.959394  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 10:34:48.959404  5661 solver.cpp:624]     Test net output #2: loss = 0.163604 (* 1 = 0.163604 loss)
I1006 10:34:48.959425  5661 solver.cpp:283] Tests completed in 63.3833s
I1006 10:34:49.510007  5661 solver.cpp:352] Iteration 44000 (1.5777 iter/s, 63.3833s/100 iter), 236.6/322.7ep, loss = 0.0470587
I1006 10:34:49.510030  5661 solver.cpp:376]     Train net output #0: loss = 0.040529 (* 1 = 0.040529 loss)
I1006 10:34:49.510037  5661 sgd_solver.cpp:172] Iteration 44000, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:34:49.511274  5661 solver.cpp:389] Sparsity after update:
I1006 10:34:49.512123  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 10:34:49.512130  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 10:34:49.512135  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 10:34:49.512138  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 10:34:49.512142  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 10:34:49.512145  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 10:34:49.512147  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 10:34:49.512151  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 10:34:49.512153  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 10:34:49.512156  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 10:34:49.512159  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 10:34:49.512162  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 10:34:49.512166  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 10:34:49.512168  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 10:34:49.512171  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 10:34:49.512174  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 10:34:49.512177  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 10:34:49.512181  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 10:34:49.512183  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 10:35:24.936472  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:35:24.981781  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:35:44.423086  5661 solver.cpp:352] Iteration 44100 (1.82103 iter/s, 54.9141s/100 iter), 237.2/322.7ep, loss = 0.0327171
I1006 10:35:44.423111  5661 solver.cpp:376]     Train net output #0: loss = 0.0336487 (* 1 = 0.0336487 loss)
I1006 10:35:44.423118  5661 sgd_solver.cpp:172] Iteration 44100, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:36:39.462455  5661 solver.cpp:352] Iteration 44200 (1.81685 iter/s, 55.0404s/100 iter), 237.7/322.7ep, loss = 0.0311765
I1006 10:36:39.462620  5661 solver.cpp:376]     Train net output #0: loss = 0.0343805 (* 1 = 0.0343805 loss)
I1006 10:36:39.462630  5661 sgd_solver.cpp:172] Iteration 44200, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:37:07.257860  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:37:07.278776  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:37:34.440335  5661 solver.cpp:352] Iteration 44300 (1.81888 iter/s, 54.9789s/100 iter), 238.3/322.7ep, loss = 0.0555383
I1006 10:37:34.441145  5661 solver.cpp:376]     Train net output #0: loss = 0.0612332 (* 1 = 0.0612332 loss)
I1006 10:37:34.441155  5661 sgd_solver.cpp:172] Iteration 44300, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:38:29.362713  5661 solver.cpp:352] Iteration 44400 (1.82072 iter/s, 54.9234s/100 iter), 238.8/322.7ep, loss = 0.0384882
I1006 10:38:29.363534  5661 solver.cpp:376]     Train net output #0: loss = 0.0445597 (* 1 = 0.0445597 loss)
I1006 10:38:29.363543  5661 sgd_solver.cpp:172] Iteration 44400, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:38:49.268782  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:38:49.355123  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:39:22.441608  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 10:39:24.363112  5661 solver.cpp:352] Iteration 44500 (1.81813 iter/s, 55.0014s/100 iter), 239.3/322.7ep, loss = 0.0435573
I1006 10:39:24.363135  5661 solver.cpp:376]     Train net output #0: loss = 0.0411099 (* 1 = 0.0411099 loss)
I1006 10:39:24.363142  5661 sgd_solver.cpp:172] Iteration 44500, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:40:19.368721  5661 solver.cpp:352] Iteration 44600 (1.81796 iter/s, 55.0066s/100 iter), 239.9/322.7ep, loss = 0.0388256
I1006 10:40:19.369540  5661 solver.cpp:376]     Train net output #0: loss = 0.0435198 (* 1 = 0.0435198 loss)
I1006 10:40:19.369549  5661 sgd_solver.cpp:172] Iteration 44600, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:40:31.508304  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:40:31.589262  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:41:14.347344  5661 solver.cpp:352] Iteration 44700 (1.81885 iter/s, 54.9797s/100 iter), 240.4/322.7ep, loss = 0.0455886
I1006 10:41:14.348173  5661 solver.cpp:376]     Train net output #0: loss = 0.0383168 (* 1 = 0.0383168 loss)
I1006 10:41:14.348184  5661 sgd_solver.cpp:172] Iteration 44700, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:42:09.362216  5661 solver.cpp:352] Iteration 44800 (1.81766 iter/s, 55.0159s/100 iter), 240.9/322.7ep, loss = 0.0382013
I1006 10:42:09.363026  5661 solver.cpp:376]     Train net output #0: loss = 0.0314669 (* 1 = 0.0314669 loss)
I1006 10:42:09.363035  5661 sgd_solver.cpp:172] Iteration 44800, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:42:13.868505  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:42:13.942972  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:43:04.611732  5661 solver.cpp:352] Iteration 44900 (1.80994 iter/s, 55.2506s/100 iter), 241.5/322.7ep, loss = 0.0390901
I1006 10:43:04.612540  5661 solver.cpp:376]     Train net output #0: loss = 0.0367249 (* 1 = 0.0367249 loss)
I1006 10:43:04.612550  5661 sgd_solver.cpp:172] Iteration 44900, lr = 0.001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:43:56.501747  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:43:56.567234  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:43:59.623483  5692 sgd_solver.cpp:50] MultiStep Status: Iteration 45000, step = 2
I1006 10:43:59.790715  5661 solver.cpp:352] Iteration 45000 (1.81225 iter/s, 55.18s/100 iter), 242/322.7ep, loss = 0.042519
I1006 10:43:59.790740  5661 solver.cpp:376]     Train net output #0: loss = 0.0447713 (* 1 = 0.0447713 loss)
I1006 10:43:59.790748  5661 sgd_solver.cpp:172] Iteration 45000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:43:59.792079  5661 solver.cpp:389] Sparsity after update:
I1006 10:43:59.793020  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 10:43:59.793030  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 10:43:59.793035  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 10:43:59.793038  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 10:43:59.793042  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 10:43:59.793045  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 10:43:59.793049  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 10:43:59.793052  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 10:43:59.793056  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 10:43:59.793058  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 10:43:59.793062  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 10:43:59.793066  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 10:43:59.793068  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 10:43:59.793072  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 10:43:59.793076  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 10:43:59.793078  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 10:43:59.793082  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 10:43:59.793085  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 10:43:59.793088  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 10:44:54.792188  5661 solver.cpp:352] Iteration 45100 (1.8181 iter/s, 55.0025s/100 iter), 242.6/322.7ep, loss = 0.047777
I1006 10:44:54.793097  5661 solver.cpp:376]     Train net output #0: loss = 0.0512673 (* 1 = 0.0512673 loss)
I1006 10:44:54.793108  5661 sgd_solver.cpp:172] Iteration 45100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:45:38.777338  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:45:38.838660  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:45:49.776170  5661 solver.cpp:352] Iteration 45200 (1.81868 iter/s, 54.985s/100 iter), 243.1/322.7ep, loss = 0.0529401
I1006 10:45:49.776193  5661 solver.cpp:376]     Train net output #0: loss = 0.0444558 (* 1 = 0.0444558 loss)
I1006 10:45:49.776201  5661 sgd_solver.cpp:172] Iteration 45200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:46:44.821274  5661 solver.cpp:352] Iteration 45300 (1.81666 iter/s, 55.0461s/100 iter), 243.6/322.7ep, loss = 0.0427918
I1006 10:46:44.822096  5661 solver.cpp:376]     Train net output #0: loss = 0.0515044 (* 1 = 0.0515044 loss)
I1006 10:46:44.822105  5661 sgd_solver.cpp:172] Iteration 45300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:47:21.101066  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:47:21.153885  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:47:39.761967  5661 solver.cpp:352] Iteration 45400 (1.82011 iter/s, 54.9417s/100 iter), 244.2/322.7ep, loss = 0.0428819
I1006 10:47:39.761992  5661 solver.cpp:376]     Train net output #0: loss = 0.0420795 (* 1 = 0.0420795 loss)
I1006 10:47:39.761999  5661 sgd_solver.cpp:172] Iteration 45400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:48:34.729431  5661 solver.cpp:352] Iteration 45500 (1.81922 iter/s, 54.9685s/100 iter), 244.7/322.7ep, loss = 0.042271
I1006 10:48:34.730315  5661 solver.cpp:376]     Train net output #0: loss = 0.049573 (* 1 = 0.049573 loss)
I1006 10:48:34.730325  5661 sgd_solver.cpp:172] Iteration 45500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:49:03.370671  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:49:03.416688  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:49:29.766378  5661 solver.cpp:352] Iteration 45600 (1.81693 iter/s, 55.038s/100 iter), 245.2/322.7ep, loss = 0.0523275
I1006 10:49:29.767166  5661 solver.cpp:376]     Train net output #0: loss = 0.0488073 (* 1 = 0.0488073 loss)
I1006 10:49:29.767176  5661 sgd_solver.cpp:172] Iteration 45600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:50:24.723222  5661 solver.cpp:352] Iteration 45700 (1.81958 iter/s, 54.9579s/100 iter), 245.8/322.7ep, loss = 0.042253
I1006 10:50:24.724041  5661 solver.cpp:376]     Train net output #0: loss = 0.0455136 (* 1 = 0.0455136 loss)
I1006 10:50:24.724052  5661 sgd_solver.cpp:172] Iteration 45700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:50:45.670071  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:50:45.691076  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:51:19.744716  5661 solver.cpp:352] Iteration 45800 (1.81744 iter/s, 55.0225s/100 iter), 246.3/322.7ep, loss = 0.040334
I1006 10:51:19.745581  5661 solver.cpp:376]     Train net output #0: loss = 0.0460192 (* 1 = 0.0460192 loss)
I1006 10:51:19.745590  5661 sgd_solver.cpp:172] Iteration 45800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:52:14.704900  5661 solver.cpp:352] Iteration 45900 (1.81947 iter/s, 54.9612s/100 iter), 246.9/322.7ep, loss = 0.045083
I1006 10:52:14.705729  5661 solver.cpp:376]     Train net output #0: loss = 0.034269 (* 1 = 0.034269 loss)
I1006 10:52:14.705737  5661 sgd_solver.cpp:172] Iteration 45900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:52:27.687261  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:52:27.775689  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:53:09.152107  5661 solver.cpp:538] Iteration 46000, Testing net (#0)
I1006 10:53:18.030336  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:53:18.042820  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:53:18.135782  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.95788
I1006 10:53:18.135809  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 10:53:18.135818  5661 solver.cpp:624]     Test net output #2: loss = 0.16618 (* 1 = 0.16618 loss)
I1006 10:53:18.135835  5661 solver.cpp:283] Tests completed in 63.4321s
I1006 10:53:18.686805  5661 solver.cpp:352] Iteration 46000 (1.57649 iter/s, 63.4321s/100 iter), 247.4/322.7ep, loss = 0.0353545
I1006 10:53:18.686830  5661 solver.cpp:376]     Train net output #0: loss = 0.0434334 (* 1 = 0.0434334 loss)
I1006 10:53:18.686837  5661 sgd_solver.cpp:172] Iteration 46000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:53:18.688170  5661 solver.cpp:389] Sparsity after update:
I1006 10:53:18.689115  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 10:53:18.689123  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 10:53:18.689128  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 10:53:18.689132  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 10:53:18.689136  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 10:53:18.689139  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 10:53:18.689142  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 10:53:18.689146  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 10:53:18.689148  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 10:53:18.689152  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 10:53:18.689155  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 10:53:18.689158  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 10:53:18.689162  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 10:53:18.689165  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 10:53:18.689168  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 10:53:18.689172  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 10:53:18.689175  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 10:53:18.689178  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 10:53:18.689182  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 10:54:13.625169  5661 solver.cpp:352] Iteration 46100 (1.82019 iter/s, 54.9393s/100 iter), 247.9/322.7ep, loss = 0.0400962
I1006 10:54:13.625984  5661 solver.cpp:376]     Train net output #0: loss = 0.0343548 (* 1 = 0.0343548 loss)
I1006 10:54:13.625993  5661 sgd_solver.cpp:172] Iteration 46100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:54:18.900712  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:54:18.980976  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:55:08.585418  5661 solver.cpp:352] Iteration 46200 (1.81946 iter/s, 54.9612s/100 iter), 248.5/322.7ep, loss = 0.0488799
I1006 10:55:08.586212  5661 solver.cpp:376]     Train net output #0: loss = 0.0397826 (* 1 = 0.0397826 loss)
I1006 10:55:08.586222  5661 sgd_solver.cpp:172] Iteration 46200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:56:01.124838  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:56:01.205149  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:56:03.564859  5661 solver.cpp:352] Iteration 46300 (1.81883 iter/s, 54.9804s/100 iter), 249/322.7ep, loss = 0.0384391
I1006 10:56:03.564883  5661 solver.cpp:376]     Train net output #0: loss = 0.0423373 (* 1 = 0.0423373 loss)
I1006 10:56:03.564890  5661 sgd_solver.cpp:172] Iteration 46300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:56:58.500471  5661 solver.cpp:352] Iteration 46400 (1.82029 iter/s, 54.9365s/100 iter), 249.5/322.7ep, loss = 0.0451925
I1006 10:56:58.501268  5661 solver.cpp:376]     Train net output #0: loss = 0.0419469 (* 1 = 0.0419469 loss)
I1006 10:56:58.501276  5661 sgd_solver.cpp:172] Iteration 46400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:57:43.364388  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:57:43.432117  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:57:53.501380  5661 solver.cpp:352] Iteration 46500 (1.81812 iter/s, 55.0018s/100 iter), 250.1/322.7ep, loss = 0.0494681
I1006 10:57:53.501404  5661 solver.cpp:376]     Train net output #0: loss = 0.0543596 (* 1 = 0.0543596 loss)
I1006 10:57:53.501410  5661 sgd_solver.cpp:172] Iteration 46500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:58:48.402268  5661 solver.cpp:352] Iteration 46600 (1.82144 iter/s, 54.9017s/100 iter), 250.6/322.7ep, loss = 0.0451593
I1006 10:58:48.403087  5661 solver.cpp:376]     Train net output #0: loss = 0.0514583 (* 1 = 0.0514583 loss)
I1006 10:58:48.403096  5661 sgd_solver.cpp:172] Iteration 46600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 10:59:25.499548  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:59:25.559331  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 10:59:43.326673  5661 solver.cpp:352] Iteration 46700 (1.82066 iter/s, 54.9253s/100 iter), 251.2/322.7ep, loss = 0.0577206
I1006 10:59:43.326696  5661 solver.cpp:376]     Train net output #0: loss = 0.0604137 (* 1 = 0.0604137 loss)
I1006 10:59:43.326702  5661 sgd_solver.cpp:172] Iteration 46700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:00:08.897547  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 11:00:38.277379  5661 solver.cpp:352] Iteration 46800 (1.81978 iter/s, 54.9516s/100 iter), 251.7/322.7ep, loss = 0.0403706
I1006 11:00:38.277406  5661 solver.cpp:376]     Train net output #0: loss = 0.0500282 (* 1 = 0.0500282 loss)
I1006 11:00:38.277415  5661 sgd_solver.cpp:172] Iteration 46800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:01:07.674144  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:01:07.727429  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:01:33.180891  5661 solver.cpp:352] Iteration 46900 (1.82135 iter/s, 54.9044s/100 iter), 252.2/322.7ep, loss = 0.0408784
I1006 11:01:33.180914  5661 solver.cpp:376]     Train net output #0: loss = 0.046936 (* 1 = 0.046936 loss)
I1006 11:01:33.180922  5661 sgd_solver.cpp:172] Iteration 46900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:02:28.113586  5661 solver.cpp:352] Iteration 47000 (1.82038 iter/s, 54.9336s/100 iter), 252.8/322.7ep, loss = 0.0516522
I1006 11:02:28.114418  5661 solver.cpp:376]     Train net output #0: loss = 0.0448842 (* 1 = 0.0448842 loss)
I1006 11:02:28.114428  5661 sgd_solver.cpp:172] Iteration 47000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:02:28.115767  5661 solver.cpp:389] Sparsity after update:
I1006 11:02:28.117672  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 11:02:28.117682  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 11:02:28.117688  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 11:02:28.117691  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 11:02:28.117696  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 11:02:28.117698  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 11:02:28.117702  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 11:02:28.117704  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 11:02:28.117708  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 11:02:28.117712  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 11:02:28.117714  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 11:02:28.117718  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 11:02:28.117722  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 11:02:28.117725  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 11:02:28.117728  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 11:02:28.117732  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 11:02:28.117734  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 11:02:28.117740  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 11:02:28.117744  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 11:02:49.860518  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:02:49.904651  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:03:23.098268  5661 solver.cpp:352] Iteration 47100 (1.81866 iter/s, 54.9856s/100 iter), 253.3/322.7ep, loss = 0.0435174
I1006 11:03:23.099040  5661 solver.cpp:376]     Train net output #0: loss = 0.0452447 (* 1 = 0.0452447 loss)
I1006 11:03:23.099048  5661 sgd_solver.cpp:172] Iteration 47100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:04:17.995546  5661 solver.cpp:352] Iteration 47200 (1.82155 iter/s, 54.8982s/100 iter), 253.8/322.7ep, loss = 0.0414771
I1006 11:04:17.996357  5661 solver.cpp:376]     Train net output #0: loss = 0.0315312 (* 1 = 0.0315312 loss)
I1006 11:04:17.996366  5661 sgd_solver.cpp:172] Iteration 47200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:04:32.029031  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:04:32.051805  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:05:12.968994  5661 solver.cpp:352] Iteration 47300 (1.81903 iter/s, 54.9743s/100 iter), 254.4/322.7ep, loss = 0.0469509
I1006 11:05:12.969825  5661 solver.cpp:376]     Train net output #0: loss = 0.0378063 (* 1 = 0.0378063 loss)
I1006 11:05:12.969835  5661 sgd_solver.cpp:172] Iteration 47300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:06:07.888384  5661 solver.cpp:352] Iteration 47400 (1.82082 iter/s, 54.9203s/100 iter), 254.9/322.7ep, loss = 0.0491334
I1006 11:06:07.889195  5661 solver.cpp:376]     Train net output #0: loss = 0.0623295 (* 1 = 0.0623295 loss)
I1006 11:06:07.889204  5661 sgd_solver.cpp:172] Iteration 47400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:06:14.004609  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:06:14.090518  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:07:02.795425  5661 solver.cpp:352] Iteration 47500 (1.82123 iter/s, 54.9079s/100 iter), 255.5/322.7ep, loss = 0.0414928
I1006 11:07:02.796245  5661 solver.cpp:376]     Train net output #0: loss = 0.0394128 (* 1 = 0.0394128 loss)
I1006 11:07:02.796254  5661 sgd_solver.cpp:172] Iteration 47500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:07:56.172226  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:07:56.254346  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:07:57.790067  5661 solver.cpp:352] Iteration 47600 (1.81833 iter/s, 54.9956s/100 iter), 256/322.7ep, loss = 0.0456377
I1006 11:07:57.790092  5661 solver.cpp:376]     Train net output #0: loss = 0.032311 (* 1 = 0.032311 loss)
I1006 11:07:57.790099  5661 sgd_solver.cpp:172] Iteration 47600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:08:52.664350  5661 solver.cpp:352] Iteration 47700 (1.82232 iter/s, 54.8752s/100 iter), 256.5/322.7ep, loss = 0.051116
I1006 11:08:52.664521  5661 solver.cpp:376]     Train net output #0: loss = 0.0545491 (* 1 = 0.0545491 loss)
I1006 11:08:52.664531  5661 sgd_solver.cpp:172] Iteration 47700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:09:38.321230  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:09:38.394444  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:09:47.629575  5661 solver.cpp:352] Iteration 47800 (1.8193 iter/s, 54.9661s/100 iter), 257.1/322.7ep, loss = 0.0425808
I1006 11:09:47.629598  5661 solver.cpp:376]     Train net output #0: loss = 0.0406038 (* 1 = 0.0406038 loss)
I1006 11:09:47.629606  5661 sgd_solver.cpp:172] Iteration 47800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:10:42.555462  5661 solver.cpp:352] Iteration 47900 (1.8206 iter/s, 54.9268s/100 iter), 257.6/322.7ep, loss = 0.036833
I1006 11:10:42.556298  5661 solver.cpp:376]     Train net output #0: loss = 0.0443781 (* 1 = 0.0443781 loss)
I1006 11:10:42.556306  5661 sgd_solver.cpp:172] Iteration 47900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:11:20.519259  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:11:20.585486  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:11:36.967388  5661 solver.cpp:538] Iteration 48000, Testing net (#0)
I1006 11:11:45.810941  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:11:45.822744  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:11:45.919030  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.957919
I1006 11:11:45.919057  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 11:11:45.919066  5661 solver.cpp:624]     Test net output #2: loss = 0.165423 (* 1 = 0.165423 loss)
I1006 11:11:45.919085  5661 solver.cpp:283] Tests completed in 63.3647s
I1006 11:11:46.467818  5661 solver.cpp:352] Iteration 48000 (1.57817 iter/s, 63.3647s/100 iter), 258.2/322.7ep, loss = 0.0395443
I1006 11:11:46.467840  5661 solver.cpp:376]     Train net output #0: loss = 0.03331 (* 1 = 0.03331 loss)
I1006 11:11:46.467847  5661 sgd_solver.cpp:172] Iteration 48000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:11:46.469106  5661 solver.cpp:389] Sparsity after update:
I1006 11:11:46.469957  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 11:11:46.469965  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 11:11:46.469970  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 11:11:46.469974  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 11:11:46.469976  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 11:11:46.469980  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 11:11:46.469982  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 11:11:46.469985  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 11:11:46.469988  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 11:11:46.469991  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 11:11:46.469995  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 11:11:46.469997  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 11:11:46.470000  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 11:11:46.470002  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 11:11:46.470005  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 11:11:46.470008  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 11:11:46.470011  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 11:11:46.470015  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 11:11:46.470017  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 11:12:41.351804  5661 solver.cpp:352] Iteration 48100 (1.82199 iter/s, 54.8849s/100 iter), 258.7/322.7ep, loss = 0.0515668
I1006 11:12:41.352679  5661 solver.cpp:376]     Train net output #0: loss = 0.0549808 (* 1 = 0.0549808 loss)
I1006 11:12:41.352689  5661 sgd_solver.cpp:172] Iteration 48100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:13:11.592049  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:13:11.651103  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:13:36.267395  5661 solver.cpp:352] Iteration 48200 (1.82095 iter/s, 54.9165s/100 iter), 259.2/322.7ep, loss = 0.0445908
I1006 11:13:36.267418  5661 solver.cpp:376]     Train net output #0: loss = 0.047168 (* 1 = 0.047168 loss)
I1006 11:13:36.267426  5661 sgd_solver.cpp:172] Iteration 48200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:14:31.177892  5661 solver.cpp:352] Iteration 48300 (1.82111 iter/s, 54.9115s/100 iter), 259.8/322.7ep, loss = 0.0473852
I1006 11:14:31.178702  5661 solver.cpp:376]     Train net output #0: loss = 0.0587598 (* 1 = 0.0587598 loss)
I1006 11:14:31.178712  5661 sgd_solver.cpp:172] Iteration 48300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:14:53.718530  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:14:53.774142  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:15:26.088364  5661 solver.cpp:352] Iteration 48400 (1.82111 iter/s, 54.9114s/100 iter), 260.3/322.7ep, loss = 0.0414168
I1006 11:15:26.088523  5661 solver.cpp:376]     Train net output #0: loss = 0.0432439 (* 1 = 0.0432439 loss)
I1006 11:15:26.088533  5661 sgd_solver.cpp:172] Iteration 48400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:16:21.132032  5661 solver.cpp:352] Iteration 48500 (1.81671 iter/s, 55.0447s/100 iter), 260.8/322.7ep, loss = 0.0450613
I1006 11:16:21.132879  5661 solver.cpp:376]     Train net output #0: loss = 0.0371837 (* 1 = 0.0371837 loss)
I1006 11:16:21.132889  5661 sgd_solver.cpp:172] Iteration 48500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:16:35.985599  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:16:36.031303  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:17:16.082414  5661 solver.cpp:352] Iteration 48600 (1.81979 iter/s, 54.9514s/100 iter), 261.4/322.7ep, loss = 0.0419256
I1006 11:17:16.083226  5661 solver.cpp:376]     Train net output #0: loss = 0.0325651 (* 1 = 0.0325651 loss)
I1006 11:17:16.083236  5661 sgd_solver.cpp:172] Iteration 48600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:18:11.107309  5661 solver.cpp:352] Iteration 48700 (1.81733 iter/s, 55.0259s/100 iter), 261.9/322.7ep, loss = 0.042065
I1006 11:18:11.108119  5661 solver.cpp:376]     Train net output #0: loss = 0.0492011 (* 1 = 0.0492011 loss)
I1006 11:18:11.108126  5661 sgd_solver.cpp:172] Iteration 48700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:18:18.304925  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:18:18.327921  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:19:06.105376  5661 solver.cpp:352] Iteration 48800 (1.81821 iter/s, 54.9991s/100 iter), 262.5/322.7ep, loss = 0.0517373
I1006 11:19:06.105543  5661 solver.cpp:376]     Train net output #0: loss = 0.0458496 (* 1 = 0.0458496 loss)
I1006 11:19:06.105556  5661 sgd_solver.cpp:172] Iteration 48800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:20:00.533875  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:20:00.622565  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:20:01.317665  5661 solver.cpp:352] Iteration 48900 (1.81116 iter/s, 55.2133s/100 iter), 263/322.7ep, loss = 0.0498351
I1006 11:20:01.317697  5661 solver.cpp:376]     Train net output #0: loss = 0.0353101 (* 1 = 0.0353101 loss)
I1006 11:20:01.317705  5661 sgd_solver.cpp:172] Iteration 48900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:20:54.368952  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 11:20:56.300336  5661 solver.cpp:352] Iteration 49000 (1.81872 iter/s, 54.9837s/100 iter), 263.5/322.7ep, loss = 0.0399784
I1006 11:20:56.300359  5661 solver.cpp:376]     Train net output #0: loss = 0.0441892 (* 1 = 0.0441892 loss)
I1006 11:20:56.300366  5661 sgd_solver.cpp:172] Iteration 49000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:20:56.301646  5661 solver.cpp:389] Sparsity after update:
I1006 11:20:56.304098  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 11:20:56.304106  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 11:20:56.304111  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 11:20:56.304114  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 11:20:56.304118  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 11:20:56.304121  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 11:20:56.304124  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 11:20:56.304126  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 11:20:56.304131  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 11:20:56.304132  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 11:20:56.304136  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 11:20:56.304138  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 11:20:56.304141  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 11:20:56.304144  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 11:20:56.304147  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 11:20:56.304150  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 11:20:56.304153  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 11:20:56.304157  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 11:20:56.304160  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 11:21:42.826421  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:21:42.907883  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:21:51.312832  5661 solver.cpp:352] Iteration 49100 (1.81774 iter/s, 55.0135s/100 iter), 264.1/322.7ep, loss = 0.0434236
I1006 11:21:51.312858  5661 solver.cpp:376]     Train net output #0: loss = 0.0451308 (* 1 = 0.0451308 loss)
I1006 11:21:51.312866  5661 sgd_solver.cpp:172] Iteration 49100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:22:46.266626  5661 solver.cpp:352] Iteration 49200 (1.81968 iter/s, 54.9548s/100 iter), 264.6/322.7ep, loss = 0.0451778
I1006 11:22:46.267437  5661 solver.cpp:376]     Train net output #0: loss = 0.038937 (* 1 = 0.038937 loss)
I1006 11:22:46.267446  5661 sgd_solver.cpp:172] Iteration 49200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:23:25.122992  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:23:25.206729  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:23:41.407205  5661 solver.cpp:352] Iteration 49300 (1.81351 iter/s, 55.1416s/100 iter), 265.1/322.7ep, loss = 0.0416577
I1006 11:23:41.407230  5661 solver.cpp:376]     Train net output #0: loss = 0.0503338 (* 1 = 0.0503338 loss)
I1006 11:23:41.407238  5661 sgd_solver.cpp:172] Iteration 49300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:24:36.336987  5661 solver.cpp:352] Iteration 49400 (1.82047 iter/s, 54.9308s/100 iter), 265.7/322.7ep, loss = 0.0461171
I1006 11:24:36.337803  5661 solver.cpp:376]     Train net output #0: loss = 0.0336474 (* 1 = 0.0336474 loss)
I1006 11:24:36.337813  5661 sgd_solver.cpp:172] Iteration 49400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:25:07.391389  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:25:07.458274  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:25:31.272125  5661 solver.cpp:352] Iteration 49500 (1.8203 iter/s, 54.9361s/100 iter), 266.2/322.7ep, loss = 0.0479704
I1006 11:25:31.272158  5661 solver.cpp:376]     Train net output #0: loss = 0.0415414 (* 1 = 0.0415414 loss)
I1006 11:25:31.272168  5661 sgd_solver.cpp:172] Iteration 49500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:26:26.245745  5661 solver.cpp:352] Iteration 49600 (1.81902 iter/s, 54.9746s/100 iter), 266.8/322.7ep, loss = 0.0427033
I1006 11:26:26.246574  5661 solver.cpp:376]     Train net output #0: loss = 0.0479899 (* 1 = 0.0479899 loss)
I1006 11:26:26.246582  5661 sgd_solver.cpp:172] Iteration 49600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:26:49.658311  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:26:49.718463  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:27:21.234131  5661 solver.cpp:352] Iteration 49700 (1.81853 iter/s, 54.9894s/100 iter), 267.3/322.7ep, loss = 0.0421172
I1006 11:27:21.234941  5661 solver.cpp:376]     Train net output #0: loss = 0.036979 (* 1 = 0.036979 loss)
I1006 11:27:21.234948  5661 sgd_solver.cpp:172] Iteration 49700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:28:16.167038  5661 solver.cpp:352] Iteration 49800 (1.82037 iter/s, 54.9339s/100 iter), 267.8/322.7ep, loss = 0.0385353
I1006 11:28:16.167848  5661 solver.cpp:376]     Train net output #0: loss = 0.0310064 (* 1 = 0.0310064 loss)
I1006 11:28:16.167858  5661 sgd_solver.cpp:172] Iteration 49800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:28:31.839859  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:28:31.892657  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:29:11.035341  5661 solver.cpp:352] Iteration 49900 (1.82251 iter/s, 54.8693s/100 iter), 268.4/322.7ep, loss = 0.0411537
I1006 11:29:11.036146  5661 solver.cpp:376]     Train net output #0: loss = 0.0411333 (* 1 = 0.0411333 loss)
I1006 11:29:11.036156  5661 sgd_solver.cpp:172] Iteration 49900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:30:05.415354  5661 solver.cpp:905] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_50000.caffemodel
I1006 11:30:05.428493  5661 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_50000.solverstate
I1006 11:30:05.436156  5661 solver.cpp:538] Iteration 50000, Testing net (#0)
I1006 11:30:14.273317  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:30:14.285548  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:30:14.383538  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.957937
I1006 11:30:14.383569  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 11:30:14.383579  5661 solver.cpp:624]     Test net output #2: loss = 0.166447 (* 1 = 0.166447 loss)
I1006 11:30:14.383597  5661 solver.cpp:283] Tests completed in 63.3493s
I1006 11:30:14.935111  5661 solver.cpp:352] Iteration 50000 (1.57855 iter/s, 63.3493s/100 iter), 268.9/322.7ep, loss = 0.0568977
I1006 11:30:14.935133  5661 solver.cpp:376]     Train net output #0: loss = 0.057735 (* 1 = 0.057735 loss)
I1006 11:30:14.935142  5661 sgd_solver.cpp:172] Iteration 50000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:30:14.936437  5661 solver.cpp:389] Sparsity after update:
I1006 11:30:14.937342  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 11:30:14.937350  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 11:30:14.937356  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 11:30:14.937360  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 11:30:14.937363  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 11:30:14.937366  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 11:30:14.937369  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 11:30:14.937373  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 11:30:14.937376  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 11:30:14.937379  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 11:30:14.937382  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 11:30:14.937386  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 11:30:14.937389  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 11:30:14.937392  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 11:30:14.937397  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 11:30:14.937399  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 11:30:14.937402  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 11:30:14.937407  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 11:30:14.937409  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 11:30:22.924666  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:30:22.973176  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:31:09.922847  5661 solver.cpp:352] Iteration 50100 (1.81856 iter/s, 54.9884s/100 iter), 269.4/322.7ep, loss = 0.049795
I1006 11:31:09.923698  5661 solver.cpp:376]     Train net output #0: loss = 0.0460638 (* 1 = 0.0460638 loss)
I1006 11:31:09.923707  5661 sgd_solver.cpp:172] Iteration 50100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:32:04.964896  5661 solver.cpp:352] Iteration 50200 (1.81677 iter/s, 55.0428s/100 iter), 270/322.7ep, loss = 0.0413369
I1006 11:32:04.965065  5661 solver.cpp:376]     Train net output #0: loss = 0.036874 (* 1 = 0.036874 loss)
I1006 11:32:04.965075  5661 sgd_solver.cpp:172] Iteration 50200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:32:05.262351  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:32:05.284287  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:33:00.033244  5661 solver.cpp:352] Iteration 50300 (1.8159 iter/s, 55.0691s/100 iter), 270.5/322.7ep, loss = 0.0610612
I1006 11:33:00.034050  5661 solver.cpp:376]     Train net output #0: loss = 0.052555 (* 1 = 0.052555 loss)
I1006 11:33:00.034060  5661 sgd_solver.cpp:172] Iteration 50300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:33:47.456451  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:33:47.545313  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:33:55.183580  5661 solver.cpp:352] Iteration 50400 (1.8132 iter/s, 55.1511s/100 iter), 271.1/322.7ep, loss = 0.0594916
I1006 11:33:55.183606  5661 solver.cpp:376]     Train net output #0: loss = 0.0610291 (* 1 = 0.0610291 loss)
I1006 11:33:55.183614  5661 sgd_solver.cpp:172] Iteration 50400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:34:50.218945  5661 solver.cpp:352] Iteration 50500 (1.81699 iter/s, 55.0362s/100 iter), 271.6/322.7ep, loss = 0.0649662
I1006 11:34:50.219774  5661 solver.cpp:376]     Train net output #0: loss = 0.048761 (* 1 = 0.048761 loss)
I1006 11:34:50.219785  5661 sgd_solver.cpp:172] Iteration 50500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:35:29.920701  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:35:30.001277  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:35:45.255239  5661 solver.cpp:352] Iteration 50600 (1.81695 iter/s, 55.0371s/100 iter), 272.1/322.7ep, loss = 0.0477012
I1006 11:35:45.255264  5661 solver.cpp:376]     Train net output #0: loss = 0.0383709 (* 1 = 0.0383709 loss)
I1006 11:35:45.255272  5661 sgd_solver.cpp:172] Iteration 50600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:36:40.131229  5661 solver.cpp:352] Iteration 50700 (1.82226 iter/s, 54.8768s/100 iter), 272.7/322.7ep, loss = 0.0439787
I1006 11:36:40.131399  5661 solver.cpp:376]     Train net output #0: loss = 0.0328028 (* 1 = 0.0328028 loss)
I1006 11:36:40.131407  5661 sgd_solver.cpp:172] Iteration 50700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:37:12.051267  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:37:12.130683  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:37:35.095260  5661 solver.cpp:352] Iteration 50800 (1.81934 iter/s, 54.9649s/100 iter), 273.2/322.7ep, loss = 0.048411
I1006 11:37:35.095283  5661 solver.cpp:376]     Train net output #0: loss = 0.0523022 (* 1 = 0.0523022 loss)
I1006 11:37:35.095289  5661 sgd_solver.cpp:172] Iteration 50800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:38:30.041708  5661 solver.cpp:352] Iteration 50900 (1.81993 iter/s, 54.9473s/100 iter), 273.7/322.7ep, loss = 0.0496275
I1006 11:38:30.042780  5661 solver.cpp:376]     Train net output #0: loss = 0.0514873 (* 1 = 0.0514873 loss)
I1006 11:38:30.042789  5661 sgd_solver.cpp:172] Iteration 50900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:38:54.297051  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:38:54.363281  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:39:25.058727  5661 solver.cpp:352] Iteration 51000 (1.81759 iter/s, 55.0179s/100 iter), 274.3/322.7ep, loss = 0.0480499
I1006 11:39:25.059550  5661 solver.cpp:376]     Train net output #0: loss = 0.0368155 (* 1 = 0.0368155 loss)
I1006 11:39:25.059561  5661 sgd_solver.cpp:172] Iteration 51000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:39:25.060981  5661 solver.cpp:389] Sparsity after update:
I1006 11:39:25.063129  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 11:39:25.063139  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 11:39:25.063145  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 11:39:25.063149  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 11:39:25.063153  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 11:39:25.063158  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 11:39:25.063160  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 11:39:25.063164  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 11:39:25.063169  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 11:39:25.063171  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 11:39:25.063175  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 11:39:25.063179  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 11:39:25.063184  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 11:39:25.063186  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 11:39:25.063190  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 11:39:25.063194  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 11:39:25.063197  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 11:39:25.063201  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 11:39:25.063208  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 11:40:19.953076  5661 solver.cpp:352] Iteration 51100 (1.82165 iter/s, 54.8952s/100 iter), 274.8/322.7ep, loss = 0.0415947
I1006 11:40:19.953936  5661 solver.cpp:376]     Train net output #0: loss = 0.0469106 (* 1 = 0.0469106 loss)
I1006 11:40:19.953948  5661 sgd_solver.cpp:172] Iteration 51100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:40:36.532945  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:40:36.592573  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:41:15.087301  5661 solver.cpp:352] Iteration 51200 (1.81373 iter/s, 55.1351s/100 iter), 275.4/322.7ep, loss = 0.0547492
I1006 11:41:15.088112  5661 solver.cpp:376]     Train net output #0: loss = 0.0670789 (* 1 = 0.0670789 loss)
I1006 11:41:15.088122  5661 sgd_solver.cpp:172] Iteration 51200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:41:40.767246  5681 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 11:42:10.279613  5661 solver.cpp:352] Iteration 51300 (1.81182 iter/s, 55.1932s/100 iter), 275.9/322.7ep, loss = 0.0549116
I1006 11:42:10.280434  5661 solver.cpp:376]     Train net output #0: loss = 0.045334 (* 1 = 0.045334 loss)
I1006 11:42:10.280443  5661 sgd_solver.cpp:172] Iteration 51300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:42:19.148578  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:42:19.205390  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:43:05.314604  5661 solver.cpp:352] Iteration 51400 (1.817 iter/s, 55.0359s/100 iter), 276.4/322.7ep, loss = 0.0438766
I1006 11:43:05.315413  5661 solver.cpp:376]     Train net output #0: loss = 0.040816 (* 1 = 0.040816 loss)
I1006 11:43:05.315421  5661 sgd_solver.cpp:172] Iteration 51400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:44:00.441496  5661 solver.cpp:352] Iteration 51500 (1.81397 iter/s, 55.1278s/100 iter), 277/322.7ep, loss = 0.0362256
I1006 11:44:00.442333  5661 solver.cpp:376]     Train net output #0: loss = 0.0407897 (* 1 = 0.0407897 loss)
I1006 11:44:00.442345  5661 sgd_solver.cpp:172] Iteration 51500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:44:01.572767  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:44:01.622151  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:44:55.464728  5661 solver.cpp:352] Iteration 51600 (1.81739 iter/s, 55.0241s/100 iter), 277.5/322.7ep, loss = 0.0344902
I1006 11:44:55.465541  5661 solver.cpp:376]     Train net output #0: loss = 0.0374489 (* 1 = 0.0374489 loss)
I1006 11:44:55.465553  5661 sgd_solver.cpp:172] Iteration 51600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:45:43.981369  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:45:44.004315  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:45:50.613342  5661 solver.cpp:352] Iteration 51700 (1.81325 iter/s, 55.1495s/100 iter), 278.1/322.7ep, loss = 0.0459835
I1006 11:45:50.613369  5661 solver.cpp:376]     Train net output #0: loss = 0.0344838 (* 1 = 0.0344838 loss)
I1006 11:45:50.613379  5661 sgd_solver.cpp:172] Iteration 51700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:46:45.664480  5661 solver.cpp:352] Iteration 51800 (1.81646 iter/s, 55.052s/100 iter), 278.6/322.7ep, loss = 0.0505332
I1006 11:46:45.665297  5661 solver.cpp:376]     Train net output #0: loss = 0.0553731 (* 1 = 0.0553731 loss)
I1006 11:46:45.665305  5661 sgd_solver.cpp:172] Iteration 51800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:47:26.179242  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:47:26.264792  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:47:40.699687  5661 solver.cpp:352] Iteration 51900 (1.81699 iter/s, 55.0361s/100 iter), 279.1/322.7ep, loss = 0.0481154
I1006 11:47:40.699712  5661 solver.cpp:376]     Train net output #0: loss = 0.039256 (* 1 = 0.039256 loss)
I1006 11:47:40.699719  5661 sgd_solver.cpp:172] Iteration 51900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:48:35.158308  5661 solver.cpp:538] Iteration 52000, Testing net (#0)
I1006 11:48:44.003960  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:48:44.015188  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:48:44.113234  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.958105
I1006 11:48:44.113263  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 11:48:44.113273  5661 solver.cpp:624]     Test net output #2: loss = 0.166605 (* 1 = 0.166605 loss)
I1006 11:48:44.113293  5661 solver.cpp:283] Tests completed in 63.4146s
I1006 11:48:44.665158  5661 solver.cpp:352] Iteration 52000 (1.57692 iter/s, 63.4146s/100 iter), 279.7/322.7ep, loss = 0.0469086
I1006 11:48:44.665181  5661 solver.cpp:376]     Train net output #0: loss = 0.0438683 (* 1 = 0.0438683 loss)
I1006 11:48:44.665190  5661 sgd_solver.cpp:172] Iteration 52000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:48:44.666520  5661 solver.cpp:389] Sparsity after update:
I1006 11:48:44.667448  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 11:48:44.667455  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 11:48:44.667460  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 11:48:44.667464  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 11:48:44.667467  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 11:48:44.667471  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 11:48:44.667474  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 11:48:44.667477  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 11:48:44.667481  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 11:48:44.667484  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 11:48:44.667487  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 11:48:44.667491  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 11:48:44.667495  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 11:48:44.667498  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 11:48:44.667501  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 11:48:44.667507  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 11:48:44.667511  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 11:48:44.667516  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 11:48:44.667520  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 11:49:17.488477  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:49:17.568661  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:49:39.805481  5661 solver.cpp:352] Iteration 52100 (1.81353 iter/s, 55.1412s/100 iter), 280.2/322.7ep, loss = 0.0521809
I1006 11:49:39.805506  5661 solver.cpp:376]     Train net output #0: loss = 0.0652078 (* 1 = 0.0652078 loss)
I1006 11:49:39.805514  5661 sgd_solver.cpp:172] Iteration 52100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:50:34.953480  5661 solver.cpp:352] Iteration 52200 (1.81327 iter/s, 55.1489s/100 iter), 280.7/322.7ep, loss = 0.0430048
I1006 11:50:34.954277  5661 solver.cpp:376]     Train net output #0: loss = 0.0451988 (* 1 = 0.0451988 loss)
I1006 11:50:34.954285  5661 sgd_solver.cpp:172] Iteration 52200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:51:00.026155  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:51:00.098877  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:51:30.036243  5661 solver.cpp:352] Iteration 52300 (1.81542 iter/s, 55.0837s/100 iter), 281.3/322.7ep, loss = 0.0383082
I1006 11:51:30.036388  5661 solver.cpp:376]     Train net output #0: loss = 0.05329 (* 1 = 0.05329 loss)
I1006 11:51:30.036397  5661 sgd_solver.cpp:172] Iteration 52300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:52:25.088043  5661 solver.cpp:352] Iteration 52400 (1.81644 iter/s, 55.0527s/100 iter), 281.8/322.7ep, loss = 0.032148
I1006 11:52:25.088814  5661 solver.cpp:376]     Train net output #0: loss = 0.0290188 (* 1 = 0.0290188 loss)
I1006 11:52:25.088824  5661 sgd_solver.cpp:172] Iteration 52400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:52:42.486865  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:52:42.553439  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:53:20.119863  5661 solver.cpp:352] Iteration 52500 (1.8171 iter/s, 55.0327s/100 iter), 282.4/322.7ep, loss = 0.0422555
I1006 11:53:20.120741  5661 solver.cpp:376]     Train net output #0: loss = 0.0373709 (* 1 = 0.0373709 loss)
I1006 11:53:20.120749  5661 sgd_solver.cpp:172] Iteration 52500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:54:15.047562  5661 solver.cpp:352] Iteration 52600 (1.82054 iter/s, 54.9286s/100 iter), 282.9/322.7ep, loss = 0.0444186
I1006 11:54:15.048447  5661 solver.cpp:376]     Train net output #0: loss = 0.0451958 (* 1 = 0.0451958 loss)
I1006 11:54:15.048457  5661 sgd_solver.cpp:172] Iteration 52600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:54:24.696130  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:54:24.755085  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:55:09.990737  5661 solver.cpp:352] Iteration 52700 (1.82003 iter/s, 54.9441s/100 iter), 283.4/322.7ep, loss = 0.0330185
I1006 11:55:09.991544  5661 solver.cpp:376]     Train net output #0: loss = 0.0286096 (* 1 = 0.0286096 loss)
I1006 11:55:09.991554  5661 sgd_solver.cpp:172] Iteration 52700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:56:04.881902  5661 solver.cpp:352] Iteration 52800 (1.82176 iter/s, 54.8921s/100 iter), 284/322.7ep, loss = 0.0457297
I1006 11:56:04.882709  5661 solver.cpp:376]     Train net output #0: loss = 0.0399331 (* 1 = 0.0399331 loss)
I1006 11:56:04.882717  5661 sgd_solver.cpp:172] Iteration 52800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:56:06.839637  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:56:06.892288  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:56:59.767750  5661 solver.cpp:352] Iteration 52900 (1.82193 iter/s, 54.8868s/100 iter), 284.5/322.7ep, loss = 0.0380847
I1006 11:56:59.768564  5661 solver.cpp:376]     Train net output #0: loss = 0.0300225 (* 1 = 0.0300225 loss)
I1006 11:56:59.768574  5661 sgd_solver.cpp:172] Iteration 52900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:57:48.921386  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:57:48.964169  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:57:54.733089  5661 solver.cpp:352] Iteration 53000 (1.8193 iter/s, 54.9663s/100 iter), 285/322.7ep, loss = 0.0392881
I1006 11:57:54.733114  5661 solver.cpp:376]     Train net output #0: loss = 0.0416576 (* 1 = 0.0416576 loss)
I1006 11:57:54.733122  5661 sgd_solver.cpp:172] Iteration 53000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:57:54.734486  5661 solver.cpp:389] Sparsity after update:
I1006 11:57:54.736798  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 11:57:54.736806  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 11:57:54.736814  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 11:57:54.736817  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 11:57:54.736820  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 11:57:54.736824  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 11:57:54.736827  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 11:57:54.736831  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 11:57:54.736835  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 11:57:54.736838  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 11:57:54.736841  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 11:57:54.736845  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 11:57:54.736850  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 11:57:54.736852  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 11:57:54.736856  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 11:57:54.736860  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 11:57:54.736863  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 11:57:54.736866  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 11:57:54.736871  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 11:58:49.656232  5661 solver.cpp:352] Iteration 53100 (1.82069 iter/s, 54.9241s/100 iter), 285.6/322.7ep, loss = 0.0408471
I1006 11:58:49.657083  5661 solver.cpp:376]     Train net output #0: loss = 0.0320587 (* 1 = 0.0320587 loss)
I1006 11:58:49.657090  5661 sgd_solver.cpp:172] Iteration 53100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 11:59:31.111042  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:59:31.131937  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 11:59:44.566306  5661 solver.cpp:352] Iteration 53200 (1.82113 iter/s, 54.911s/100 iter), 286.1/322.7ep, loss = 0.0466146
I1006 11:59:44.566329  5661 solver.cpp:376]     Train net output #0: loss = 0.0526519 (* 1 = 0.0526519 loss)
I1006 11:59:44.566337  5661 sgd_solver.cpp:172] Iteration 53200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:00:39.443442  5661 solver.cpp:352] Iteration 53300 (1.82222 iter/s, 54.8781s/100 iter), 286.7/322.7ep, loss = 0.0447929
I1006 12:00:39.444264  5661 solver.cpp:376]     Train net output #0: loss = 0.0396396 (* 1 = 0.0396396 loss)
I1006 12:00:39.444274  5661 sgd_solver.cpp:172] Iteration 53300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:01:13.109206  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:01:13.186574  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:01:34.506417  5661 solver.cpp:352] Iteration 53400 (1.81607 iter/s, 55.0639s/100 iter), 287.2/322.7ep, loss = 0.039062
I1006 12:01:34.506443  5661 solver.cpp:376]     Train net output #0: loss = 0.0377514 (* 1 = 0.0377514 loss)
I1006 12:01:34.506453  5661 sgd_solver.cpp:172] Iteration 53400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:02:27.629237  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 12:02:29.568965  5661 solver.cpp:352] Iteration 53500 (1.81609 iter/s, 55.0635s/100 iter), 287.7/322.7ep, loss = 0.0435224
I1006 12:02:29.568989  5661 solver.cpp:376]     Train net output #0: loss = 0.045304 (* 1 = 0.045304 loss)
I1006 12:02:29.568996  5661 sgd_solver.cpp:172] Iteration 53500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:02:55.522058  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:02:55.591620  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:03:24.652215  5661 solver.cpp:352] Iteration 53600 (1.8154 iter/s, 55.0842s/100 iter), 288.3/322.7ep, loss = 0.0424907
I1006 12:03:24.653009  5661 solver.cpp:376]     Train net output #0: loss = 0.047844 (* 1 = 0.047844 loss)
I1006 12:03:24.653020  5661 sgd_solver.cpp:172] Iteration 53600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:04:19.675374  5661 solver.cpp:352] Iteration 53700 (1.81738 iter/s, 55.0242s/100 iter), 288.8/322.7ep, loss = 0.0459162
I1006 12:04:19.676149  5661 solver.cpp:376]     Train net output #0: loss = 0.0518952 (* 1 = 0.0518952 loss)
I1006 12:04:19.676157  5661 sgd_solver.cpp:172] Iteration 53700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:04:37.924304  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:04:38.000334  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:05:14.731331  5661 solver.cpp:352] Iteration 53800 (1.81629 iter/s, 55.0573s/100 iter), 289.3/322.7ep, loss = 0.0478969
I1006 12:05:14.732192  5661 solver.cpp:376]     Train net output #0: loss = 0.038943 (* 1 = 0.038943 loss)
I1006 12:05:14.732206  5661 sgd_solver.cpp:172] Iteration 53800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:06:09.840939  5661 solver.cpp:352] Iteration 53900 (1.81452 iter/s, 55.1109s/100 iter), 289.9/322.7ep, loss = 0.0629401
I1006 12:06:09.841122  5661 solver.cpp:376]     Train net output #0: loss = 0.083015 (* 1 = 0.083015 loss)
I1006 12:06:09.841131  5661 sgd_solver.cpp:172] Iteration 53900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:06:20.378069  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:06:20.445987  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:07:04.366747  5661 solver.cpp:538] Iteration 54000, Testing net (#0)
I1006 12:07:13.224453  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:07:13.235642  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:07:13.332360  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.958185
I1006 12:07:13.332388  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 12:07:13.332397  5661 solver.cpp:624]     Test net output #2: loss = 0.165843 (* 1 = 0.165843 loss)
I1006 12:07:13.332415  5661 solver.cpp:283] Tests completed in 63.4929s
I1006 12:07:13.883183  5661 solver.cpp:352] Iteration 54000 (1.57498 iter/s, 63.4929s/100 iter), 290.4/322.7ep, loss = 0.0318721
I1006 12:07:13.883208  5661 solver.cpp:376]     Train net output #0: loss = 0.0352631 (* 1 = 0.0352631 loss)
I1006 12:07:13.883215  5661 sgd_solver.cpp:172] Iteration 54000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:07:13.884572  5661 solver.cpp:389] Sparsity after update:
I1006 12:07:13.885526  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 12:07:13.885535  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 12:07:13.885541  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 12:07:13.885545  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 12:07:13.885550  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 12:07:13.885552  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 12:07:13.885555  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 12:07:13.885560  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 12:07:13.885562  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 12:07:13.885565  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 12:07:13.885570  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 12:07:13.885572  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 12:07:13.885576  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 12:07:13.885581  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 12:07:13.885589  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 12:07:13.885596  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 12:07:13.885602  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 12:07:13.885608  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 12:07:13.885614  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 12:08:08.811185  5661 solver.cpp:352] Iteration 54100 (1.82053 iter/s, 54.9292s/100 iter), 291/322.7ep, loss = 0.0476048
I1006 12:08:08.812043  5661 solver.cpp:376]     Train net output #0: loss = 0.0504135 (* 1 = 0.0504135 loss)
I1006 12:08:08.812052  5661 sgd_solver.cpp:172] Iteration 54100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:08:11.610242  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:08:11.669713  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:09:03.831935  5661 solver.cpp:352] Iteration 54200 (1.81746 iter/s, 55.0219s/100 iter), 291.5/322.7ep, loss = 0.0442508
I1006 12:09:03.832772  5661 solver.cpp:376]     Train net output #0: loss = 0.0514318 (* 1 = 0.0514318 loss)
I1006 12:09:03.832779  5661 sgd_solver.cpp:172] Iteration 54200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:09:53.957193  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:09:54.009001  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:09:58.947033  5661 solver.cpp:352] Iteration 54300 (1.81435 iter/s, 55.1163s/100 iter), 292/322.7ep, loss = 0.0523849
I1006 12:09:58.947063  5661 solver.cpp:376]     Train net output #0: loss = 0.0554078 (* 1 = 0.0554078 loss)
I1006 12:09:58.947072  5661 sgd_solver.cpp:172] Iteration 54300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:10:53.984869  5661 solver.cpp:352] Iteration 54400 (1.81689 iter/s, 55.039s/100 iter), 292.6/322.7ep, loss = 0.0373367
I1006 12:10:53.985689  5661 solver.cpp:376]     Train net output #0: loss = 0.0409107 (* 1 = 0.0409107 loss)
I1006 12:10:53.985699  5661 sgd_solver.cpp:172] Iteration 54400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:11:36.285259  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:11:36.327455  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:11:48.919823  5661 solver.cpp:352] Iteration 54500 (1.8203 iter/s, 54.9361s/100 iter), 293.1/322.7ep, loss = 0.0371735
I1006 12:11:48.919845  5661 solver.cpp:376]     Train net output #0: loss = 0.0349112 (* 1 = 0.0349112 loss)
I1006 12:11:48.919853  5661 sgd_solver.cpp:172] Iteration 54500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:12:43.859549  5661 solver.cpp:352] Iteration 54600 (1.82014 iter/s, 54.9409s/100 iter), 293.6/322.7ep, loss = 0.0447278
I1006 12:12:43.860421  5661 solver.cpp:376]     Train net output #0: loss = 0.0381653 (* 1 = 0.0381653 loss)
I1006 12:12:43.860431  5661 sgd_solver.cpp:172] Iteration 54600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:13:18.474267  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:13:18.501715  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:13:38.786303  5661 solver.cpp:352] Iteration 54700 (1.82057 iter/s, 54.9279s/100 iter), 294.2/322.7ep, loss = 0.0458354
I1006 12:13:38.786326  5661 solver.cpp:376]     Train net output #0: loss = 0.0336754 (* 1 = 0.0336754 loss)
I1006 12:13:38.786334  5661 sgd_solver.cpp:172] Iteration 54700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:14:33.726641  5661 solver.cpp:352] Iteration 54800 (1.82012 iter/s, 54.9414s/100 iter), 294.7/322.7ep, loss = 0.0508753
I1006 12:14:33.727461  5661 solver.cpp:376]     Train net output #0: loss = 0.04208 (* 1 = 0.04208 loss)
I1006 12:14:33.727470  5661 sgd_solver.cpp:172] Iteration 54800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:15:00.481292  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:15:00.568162  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:15:28.734802  5661 solver.cpp:352] Iteration 54900 (1.81788 iter/s, 55.0092s/100 iter), 295.3/322.7ep, loss = 0.0498785
I1006 12:15:28.735620  5661 solver.cpp:376]     Train net output #0: loss = 0.0524456 (* 1 = 0.0524456 loss)
I1006 12:15:28.735632  5661 sgd_solver.cpp:172] Iteration 54900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:16:23.734766  5661 solver.cpp:352] Iteration 55000 (1.81815 iter/s, 55.001s/100 iter), 295.8/322.7ep, loss = 0.0408791
I1006 12:16:23.735563  5661 solver.cpp:376]     Train net output #0: loss = 0.0451643 (* 1 = 0.0451643 loss)
I1006 12:16:23.735574  5661 sgd_solver.cpp:172] Iteration 55000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:16:23.737108  5661 solver.cpp:389] Sparsity after update:
I1006 12:16:23.739106  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 12:16:23.739116  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 12:16:23.739123  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 12:16:23.739127  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 12:16:23.739131  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 12:16:23.739136  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 12:16:23.739140  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 12:16:23.739145  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 12:16:23.739148  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 12:16:23.739152  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 12:16:23.739156  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 12:16:23.739161  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 12:16:23.739164  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 12:16:23.739168  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 12:16:23.739172  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 12:16:23.739176  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 12:16:23.739181  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 12:16:23.739184  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 12:16:23.739189  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 12:16:42.781281  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:16:42.860895  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:17:18.752641  5661 solver.cpp:352] Iteration 55100 (1.81756 iter/s, 55.0189s/100 iter), 296.3/322.7ep, loss = 0.0416909
I1006 12:17:18.753655  5661 solver.cpp:376]     Train net output #0: loss = 0.0351355 (* 1 = 0.0351355 loss)
I1006 12:17:18.753664  5661 sgd_solver.cpp:172] Iteration 55100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:18:13.650051  5661 solver.cpp:352] Iteration 55200 (1.82155 iter/s, 54.8985s/100 iter), 296.9/322.7ep, loss = 0.0418658
I1006 12:18:13.650888  5661 solver.cpp:376]     Train net output #0: loss = 0.0289717 (* 1 = 0.0289717 loss)
I1006 12:18:13.650898  5661 sgd_solver.cpp:172] Iteration 55200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:18:24.957687  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:18:25.030382  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:19:08.610409  5661 solver.cpp:352] Iteration 55300 (1.81946 iter/s, 54.9614s/100 iter), 297.4/322.7ep, loss = 0.0441193
I1006 12:19:08.611219  5661 solver.cpp:376]     Train net output #0: loss = 0.047575 (* 1 = 0.047575 loss)
I1006 12:19:08.611229  5661 sgd_solver.cpp:172] Iteration 55300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:20:03.567880  5661 solver.cpp:352] Iteration 55400 (1.81955 iter/s, 54.9585s/100 iter), 297.9/322.7ep, loss = 0.0401155
I1006 12:20:03.568029  5661 solver.cpp:376]     Train net output #0: loss = 0.0391061 (* 1 = 0.0391061 loss)
I1006 12:20:03.568037  5661 sgd_solver.cpp:172] Iteration 55400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:20:07.195377  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:20:07.261602  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:20:58.511451  5661 solver.cpp:352] Iteration 55500 (1.82002 iter/s, 54.9446s/100 iter), 298.5/322.7ep, loss = 0.0449968
I1006 12:20:58.511591  5661 solver.cpp:376]     Train net output #0: loss = 0.0380398 (* 1 = 0.0380398 loss)
I1006 12:20:58.511600  5661 sgd_solver.cpp:172] Iteration 55500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:21:49.483157  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:21:49.545285  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:21:53.589431  5661 solver.cpp:352] Iteration 55600 (1.81557 iter/s, 55.079s/100 iter), 299/322.7ep, loss = 0.0469859
I1006 12:21:53.589453  5661 solver.cpp:376]     Train net output #0: loss = 0.0447128 (* 1 = 0.0447128 loss)
I1006 12:21:53.589460  5661 sgd_solver.cpp:172] Iteration 55600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:22:48.664348  5661 solver.cpp:352] Iteration 55700 (1.81568 iter/s, 55.0759s/100 iter), 299.6/322.7ep, loss = 0.0342878
I1006 12:22:48.664510  5661 solver.cpp:376]     Train net output #0: loss = 0.0308439 (* 1 = 0.0308439 loss)
I1006 12:22:48.664525  5661 sgd_solver.cpp:172] Iteration 55700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:23:14.278970  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 12:23:31.929491  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:23:31.980890  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:23:43.753883  5661 solver.cpp:352] Iteration 55800 (1.81519 iter/s, 55.0905s/100 iter), 300.1/322.7ep, loss = 0.0458281
I1006 12:23:43.753909  5661 solver.cpp:376]     Train net output #0: loss = 0.0445704 (* 1 = 0.0445704 loss)
I1006 12:23:43.753917  5661 sgd_solver.cpp:172] Iteration 55800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:24:38.773536  5661 solver.cpp:352] Iteration 55900 (1.8175 iter/s, 55.0207s/100 iter), 300.6/322.7ep, loss = 0.0439283
I1006 12:24:38.774421  5661 solver.cpp:376]     Train net output #0: loss = 0.0466064 (* 1 = 0.0466064 loss)
I1006 12:24:38.774432  5661 sgd_solver.cpp:172] Iteration 55900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:25:14.309162  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:25:14.358232  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:25:33.291702  5661 solver.cpp:538] Iteration 56000, Testing net (#0)
I1006 12:25:42.141439  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:25:42.152603  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:25:42.249579  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.95812
I1006 12:25:42.249605  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 12:25:42.249614  5661 solver.cpp:624]     Test net output #2: loss = 0.165317 (* 1 = 0.165317 loss)
I1006 12:25:42.249634  5661 solver.cpp:283] Tests completed in 63.4773s
I1006 12:25:42.798416  5661 solver.cpp:352] Iteration 56000 (1.57537 iter/s, 63.4773s/100 iter), 301.2/322.7ep, loss = 0.0382838
I1006 12:25:42.798439  5661 solver.cpp:376]     Train net output #0: loss = 0.0367305 (* 1 = 0.0367305 loss)
I1006 12:25:42.798445  5661 sgd_solver.cpp:172] Iteration 56000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:25:42.799680  5661 solver.cpp:389] Sparsity after update:
I1006 12:25:42.800535  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 12:25:42.800542  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 12:25:42.800547  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 12:25:42.800550  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 12:25:42.800554  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 12:25:42.800557  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 12:25:42.800559  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 12:25:42.800562  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 12:25:42.800565  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 12:25:42.800568  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 12:25:42.800571  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 12:25:42.800575  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 12:25:42.800577  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 12:25:42.800580  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 12:25:42.800582  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 12:25:42.800585  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 12:25:42.800588  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 12:25:42.800591  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 12:25:42.800595  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 12:26:37.782321  5661 solver.cpp:352] Iteration 56100 (1.81868 iter/s, 54.9849s/100 iter), 301.7/322.7ep, loss = 0.0331112
I1006 12:26:37.783182  5661 solver.cpp:376]     Train net output #0: loss = 0.0352402 (* 1 = 0.0352402 loss)
I1006 12:26:37.783191  5661 sgd_solver.cpp:172] Iteration 56100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:27:05.577718  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:27:05.598798  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:27:32.807401  5661 solver.cpp:352] Iteration 56200 (1.81732 iter/s, 55.0261s/100 iter), 302.3/322.7ep, loss = 0.0505626
I1006 12:27:32.808218  5661 solver.cpp:376]     Train net output #0: loss = 0.0566977 (* 1 = 0.0566977 loss)
I1006 12:27:32.808228  5661 sgd_solver.cpp:172] Iteration 56200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:28:27.899000  5661 solver.cpp:352] Iteration 56300 (1.81513 iter/s, 55.0926s/100 iter), 302.8/322.7ep, loss = 0.0402312
I1006 12:28:27.899814  5661 solver.cpp:376]     Train net output #0: loss = 0.0503371 (* 1 = 0.0503371 loss)
I1006 12:28:27.899825  5661 sgd_solver.cpp:172] Iteration 56300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:28:47.799257  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:28:47.888340  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:29:23.075717  5661 solver.cpp:352] Iteration 56400 (1.81233 iter/s, 55.1777s/100 iter), 303.3/322.7ep, loss = 0.05375
I1006 12:29:23.076544  5661 solver.cpp:376]     Train net output #0: loss = 0.0506975 (* 1 = 0.0506975 loss)
I1006 12:29:23.076553  5661 sgd_solver.cpp:172] Iteration 56400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:30:18.208847  5661 solver.cpp:352] Iteration 56500 (1.81376 iter/s, 55.1341s/100 iter), 303.9/322.7ep, loss = 0.0353088
I1006 12:30:18.209007  5661 solver.cpp:376]     Train net output #0: loss = 0.0316742 (* 1 = 0.0316742 loss)
I1006 12:30:18.209015  5661 sgd_solver.cpp:172] Iteration 56500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:30:30.350028  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:30:30.429126  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:31:13.309552  5661 solver.cpp:352] Iteration 56600 (1.81483 iter/s, 55.1017s/100 iter), 304.4/322.7ep, loss = 0.044533
I1006 12:31:13.310413  5661 solver.cpp:376]     Train net output #0: loss = 0.0352452 (* 1 = 0.0352452 loss)
I1006 12:31:13.310422  5661 sgd_solver.cpp:172] Iteration 56600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:32:08.378862  5661 solver.cpp:352] Iteration 56700 (1.81586 iter/s, 55.0703s/100 iter), 304.9/322.7ep, loss = 0.0370826
I1006 12:32:08.379020  5661 solver.cpp:376]     Train net output #0: loss = 0.0338678 (* 1 = 0.0338678 loss)
I1006 12:32:08.379030  5661 sgd_solver.cpp:172] Iteration 56700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:32:12.846236  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:32:12.919508  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:33:03.446653  5661 solver.cpp:352] Iteration 56800 (1.81591 iter/s, 55.0688s/100 iter), 305.5/322.7ep, loss = 0.038803
I1006 12:33:03.447429  5661 solver.cpp:376]     Train net output #0: loss = 0.0374627 (* 1 = 0.0374627 loss)
I1006 12:33:03.447439  5661 sgd_solver.cpp:172] Iteration 56800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:33:55.096688  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:33:55.164732  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:33:58.405128  5661 solver.cpp:352] Iteration 56900 (1.81952 iter/s, 54.9594s/100 iter), 306/322.7ep, loss = 0.0481239
I1006 12:33:58.405158  5661 solver.cpp:376]     Train net output #0: loss = 0.0459665 (* 1 = 0.0459665 loss)
I1006 12:33:58.405166  5661 sgd_solver.cpp:172] Iteration 56900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:34:53.363962  5661 solver.cpp:352] Iteration 57000 (1.81951 iter/s, 54.9598s/100 iter), 306.6/322.7ep, loss = 0.0453472
I1006 12:34:53.364744  5661 solver.cpp:376]     Train net output #0: loss = 0.0442356 (* 1 = 0.0442356 loss)
I1006 12:34:53.364753  5661 sgd_solver.cpp:172] Iteration 57000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:34:53.365989  5661 solver.cpp:389] Sparsity after update:
I1006 12:34:53.368073  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 12:34:53.368083  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 12:34:53.368088  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 12:34:53.368090  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 12:34:53.368093  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 12:34:53.368096  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 12:34:53.368099  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 12:34:53.368103  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 12:34:53.368105  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 12:34:53.368108  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 12:34:53.368110  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 12:34:53.368113  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 12:34:53.368116  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 12:34:53.368119  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 12:34:53.368122  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 12:34:53.368125  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 12:34:53.368129  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 12:34:53.368131  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 12:34:53.368134  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 12:35:37.388860  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:35:37.447890  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:35:48.414047  5661 solver.cpp:352] Iteration 57100 (1.8165 iter/s, 55.051s/100 iter), 307.1/322.7ep, loss = 0.0413551
I1006 12:35:48.414069  5661 solver.cpp:376]     Train net output #0: loss = 0.038264 (* 1 = 0.038264 loss)
I1006 12:35:48.414077  5661 sgd_solver.cpp:172] Iteration 57100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:36:43.507103  5661 solver.cpp:352] Iteration 57200 (1.81508 iter/s, 55.094s/100 iter), 307.6/322.7ep, loss = 0.0458018
I1006 12:36:43.507958  5661 solver.cpp:376]     Train net output #0: loss = 0.0429069 (* 1 = 0.0429069 loss)
I1006 12:36:43.507968  5661 sgd_solver.cpp:172] Iteration 57200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:37:19.769295  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:37:19.822840  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:37:38.509608  5661 solver.cpp:352] Iteration 57300 (1.81807 iter/s, 55.0035s/100 iter), 308.2/322.7ep, loss = 0.0474382
I1006 12:37:38.509639  5661 solver.cpp:376]     Train net output #0: loss = 0.0492777 (* 1 = 0.0492777 loss)
I1006 12:37:38.509649  5661 sgd_solver.cpp:172] Iteration 57300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:38:33.438297  5661 solver.cpp:352] Iteration 57400 (1.82051 iter/s, 54.9296s/100 iter), 308.7/322.7ep, loss = 0.040676
I1006 12:38:33.439119  5661 solver.cpp:376]     Train net output #0: loss = 0.0377647 (* 1 = 0.0377647 loss)
I1006 12:38:33.439128  5661 sgd_solver.cpp:172] Iteration 57400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:39:02.052552  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:39:02.100901  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:39:28.436199  5661 solver.cpp:352] Iteration 57500 (1.81822 iter/s, 54.9988s/100 iter), 309.2/322.7ep, loss = 0.0579995
I1006 12:39:28.437075  5661 solver.cpp:376]     Train net output #0: loss = 0.0540214 (* 1 = 0.0540214 loss)
I1006 12:39:28.437084  5661 sgd_solver.cpp:172] Iteration 57500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:40:23.345149  5661 solver.cpp:352] Iteration 57600 (1.82117 iter/s, 54.9098s/100 iter), 309.8/322.7ep, loss = 0.0489793
I1006 12:40:23.345963  5661 solver.cpp:376]     Train net output #0: loss = 0.0550864 (* 1 = 0.0550864 loss)
I1006 12:40:23.345973  5661 sgd_solver.cpp:172] Iteration 57600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:40:44.225390  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:40:44.247841  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:41:18.301801  5661 solver.cpp:352] Iteration 57700 (1.81959 iter/s, 54.9576s/100 iter), 310.3/322.7ep, loss = 0.0398719
I1006 12:41:18.302604  5661 solver.cpp:376]     Train net output #0: loss = 0.0442167 (* 1 = 0.0442167 loss)
I1006 12:41:18.302614  5661 sgd_solver.cpp:172] Iteration 57700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:42:13.195140  5661 solver.cpp:352] Iteration 57800 (1.82168 iter/s, 54.8942s/100 iter), 310.9/322.7ep, loss = 0.052162
I1006 12:42:13.195960  5661 solver.cpp:376]     Train net output #0: loss = 0.043597 (* 1 = 0.043597 loss)
I1006 12:42:13.195969  5661 sgd_solver.cpp:172] Iteration 57800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:42:26.163769  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:42:26.251010  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:43:08.112169  5661 solver.cpp:352] Iteration 57900 (1.8209 iter/s, 54.9179s/100 iter), 311.4/322.7ep, loss = 0.0505763
I1006 12:43:08.113001  5661 solver.cpp:376]     Train net output #0: loss = 0.0501188 (* 1 = 0.0501188 loss)
I1006 12:43:08.113011  5661 sgd_solver.cpp:172] Iteration 57900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:44:01.177409  5680 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 12:44:02.553576  5661 solver.cpp:538] Iteration 58000, Testing net (#0)
I1006 12:44:11.382908  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:44:11.394027  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:44:11.490716  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.958079
I1006 12:44:11.490743  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 12:44:11.490752  5661 solver.cpp:624]     Test net output #2: loss = 0.165466 (* 1 = 0.165466 loss)
I1006 12:44:11.490770  5661 solver.cpp:283] Tests completed in 63.3797s
I1006 12:44:12.041121  5661 solver.cpp:352] Iteration 58000 (1.57779 iter/s, 63.3797s/100 iter), 311.9/322.7ep, loss = 0.0563187
I1006 12:44:12.041146  5661 solver.cpp:376]     Train net output #0: loss = 0.0543983 (* 1 = 0.0543983 loss)
I1006 12:44:12.041152  5661 sgd_solver.cpp:172] Iteration 58000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:44:12.042443  5661 solver.cpp:389] Sparsity after update:
I1006 12:44:12.043340  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 12:44:12.043349  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 12:44:12.043354  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 12:44:12.043357  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 12:44:12.043360  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 12:44:12.043365  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 12:44:12.043367  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 12:44:12.043370  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 12:44:12.043373  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 12:44:12.043376  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 12:44:12.043380  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 12:44:12.043383  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 12:44:12.043386  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 12:44:12.043390  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 12:44:12.043393  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 12:44:12.043396  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 12:44:12.043400  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 12:44:12.043402  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 12:44:12.043406  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 12:44:17.336836  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:44:17.414999  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:45:07.016665  5661 solver.cpp:352] Iteration 58100 (1.81896 iter/s, 54.9765s/100 iter), 312.5/322.7ep, loss = 0.0468536
I1006 12:45:07.017522  5661 solver.cpp:376]     Train net output #0: loss = 0.0469898 (* 1 = 0.0469898 loss)
I1006 12:45:07.017531  5661 sgd_solver.cpp:172] Iteration 58100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:45:59.559514  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:45:59.635037  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:46:02.020498  5661 solver.cpp:352] Iteration 58200 (1.81802 iter/s, 55.0048s/100 iter), 313/322.7ep, loss = 0.0402846
I1006 12:46:02.020530  5661 solver.cpp:376]     Train net output #0: loss = 0.0409814 (* 1 = 0.0409814 loss)
I1006 12:46:02.020539  5661 sgd_solver.cpp:172] Iteration 58200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:46:56.954972  5661 solver.cpp:352] Iteration 58300 (1.82032 iter/s, 54.9354s/100 iter), 313.5/322.7ep, loss = 0.0475796
I1006 12:46:56.955770  5661 solver.cpp:376]     Train net output #0: loss = 0.0345343 (* 1 = 0.0345343 loss)
I1006 12:46:56.955782  5661 sgd_solver.cpp:172] Iteration 58300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:47:41.837597  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:47:41.907546  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:47:52.003319  5661 solver.cpp:352] Iteration 58400 (1.81655 iter/s, 55.0493s/100 iter), 314.1/322.7ep, loss = 0.0466896
I1006 12:47:52.003341  5661 solver.cpp:376]     Train net output #0: loss = 0.0450391 (* 1 = 0.0450391 loss)
I1006 12:47:52.003348  5661 sgd_solver.cpp:172] Iteration 58400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:48:47.015879  5661 solver.cpp:352] Iteration 58500 (1.81773 iter/s, 55.0135s/100 iter), 314.6/322.7ep, loss = 0.043576
I1006 12:48:47.015992  5661 solver.cpp:376]     Train net output #0: loss = 0.0532033 (* 1 = 0.0532033 loss)
I1006 12:48:47.016001  5661 sgd_solver.cpp:172] Iteration 58500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:49:24.193853  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:49:24.253713  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:49:42.023286  5661 solver.cpp:352] Iteration 58600 (1.8179 iter/s, 55.0084s/100 iter), 315.2/322.7ep, loss = 0.0545079
I1006 12:49:42.023309  5661 solver.cpp:376]     Train net output #0: loss = 0.0611543 (* 1 = 0.0611543 loss)
I1006 12:49:42.023315  5661 sgd_solver.cpp:172] Iteration 58600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:50:37.029616  5661 solver.cpp:352] Iteration 58700 (1.81794 iter/s, 55.0073s/100 iter), 315.7/322.7ep, loss = 0.0412672
I1006 12:50:37.030480  5661 solver.cpp:376]     Train net output #0: loss = 0.0525215 (* 1 = 0.0525215 loss)
I1006 12:50:37.030489  5661 sgd_solver.cpp:172] Iteration 58700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:51:06.497629  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:51:06.550621  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:51:32.008631  5661 solver.cpp:352] Iteration 58800 (1.81884 iter/s, 54.98s/100 iter), 316.2/322.7ep, loss = 0.0439319
I1006 12:51:32.008781  5661 solver.cpp:376]     Train net output #0: loss = 0.0486527 (* 1 = 0.0486527 loss)
I1006 12:51:32.008790  5661 sgd_solver.cpp:172] Iteration 58800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:52:26.969509  5661 solver.cpp:352] Iteration 58900 (1.81944 iter/s, 54.9619s/100 iter), 316.8/322.7ep, loss = 0.0486436
I1006 12:52:26.970386  5661 solver.cpp:376]     Train net output #0: loss = 0.0469285 (* 1 = 0.0469285 loss)
I1006 12:52:26.970396  5661 sgd_solver.cpp:172] Iteration 58900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:52:48.691141  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:52:48.739506  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:53:22.052379  5661 solver.cpp:352] Iteration 59000 (1.81541 iter/s, 55.0838s/100 iter), 317.3/322.7ep, loss = 0.0458449
I1006 12:53:22.053172  5661 solver.cpp:376]     Train net output #0: loss = 0.0362929 (* 1 = 0.0362929 loss)
I1006 12:53:22.053182  5661 sgd_solver.cpp:172] Iteration 59000, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:53:22.054524  5661 solver.cpp:389] Sparsity after update:
I1006 12:53:22.056833  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 12:53:22.056843  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 12:53:22.056849  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 12:53:22.056852  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 12:53:22.056855  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 12:53:22.056859  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 12:53:22.056862  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 12:53:22.056866  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 12:53:22.056869  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 12:53:22.056872  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 12:53:22.056876  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 12:53:22.056879  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 12:53:22.056883  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 12:53:22.056886  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 12:53:22.056890  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 12:53:22.056893  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 12:53:22.056897  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 12:53:22.056901  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 12:53:22.056905  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 12:54:17.077673  5661 solver.cpp:352] Iteration 59100 (1.81731 iter/s, 55.0263s/100 iter), 317.8/322.7ep, loss = 0.0372761
I1006 12:54:17.078478  5661 solver.cpp:376]     Train net output #0: loss = 0.0281799 (* 1 = 0.0281799 loss)
I1006 12:54:17.078487  5661 sgd_solver.cpp:172] Iteration 59100, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:54:31.104357  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:54:31.136010  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:55:12.037122  5661 solver.cpp:352] Iteration 59200 (1.81949 iter/s, 54.9604s/100 iter), 318.4/322.7ep, loss = 0.0416913
I1006 12:55:12.037986  5661 solver.cpp:376]     Train net output #0: loss = 0.0396057 (* 1 = 0.0396057 loss)
I1006 12:55:12.037997  5661 sgd_solver.cpp:172] Iteration 59200, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:56:06.945854  5661 solver.cpp:352] Iteration 59300 (1.82117 iter/s, 54.9097s/100 iter), 318.9/322.7ep, loss = 0.0373639
I1006 12:56:06.946686  5661 solver.cpp:376]     Train net output #0: loss = 0.0461349 (* 1 = 0.0461349 loss)
I1006 12:56:06.946694  5661 sgd_solver.cpp:172] Iteration 59300, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:56:13.064383  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:56:13.149559  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:57:01.857285  5661 solver.cpp:352] Iteration 59400 (1.82108 iter/s, 54.9124s/100 iter), 319.5/322.7ep, loss = 0.0368863
I1006 12:57:01.858135  5661 solver.cpp:376]     Train net output #0: loss = 0.03573 (* 1 = 0.03573 loss)
I1006 12:57:01.858144  5661 sgd_solver.cpp:172] Iteration 59400, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:57:55.163589  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:57:55.243839  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:57:56.790758  5661 solver.cpp:352] Iteration 59500 (1.82035 iter/s, 54.9344s/100 iter), 320/322.7ep, loss = 0.0538186
I1006 12:57:56.790786  5661 solver.cpp:376]     Train net output #0: loss = 0.0430964 (* 1 = 0.0430964 loss)
I1006 12:57:56.790794  5661 sgd_solver.cpp:172] Iteration 59500, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:58:51.747200  5661 solver.cpp:352] Iteration 59600 (1.81959 iter/s, 54.9574s/100 iter), 320.5/322.7ep, loss = 0.0467973
I1006 12:58:51.748040  5661 solver.cpp:376]     Train net output #0: loss = 0.0499925 (* 1 = 0.0499925 loss)
I1006 12:58:51.748049  5661 sgd_solver.cpp:172] Iteration 59600, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 12:59:37.350069  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:59:37.423501  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 12:59:46.656947  5661 solver.cpp:352] Iteration 59700 (1.82114 iter/s, 54.9107s/100 iter), 321.1/322.7ep, loss = 0.0422181
I1006 12:59:46.656972  5661 solver.cpp:376]     Train net output #0: loss = 0.0341798 (* 1 = 0.0341798 loss)
I1006 12:59:46.656980  5661 sgd_solver.cpp:172] Iteration 59700, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 13:00:41.523188  5661 solver.cpp:352] Iteration 59800 (1.82258 iter/s, 54.8672s/100 iter), 321.6/322.7ep, loss = 0.0411976
I1006 13:00:41.524010  5661 solver.cpp:376]     Train net output #0: loss = 0.0400202 (* 1 = 0.0400202 loss)
I1006 13:00:41.524019  5661 sgd_solver.cpp:172] Iteration 59800, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 13:01:19.581840  5684 data_reader.cpp:320] Restarting data pre-fetching
I1006 13:01:19.651527  5682 data_reader.cpp:320] Restarting data pre-fetching
I1006 13:01:36.592289  5661 solver.cpp:352] Iteration 59900 (1.81587 iter/s, 55.0701s/100 iter), 322.2/322.7ep, loss = 0.0443259
I1006 13:01:36.592314  5661 solver.cpp:376]     Train net output #0: loss = 0.0387541 (* 1 = 0.0387541 loss)
I1006 13:01:36.592320  5661 sgd_solver.cpp:172] Iteration 59900, lr = 0.0001, m = 0.9, wd = 1e-05, gs = 1
I1006 13:02:31.004722  5661 solver.cpp:352] Iteration 59999 (1.81941 iter/s, 54.4134s/99 iter), 322.7/322.7ep, loss = 0.0509148
I1006 13:02:31.005547  5661 solver.cpp:376]     Train net output #0: loss = 0.0533148 (* 1 = 0.0533148 loss)
I1006 13:02:31.005853  5661 solver.cpp:905] Snapshotting to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_60000.caffemodel
I1006 13:02:31.017402  5661 sgd_solver.cpp:398] Snapshotting solver state to binary proto file training/cityscapes5_jsegnet21v2_2018-10-04_23-38-05/sparse/cityscapes5_jsegnet21v2_iter_60000.solverstate
I1006 13:02:31.026082  5661 solver.cpp:419] Sparsity after training:
I1006 13:02:31.027170  5661 net.cpp:2727] Num Params(17), Sparsity (zero_weights/count): 
I1006 13:02:31.027179  5661 net.cpp:2738] conv1a_param_0(0.317) 
I1006 13:02:31.027190  5661 net.cpp:2738] conv1b_param_0(0.695) 
I1006 13:02:31.027194  5661 net.cpp:2738] ctx_conv1_param_0(0.809) 
I1006 13:02:31.027199  5661 net.cpp:2738] ctx_conv2_param_0(0.809) 
I1006 13:02:31.027201  5661 net.cpp:2738] ctx_conv3_param_0(0.809) 
I1006 13:02:31.027204  5661 net.cpp:2738] ctx_conv4_param_0(0.809) 
I1006 13:02:31.027209  5661 net.cpp:2738] ctx_final_param_0(0.354) 
I1006 13:02:31.027213  5661 net.cpp:2738] out3a_param_0(0.809) 
I1006 13:02:31.027216  5661 net.cpp:2738] out5a_param_0(0.81) 
I1006 13:02:31.027220  5661 net.cpp:2738] res2a_branch2a_param_0(0.8) 
I1006 13:02:31.027223  5661 net.cpp:2738] res2a_branch2b_param_0(0.731) 
I1006 13:02:31.027226  5661 net.cpp:2738] res3a_branch2a_param_0(0.808) 
I1006 13:02:31.027232  5661 net.cpp:2738] res3a_branch2b_param_0(0.793) 
I1006 13:02:31.027235  5661 net.cpp:2738] res4a_branch2a_param_0(0.81) 
I1006 13:02:31.027241  5661 net.cpp:2738] res4a_branch2b_param_0(0.805) 
I1006 13:02:31.027245  5661 net.cpp:2738] res5a_branch2a_param_0(0.809) 
I1006 13:02:31.027251  5661 net.cpp:2738] res5a_branch2b_param_0(0.81) 
I1006 13:02:31.027254  5661 net.cpp:2742] Total Sparsity (zero_weights/count) =  (2.17197e+06/2.69117e+06) 0.807
I1006 13:02:31.145781  5661 solver.cpp:501] Iteration 60000, loss = 0.0320298
I1006 13:02:31.145813  5661 solver.cpp:538] Iteration 60000, Testing net (#0)
I1006 13:02:39.969820  5687 blocking_queue.cpp:40] Data layer prefetch queue empty
I1006 13:02:39.976511  5690 data_reader.cpp:320] Restarting data pre-fetching
I1006 13:02:39.988224  5688 data_reader.cpp:320] Restarting data pre-fetching
I1006 13:02:40.085002  5661 solver.cpp:624]     Test net output #0: accuracy/top1 = 0.958187
I1006 13:02:40.085031  5661 solver.cpp:624]     Test net output #1: accuracy/top5 = 0.999999
I1006 13:02:40.085039  5661 solver.cpp:624]     Test net output #2: loss = 0.166175 (* 1 = 0.166175 loss)
I1006 13:02:40.085057  5661 caffe.cpp:268] Solver performance on device 0: 1.724 * 8 = 27.58 img/sec (60000 itr in 3.481e+04 sec)
I1006 13:02:40.085069  5661 caffe.cpp:271] Optimization Done in 9h 40m 39s
